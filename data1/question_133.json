{"question_id": 133, "title": "Will Google's AlphaGo beat world champion Lee Sedol in the five-game Go match planned for March 2016?", "possible_answers": ["Yes", "No"], "crowd_forecast": [0.98, 0.02], "correct_answer": "Yes", "correct_forecast": 0.98, "preds": [[14145, "2016-03-12T14:08:04Z", [1.0], "I just believe."], [22108, "2016-03-12T14:00:35Z", [1.0], "Post hoc."], [20044, "2016-03-12T13:43:59Z", [1.0], ""], [22114, "2016-03-12T13:31:05Z", [1.0], ""], [21853, "2016-03-12T12:52:25Z", [1.0], ""], [689, "2016-03-12T12:47:27Z", [1.0], "Comment deleted on May 10, 2016 11:10PM UTC"], [241, "2016-03-12T12:10:38Z", [1.0], ""], [20226, "2016-03-12T11:17:31Z", [1.0], ""], [4936, "2016-03-12T10:46:38Z", [1.0], ""], [22101, "2016-03-12T10:28:17Z", [0.85], ""], [19432, "2016-03-12T10:23:11Z", [0.9], ""], [21814, "2016-03-12T10:12:31Z", [1.0], "Comment deleted on May 11, 2020 09:01AM UTC"], [20692, "2016-03-12T09:59:11Z", [1.0], ""], [20448, "2016-03-12T09:56:04Z", [1.0], ""], [18915, "2016-03-12T09:42:13Z", [0.8], ""], [22100, "2016-03-12T09:20:27Z", [0.99], ""], [1474, "2016-03-12T08:55:05Z", [0.95], "100"], [22035, "2016-03-12T08:52:43Z", [1.0], ""], [12874, "2016-03-12T08:37:11Z", [1.0], ""], [20501, "2016-03-12T07:28:51Z", [1.0], ""], [436, "2016-03-12T07:02:09Z", [1.0], ""], [436, "2016-03-12T06:39:15Z", [0.99], ""], [2010, "2016-03-12T06:35:05Z", [1.0], ""], [5001, "2016-03-12T05:43:27Z", [0.99], ""], [2010, "2016-03-12T05:32:11Z", [0.99], ""], [212, "2016-03-12T04:49:53Z", [1.0], ""], [855, "2016-03-12T04:40:33Z", [1.0], "Given its standing two games up to 0.  I would find it very surprising if Alpha Go did not win at least one game.  Interesting article in Wired today.  http://www.wired.com/2016/03/sadness-beauty-watching-googles-ai-play-go/\nParticularly liked the comments by Fan Hui that he has moved up in the ranking after playing Alpha Go for the last 6 months."], [102, "2016-03-12T04:00:03Z", [0.87], "https://larswericson.wordpress.com/2016/03/12/gitrep-11mar16pm/"], [19770, "2016-03-12T02:21:20Z", [0.98], ""], [22086, "2016-03-12T01:24:22Z", [0.95], ""], [22086, "2016-03-12T01:24:09Z", [0.85], ""], [12595, "2016-03-12T00:19:29Z", [0.95], ""], [10921, "2016-03-12T00:00:53Z", [0.99], ""], [22079, "2016-03-11T23:52:28Z", [1.0], "Inevitable at this point, certainly?"], [2593, "2016-03-11T21:59:38Z", [0.95], "One of the ultimate strengths of AlphaGo is is the play in the middle of the board. This is a very difficult part of the board for human players as the complexity is very difficult to plan for a hundred moves ahead. The computer uses that advantage right from the beginning. The 2 or 3 moves that  surprised the human players eventually tied  into a strong middle position but only after dozens of moves were made. The professionals were asking themselves \"why there?\" In the last game Lee spent a good 25 minutes trying to figure out a move by AlphaGo that was not expected. \n Not only will Alphago increase the ability of computers to do complex tasks but will also help the Go community develop new strategies. Adding new moves to a 2,500 year old game is amazing. "], [837, "2016-03-11T21:24:11Z", [1.0], ""], [51, "2016-03-11T21:15:04Z", [1.0], "Affirming naturally.  Now this is Google showing off their new impressive toy.  Good for them by the way. But I wonder what AIs are out there, funded with hidden budgets and with far more aggressive purposes.  If you want to really scare the crap out of yourself, read a couple books by Matthew Mather on this general subject.    "], [21164, "2016-03-11T21:09:17Z", [0.95], "Based on the current track record, and that fact that Go requires a player to gauge an opponents body language to reach a truly comprehensive understanding of their mindset. Lee Sedol lacks the ability to gauge the computers body language as he would a human opponent, thus he is at a disadvantage"], [14702, "2016-03-11T20:37:47Z", [1.0], ""], [18967, "2016-03-11T19:50:31Z", [0.95], ""], [691, "2016-03-11T18:59:07Z", [0.99], "Grim days for the meat fans:  A machine can figure you out faster than you can figure out the machine.  So far, of course, only or mostly in fairly tightly constrained contexts, but get used to it."], [20226, "2016-03-11T18:42:06Z", [0.89], ""], [17820, "2016-03-11T18:02:18Z", [1.0], "too good so far."], [22058, "2016-03-11T17:54:42Z", [0.95], ""], [689, "2016-03-11T17:30:52Z", [1.0], ""], [2010, "2016-03-11T17:12:40Z", [0.95], ""], [55, "2016-03-11T16:42:09Z", [1.0], "Won a second game!"], [19807, "2016-03-11T16:20:06Z", [0.99], "Resistance is futile."], [22052, "2016-03-11T16:17:04Z", [0.96], ""], [176, "2016-03-11T15:36:21Z", [1.0], "Holy schnikes! "], [22049, "2016-03-11T15:28:35Z", [0.34], ""], [20135, "2016-03-11T15:28:30Z", [0.95], ""], [20031, "2016-03-11T13:50:21Z", [0.98], ""], [21924, "2016-03-11T13:22:39Z", [1.0], "Sedol has less than .5% chance of winning. Rounding to nearest %."], [20497, "2016-03-11T13:13:37Z", [1.0], ""], [22040, "2016-03-11T13:10:12Z", [1.0], ""], [18762, "2016-03-11T12:36:01Z", [0.99], ""], [22036, "2016-03-11T11:54:54Z", [1.0], ""], [22031, "2016-03-11T11:54:15Z", [0.95], "if it knows every possible move and counter"], [19990, "2016-03-11T09:29:02Z", [1.0], ""], [21817, "2016-03-11T09:13:41Z", [0.9], ""], [4936, "2016-03-11T08:55:41Z", [0.99], ""], [20078, "2016-03-11T08:31:30Z", [0.99], ""], [14656, "2016-03-11T07:48:01Z", [0.99], ""], [22025, "2016-03-11T07:29:26Z", [0.85], "The human has lost the first game and the European champion lost all 5. Nothing so far has shown that intuition and feel will best computation"], [19956, "2016-03-11T06:23:36Z", [0.9], ""], [22003, "2016-03-11T05:18:56Z", [0.95], ""], [1106, "2016-03-11T04:46:48Z", [1.0], "human cannot beat computer AI in a game rely on probability alone, just like no human can calculate faster than a computer no matter how good he is. it is not about how powerful over the opponent, it is about how little mistakes you make in a given situation, computer like alphago basically has close to zero faults for every moves because it can calculate all the probablity of winning each step without error. a human can not beat alphago by following a given set of rules which the algorithm has already trained itself to perfection. He can only win by smashing the computer during the game, alphago can't foresee this happen. by the way, AlphaGo is the beginning of SkyNet. hahaha"], [12753, "2016-03-11T04:28:06Z", [1.0], ""], [18967, "2016-03-11T04:00:11Z", [0.9], ""], [3320, "2016-03-11T03:52:13Z", [0.97], "I've done a complete reversal.  Not long before the first game, I put AlphaGo's chances at about 10% (I think).  After the first game, I moved up only to 35%, thinking that Lee Sedol had been over-confident and had tried some doubtful moves to test the system.  After two wins for AlphaGo, it seems to me highly unlikely that Lee can win three straight.   I was struck by a comment by one of the programmers on the AlphaGo design team, made during the English-language commentary.  The American pro (Michael Redmond, 9p) had remarked that one of AlphaGo's moves were what strong Go players would call \"slow.\"  A slow move is not a bad move, just one that is less than optimal, usually because it allows one's opponent to regain the initiative in exchange for what appears at the time to be only a modest gain.  The programmer said, in effect, this isn't a bug -- it's a feature.  AlphaGo doesn't care about the size of a win.  Given a choice between two moves -- one that the program's database suggests will offer a 75% probability of a 20-point win and another with an 80% probability of  a 2-point win, AlphaGo will choose the second move.  Few human players, even very good players, will do this.  It's commonplace for Go players to say (often about their own losses), \"I was greedy.\"  I've lost a lot of games due to this kind of overplay.  \"Greed\" aside, it often seems reasonable at the time.  Most of us have no way of knowing the probabilities of a win or loss until late in a game, so naturally we tend to push hard.  (Or sometimes we're intimidated by a strong player and make \"slow\" moves out of fear.)  AlphaGo knows neither greed nor fear and has enough games in its database to make an educated guess on the probability that a move will lead to a win, even if the win is only by 1-2 points and the probability only 51%.  So I'd agree with a comment by GJDrew that Lee Sedol may lack AlphaGo's \"objectivity,\" but I'd disagree with the implication that its due to his lack of study or anything else.  But my present guess is that it'll be almost impossible for Lee to win three straight against a program that represents objectivity quantified."], [19626, "2016-03-11T03:40:43Z", [0.85], ""], [1529, "2016-03-11T03:36:37Z", [1.0], "Not changing my %, but simply want to add a comment in respect of Sedol.... I hope the Go aficionados rally behind Sedol. To date, he has heaped the responsibility of the losses on his shoulders saying he made a mistake early, etc. But really, especially in the first game when he was going first, the strategy had to have been discussed among many inside experts. Probably the strategy was to put AG to the test with a slightly unconventional attack. The fact that AG surmounted the challenge is to AG's credit, not a blemish on Sedol. Several 9-Dan commentators throughout both matches were finding faults with AG's moves, but in the end the AG strategy was found to be superior. In fact, some moves of AG were considered inferior, but later determined to be superior...thereby opening new strategies for GO's corporeal followers . Whether it be chess or jeopardy, or other things, \"computers\" have markedly advanced in ways we could not imagine. Sedol should take no blame. He should hold his head high. Go players and his followers should show their pride in him in even putting himself at risk that he has. Just at a personal level, I really hope that he is treated respectfully and recognized for his many, many achievements. Google's Ag is truly a breakthrough and the \"AI\" modeling it is performing is applicable in hundreds, if not thousands, of different ways for humankind."], [4992, "2016-03-11T03:27:29Z", [0.99], ""], [2, "2016-03-11T03:13:19Z", [0.9], "Notice that the next match is not until the 12th.  So Sedol has a few days to recoup.   Not a change, but just noticed this in Lee's wikipedia entry: \"He ranks second in international titles (18), behind only Lee Chang-ho (21). Despite this, he describes his opening play as \"very weak\".[2]\" https://en.wikipedia.org/wiki/Lee_Sedol - I don't know if he's being humble or that's actually his weakness.  But I certainly didn't expect the world champ to be weak in openings.  That might be one of a reason why he's getting outplayed by the machine.  Some more digging... http://senseis.xmp.net/?YiSeTol \"However, a critical difference between the two is that while Cho is more orthodox and conventional in his view of the mid-game such that battles are a consequence of positional struggles, Lee emphasizes battles such that positional struggles are simply a prelude of a decisive battle and can even be forsaken. It can often be observed that Lee engages in battles shortly after the opening without ensuring positional superiority nor out of necessity to make up for an inferior position. More often than one may suspect, positions are determined by the battles in Lee's games. At the same time, unlike Yoo (or Cho to some extent), Lee's vicious attacks are designed to be decisive instead of to be a way to maintain or shift positional superiority without engaging in do or die showdowns.\" - Sounds like a tactical player.  Don't want to over-generalize, but if it was chess, that would be bad matchup against a computer.  I think I prefer a slow defensive positional player, now that I see the Go machine is pretty good.  Even more interesting is the description of Lee's weaknesses (last edited 2014, seems prescient): \"Lee's weaknesses have also been exposed. First of all, his judgement of position is not of top notch -- again relatively speaking, that is. Perhaps because of this, he has chosen to live and die with showdown battles. Or, his aggressive disposition/preference and hence the style have prevented him from studying this particular aspect of the game. Whichever is true, it seems that the relative deficiency in perhaps the most important aspect of the game may prove to be his biggest obstacle to win consistently against top rated players. Secondly, he appears to turn overly pessimistic when things do not go as he planned. Combined with the deficiency discussed above, this once led him to resign in 106 moves, and he was thought to be still ahead in the game even by his opponent. In other words, he can be emotional and displays the lack of objectivity from time to time. This tendency may prove to be another obstacle for him to win consistently. Lastly, he is not as studious and relies too heavily upon his talent. This will hinder his growth as a Go player and may not help him to accommodate changes necessary to stay afloat in the long run. Lee !Changho has been able to survive on the top over a decade mainly because his constant study of Go has deepened and broadened his game. Can Lee enjoy such longevity relying on talent alone? Considering that Cho HunHyun, a natural, had to redefine himself (and still is outclassed by Lee !Changho) in order to survive the onslaught of the 90's in the Korean Go scene, Lee Sedol will have to further his game and possibly reinvent himself in the future. \" - That's not a very good profile for a guy to come back from 0-2.  Unless he flashes some unexpected genius, which we haven't seen up to now.  I imagine a machine won't be impressed by that, while a flash of unexpected genius might turn the tide against a person.... On the other hand, if he really figures something out, he can sweep the machine because the machine won't be able to adjust either."], [20053, "2016-03-11T03:04:27Z", [0.98], ""], [14841, "2016-03-11T02:37:32Z", [1.0], ""], [14841, "2016-03-11T02:37:24Z", [0.7], ""], [22012, "2016-03-11T02:23:11Z", [0.9], ""], [20410, "2016-03-11T02:21:53Z", [0.9], ""], [16023, "2016-03-11T02:21:37Z", [0.97], ""], [18969, "2016-03-11T02:20:53Z", [0.97], "Down 0-2..."], [131, "2016-03-11T02:15:43Z", [0.9], ""], [16023, "2016-03-11T02:07:04Z", [0.95], ""], [14137, "2016-03-11T01:23:26Z", [0.99], "Revising up my forecast again (from 98%) following the second match win to AlphaGo. I began at 75% and revised to 98 after the first match."], [15331, "2016-03-11T01:11:31Z", [0.88], ""], [15331, "2016-03-11T01:11:25Z", [0.75], ""], [22009, "2016-03-11T01:02:54Z", [0.9], "AlphaGo will win four and tie one with Lee Sedol. "], [12879, "2016-03-11T00:57:25Z", [0.96], ""], [22007, "2016-03-11T00:29:19Z", [1.0], ""], [20217, "2016-03-11T00:24:16Z", [1.0], ""], [180, "2016-03-11T00:11:40Z", [0.9], ""], [15314, "2016-03-10T23:49:57Z", [0.95], ""], [22005, "2016-03-10T22:57:04Z", [0.99], ""], [14301, "2016-03-10T22:52:40Z", [0.95], ""], [15621, "2016-03-10T22:40:55Z", [1.0], ""], [2586, "2016-03-10T22:36:43Z", [0.88], "The king is dead. Long live the king. The two games have been close (i watched them). If I give each 50% chance of winning then Sedol has 12.5% chance of winning next three in a row. Full disclosure: I was ardently at 0% before. I believe most forecasts were too high based on information most people actually had. This makes me also believe that most people will not appreciate just what an astounding accomplishment these two victories were for team AlphaGo (note well: played under the union jack - the U.K. flag, not the stars and stripe.) I think many will say - yeah, so what a computer finally won at Go, whatever. Yet this was an accomplishment without precedent. From my point of view this is almost Nobel Prize type level accomplishment.  It was a strange feeling to watch. Strange to see a mystery of the universe pop out (with no explanation or reason) from the abyss of the AlphaGo program. To illustrate, here's a hypothetical conversation: Human go player : \"Why did you play there? Please,  I just want to know why so I can understand.\"\nAlphaGo: \"Because - just because\" Analysis of Game #2: I kept expecting to see AlphaGo's positions collapse under Sedol's subtle board-wide pressure, and then lo behold instead its stressed positions coalesced at the very end into a giant territory diagonally across the board. Sedol saved part of his soon-to-be surrounded remnants with a nice snap-back tactic on the left side but he'd lost/traded too much. Sedol had alot of territory but AlphaGo had just bit more.  It seems to me AlphaGo is creepily able to play in a way to slide into victory with just barely enough - as if it maybe is so strong it can gauge just how much risk it needs to take to have best odds of a win, but no more, which made the game exciting to watch. This would make sense from a maximizing function point of view. In my opinion Sedol needs to really jack up the complication early on by preventing AlphaGo from having any safe groups anywhere on the board. Keep everything in play as long as possible and load on early Ko fights. In both Games seemed to me Sedol was ahead in early-mid game. AlphaGo will likely have advantage in counting value of positions so need a decisive win against it. Gotta force AlphaGo into playing a high-risk high-reward Game. The more settled territory you give it early on the more it can crunch the value of remaining play and optimize it. All this Just my guess though."], [14258, "2016-03-10T22:33:38Z", [1.0], ""], [21822, "2016-03-10T22:25:21Z", [0.75], ""], [12874, "2016-03-10T22:06:54Z", [0.99], ""], [21997, "2016-03-10T21:52:17Z", [0.8], ""], [19306, "2016-03-10T21:31:00Z", [0.97], ""], [19306, "2016-03-10T21:19:32Z", [0.9], ""], [19306, "2016-03-10T21:15:07Z", [0.78], "Alphago is good and only need one more game"], [14563, "2016-03-10T21:11:54Z", [0.9], ""], [20031, "2016-03-10T20:52:59Z", [0.95], ""], [11885, "2016-03-10T20:51:35Z", [1.0], ""], [1629, "2016-03-10T20:27:10Z", [0.93], "Lee Sedol loss his first game to AlphaGo http://www.dailymail.co.uk/sciencetech/article-3483569/It-s-1-0-AlphaGo-Google-s-DeepMind-computer-BEATS-human-champion-Lee-Sedol-battle.html. Now its 2-0 http://www.dailymail.co.uk/sciencetech/article-3485328/Now-s-2-0-AlphaGo-Google-s-DeepMind-computer-takes-second-victory-against-human-champion-Lee-Sedol.html Its not looking good for Lee Sedol but he might rally round, will up my score if he loses the next game."], [20562, "2016-03-10T20:23:38Z", [1.0], "Taking what I previously rounded down to 99% and rounding up to 100%"], [20247, "2016-03-10T20:23:26Z", [0.99], ""], [55, "2016-03-10T19:57:36Z", [0.98], ""], [83, "2016-03-10T19:56:40Z", [1.0], "Match 2 comments:\nLee Sedol - never a point were I felt I was leading\nAlphGo - confident of victory at the mid point\nProfessional Commentators - At mid point, were unable to determine who was winning\nhttp://www.theverge.com/2016/3/10/11191184/lee-sedol-alphago-go-deepmind-google-match-2-result Question: how many matches would it take for Lee Sedol to figure out how to defeat AlpaGo?"], [20588, "2016-03-10T19:55:59Z", [0.98], "Updating from 85 to 98. Sorry, humans."], [961, "2016-03-10T19:37:11Z", [0.95], ""], [651, "2016-03-10T19:29:41Z", [0.9], "Two victories by AlphaGo suggest that initial pessimistic expectations were in need of revision."], [767, "2016-03-10T19:22:05Z", [1.0], "After watching game one, I increased my forecast from 35 to 70.  After watching game two, I'm going for 100.  Although I was wrong, I've really enjoyed forecasting this question. Truly historic, not to mention absolutely exciting to watch.   The matter of intuition in this particular game --which we all know no computer had been able to win until last October -- was the one factor holding me back from thinking AlphaGo had developed enough to beat the #2 player in the world.  Yet, here is something someone tweeted last night while livestreaming the match: \"The thing we refer to as intuition seems to fall down to pattern recognition\" #AlphaGo  There you go. "], [21985, "2016-03-10T19:10:14Z", [0.9], ""], [60, "2016-03-10T19:01:53Z", [1.0], "Raising form 98 to 100. Alright AlphaGo, I was wrong, I was wrong. (A little humo[u]r -- just the first scene --> https://www.youtube.com/watch?v=f_Ad0tHFqgY) After watching the first two games, I just don't see a path where Sedol is able to win 3 (!) games in a row. Of course, it's certainly possible, but with the way that AlphaGo is handling Sedol and more importantly, *surprising* Sedol with some moves, I expect that AlphaGo will win tomorrow and \"cinch\" the match."], [21980, "2016-03-10T18:39:54Z", [0.99], ""], [13361, "2016-03-10T18:36:12Z", [1.0], "Now that it's 2-0, AlphaGo's win looks very likely."], [19311, "2016-03-10T18:35:37Z", [1.0], ""], [10745, "2016-03-10T18:35:14Z", [1.0], ""], [21980, "2016-03-10T18:33:04Z", [0.87], ""], [4794, "2016-03-10T18:24:14Z", [1.0], ""], [12591, "2016-03-10T18:04:56Z", [0.98], ""], [1468, "2016-03-10T17:58:05Z", [0.96], ""], [14777, "2016-03-10T17:56:45Z", [0.8], ""], [14777, "2016-03-10T17:54:38Z", [0.4], ""], [17875, "2016-03-10T17:52:56Z", [1.0], "I started at 75% and was wrong about Sedol's ability to anticipate AlphaGo's innovative and unexpected play.  So, back up to 100%  : ) "], [2, "2016-03-10T17:19:06Z", [0.9], ""], [16742, "2016-03-10T17:16:56Z", [0.99], "2/5"], [38, "2016-03-10T17:14:18Z", [1.0], ""], [38, "2016-03-10T17:13:54Z", [0.97], ""], [12630, "2016-03-10T17:12:46Z", [1.0], ""], [14145, "2016-03-10T17:05:21Z", [0.97], ""], [16160, "2016-03-10T17:01:32Z", [1.0], ""], [16160, "2016-03-10T17:01:21Z", [0.95], ""], [20549, "2016-03-10T16:54:30Z", [0.99], "Hate to jump on the bandwagon but this looks like it's cinched. Unless Lee pulls out a victory in game 3, I don't see this forecast changing."], [15166, "2016-03-10T16:53:15Z", [0.98], ""], [12874, "2016-03-10T16:45:45Z", [0.98], ""], [21964, "2016-03-10T16:45:38Z", [0.95], ""], [19352, "2016-03-10T16:33:05Z", [1.0], ""], [12479, "2016-03-10T16:30:47Z", [0.99], ""], [21968, "2016-03-10T16:18:33Z", [1.0], ""], [21924, "2016-03-10T16:15:42Z", [0.99], "Sedol will have to win all three of the subsequent games.  Pretty much impossible task given how things look. Also i would discount what Go commentators have to say in general.  They evaluate moves based on heuristics they know, which were necessary to make learning go mentally tractable.  AlphaGo doesn't need heuristics, so can trade off \"understandable, salient but approximate rule of thumb\" for \"optimal but less understandable\", in its play.  So outcome speaks more than \"expert analysis\" of game details as understood by humans. \n"], [21958, "2016-03-10T16:05:18Z", [0.85], "The approach taken by the developers at Google is a very good one for the game of Go.  The combination of raw compute power (to examine outcomes from initial guesses from learned strategies) and reinforcement learning is going to lead ultimately to human defeat.  My initial guess is clearly on the strong side of yes, but at the moment I don't have a good feel between the relative strengths between the player in the previous victory and Lee.  It's not clear to me how the ranking differences accurately measure the (absolute) differences in skill.  My suspicion is that the differences in rankings exaggerate the absolute differences and that the players are closer to one another in compute/skill than indicated by rank."], [19476, "2016-03-10T16:05:11Z", [0.96], "AlphaGo won the second round of the 5 game match against Lee Sedol.  Even though Lee Sedol admited to make mistakes, I believe that the program has now very clear chances of taking the victory.  I might bounce back with my forecast if Lee Sedol manages to win two games in a row."], [4288, "2016-03-10T15:58:55Z", [0.86], "Based on second game result. Economist provides a nice summary of the two algorithms being used by AlphaGo. http://www.economist.com/news/science-and-technology/21694540-win-or-lose-best-five-battle-contest-another-milestone "], [10930, "2016-03-10T15:42:51Z", [0.96], ""], [1738, "2016-03-10T15:41:46Z", [0.95], ""], [3363, "2016-03-10T15:31:22Z", [0.91], "Still below 100 just in case Lee Sedol is secretly fooling us all."], [19807, "2016-03-10T15:29:26Z", [0.95], "Ooops... AlphaGo is 2-0...  I guess I was, like, wrong... the dudes at Google are pros. :-)"], [14027, "2016-03-10T15:17:27Z", [1.0], ""], [2, "2016-03-10T15:14:54Z", [0.88], ""], [19148, "2016-03-10T15:12:25Z", [0.98], ""], [19148, "2016-03-10T15:12:04Z", [0.95], ""], [19148, "2016-03-10T15:10:40Z", [0.85], ""], [19148, "2016-03-10T15:08:38Z", [0.7], ""], [19148, "2016-03-10T15:06:59Z", [0.8], ""], [2, "2016-03-10T15:00:36Z", [0.93], "Moving up a bit more after reading Lee's own assessment: http://www.theverge.com/2016/3/10/11191184/lee-sedol-alphago-go-deepmind-google-match-2-result \"Yesterday I was surprised but today it's more than that \u2014 I am speechless,\" said Lee in the post-game press conference. \"I admit that it was a very clear loss on my part. From the very beginning of the game I did not feel like there was a point that I was leading.\"  - I watched a lot of the stream, and I thought the 9-dan expert commentator was thinking Lee was doing quite well during most of it.  But it turned out not to be the case, nor did Lee think so.  If the computer that much ahead of the commentator, then it's much stronger than I thought.  Or like the article says: \"The close nature of the game appears to offer validation of AlphaGo's evaluative ability, the main roadblock to proficiency for previous Go programs. Hassabis says that AlphaGo was confident in victory from the midway point of the game, even though the professional commentators couldn't tell which player was ahead.\" http://www.theverge.com/2016/3/10/11192774/demis-hassabis-interview-alphago-google-deepmind-ai \"I was quite surprised that even on the live commentary Michael Redmond was having difficulty counting out the game, and he\u2019s a 9-dan pro! And that just shows you how hard it is to write a valuation function for Go.\""], [21758, "2016-03-10T15:00:17Z", [0.99], ""], [2, "2016-03-10T14:56:22Z", [0.9], "No change.  Just more postmorteming...  another factoid I missed that I wish I knew earlier: http://demishassabis.com/biography/ That's the CEO of Deepmind.  \"I also play many games to an accomplished level including chess, shogi and poker and won the World Games Championships at the Mind Sports Olympiad a record 5-times before retiring from competitive play in 2003. \" He's both a computer AI guy and an expert level games player.  To me, that's a lot more credibility than just an AI guy saying they can win.  Most games players are much more calibrated in terms of estimating winning chances, and hate to lose publicly."], [241, "2016-03-10T14:54:28Z", [0.75], ""], [20044, "2016-03-10T14:52:19Z", [0.99], "Even ignoring my previous reasoning, and assuming AlphaGo and Lee Sedol are evenly matched with each having a 50% chance of winning a game (although this now seems unlikely, considering AlphaGo played an essentially errorless game in round 2), Lee Sedol would have just a 12.5% chance of winning the remaining 3 games.  In addition, the fact that Lee Sedol is human cannot be ignored. He must be feeling some pressure and in fact has looked upset or uncomfortable at various points throughout the matches. When AlphaGo played a particularly unusual move during match 2, Lee Sedol went so far as to leave the room. His ability to manage time also eventually became an issue, while AlphaGo was obviously under no such constraint. Conversely, AlphaGo has the luxury of feeling no pressure, no anxiety, and no emotion which might distract it from the game. This decreases the likelihood that Lee Sedol might win further.  Lastly, taking my original analysis into consideration (as well as the strength of AlphaGo's play during match 2), leads me to conclude that victory is all but certain for the AI. "], [1529, "2016-03-10T14:50:47Z", [1.0], ""], [14327, "2016-03-10T14:50:38Z", [1.0], "As others have noted - I strongly doubt that Sedol will win all 3 of the next games given \n1. His loss of both of the previous two \n2. His statements after the match, saying he felt like AlphaGo was in the lead the whole time "], [4992, "2016-03-10T14:44:34Z", [0.95], ""], [90, "2016-03-10T14:18:38Z", [0.99], ""], [5001, "2016-03-10T14:15:13Z", [0.97], "AlphaGo played more creatively! It's as if the traditional human/computer roles were reversed. "], [21951, "2016-03-10T14:09:18Z", [0.95], ""], [21853, "2016-03-10T14:07:46Z", [0.98], ""], [773, "2016-03-10T14:00:05Z", [0.92], ""], [100, "2016-03-10T13:51:01Z", [1.0], ""], [436, "2016-03-10T13:41:57Z", [0.94], ""], [19218, "2016-03-10T13:32:26Z", [0.97], "My old prediction was plainly silly! It's very hard to imagine a turnaround at this point."], [16026, "2016-03-10T13:32:10Z", [0.98], "AlphaGo won against lee in both going first and later, and mental pressure for lee is very high.\n"], [19170, "2016-03-10T13:31:53Z", [0.99], ""], [15477, "2016-03-10T13:26:49Z", [0.95], "Now up 2-0.  Very slim chance Lee can pull this off."], [6518, "2016-03-10T13:24:55Z", [0.98], ""], [18762, "2016-03-10T13:22:37Z", [0.9], ""], [1218, "2016-03-10T13:20:23Z", [0.99], ""], [2010, "2016-03-10T13:16:27Z", [0.95], ""], [19106, "2016-03-10T13:16:04Z", [0.97], "That game felt like it could have gone Lee's way. I'm not sure whether that's a sign that it was actually close, or that traditional human evaluations of game board value are off, and Alphago was \"winning\" the entire time. I am adjusting downwards the probability that Lee beats AG in a given game, to 30%. This yields a win probability for Lee of 2.7%."], [110, "2016-03-10T13:10:19Z", [1.0], "Throwing in the towel. #teamhumans, we're in trouble!"], [21800, "2016-03-10T13:04:20Z", [1.0], "Coming back from 2-0 down against an AI opponent should be next to impossible."], [12302, "2016-03-10T12:57:12Z", [0.99], ""], [915, "2016-03-10T12:34:38Z", [1.0], "Google = Skynet."], [73, "2016-03-10T12:31:45Z", [1.0], ""], [20135, "2016-03-10T12:19:47Z", [0.85], ""], [691, "2016-03-10T12:15:26Z", [0.94], ""], [689, "2016-03-10T12:15:16Z", [0.89], "Comment deleted on May 10, 2016 11:09PM UTC"], [20082, "2016-03-10T12:09:42Z", [1.0], ""], [12289, "2016-03-10T12:08:32Z", [1.0], ""], [19427, "2016-03-10T12:00:46Z", [1.0], ""], [19308, "2016-03-10T11:33:00Z", [0.99], ""], [19308, "2016-03-10T11:32:43Z", [0.9], ""], [51, "2016-03-10T11:24:07Z", [1.0], "AI just won second game.  Only needs to win one more to win three of five.  "], [1646, "2016-03-10T11:22:04Z", [1.0], "crushed..."], [16136, "2016-03-10T11:08:20Z", [0.99], ""], [13432, "2016-03-10T10:57:05Z", [0.99], ""], [17819, "2016-03-10T10:38:13Z", [0.9], "If alpha go wins only one more game it wins the five-game match. Judging by the way it played today, I see it as very unlikely that it will lose tomorrow's game."], [17819, "2016-03-10T10:34:30Z", [0.8], ""], [19432, "2016-03-10T10:23:08Z", [0.7], "as per comments and articles looks like deep mind is slightly better. still needs 1/3 matches to win the series, would give it a slightly higher chance than just 2/3rds probability"], [19432, "2016-03-10T10:20:18Z", [0.69], ""], [20230, "2016-03-10T10:17:35Z", [0.95], ""], [19432, "2016-03-10T10:12:09Z", [0.7], ""], [19815, "2016-03-10T10:12:02Z", [0.96], ""], [20448, "2016-03-10T09:48:30Z", [0.9], ""], [18834, "2016-03-10T09:46:16Z", [1.0], ""], [15471, "2016-03-10T09:32:02Z", [1.0], ""], [20131, "2016-03-10T09:25:28Z", [0.85], ""], [19449, "2016-03-10T09:04:47Z", [0.92], "Looking less and less likely"], [20501, "2016-03-10T08:55:27Z", [0.99], ""], [10759, "2016-03-10T08:52:43Z", [1.0], ""], [4634, "2016-03-10T08:51:57Z", [1.0], ""], [13377, "2016-03-10T08:49:02Z", [0.95], ""], [20156, "2016-03-10T08:43:30Z", [1.0], ""], [2, "2016-03-10T08:36:37Z", [0.9], ""], [68, "2016-03-10T08:33:58Z", [1.0], "AlphaGo wins 2-0! Lee Seedol has to win the last three in a row. Chances of that happening if both were exactly equal is 12.5%. I'm going to 100 till the next game and will come down to 75 if he wins it."], [1993, "2016-03-10T08:33:52Z", [1.0], "I watched most of the match this time. Lee Sedol and AlphaGo seemed evenly matched most of the time. But Lee missed potential for points in the center of the board, which the commentator also missed until the opportunity had passed. I wonder if the fact that humans are accustomed to analyzing the board visually is a disadvantage when playing against a computer. Result: Lee resigned when they were both in overtime. "], [12118, "2016-03-10T08:32:34Z", [0.99], ""], [19770, "2016-03-10T08:30:37Z", [0.96], ""], [20692, "2016-03-10T08:29:55Z", [0.99], ""], [11080, "2016-03-10T08:28:01Z", [1.0], "AlphaGo has just won the second game. This looks like it's done - don't see Lee Sedol coming back from a 2-0 hole."], [498, "2016-03-10T08:27:35Z", [0.98], ""], [15621, "2016-03-10T08:23:22Z", [0.99], ""], [2, "2016-03-10T08:18:30Z", [0.85], ""], [10783, "2016-03-10T08:16:48Z", [1.0], ""], [2, "2016-03-10T08:12:58Z", [0.8], ""], [2, "2016-03-10T08:11:12Z", [0.75], "Looks like AlphaGo is ahead in the 2nd game too, moving up. Also, the 9-dan expert commentor seemed to like Lee's position throughout most of the match, and then AlphaGo seemed to surprise him by pulling ahead.  So that's not a good sign for human expertise."], [60, "2016-03-10T07:59:23Z", [0.98], "Upping from 91 to 98. I think Lee Sedol lost (game's still going)."], [21939, "2016-03-10T07:53:56Z", [0.99], "Before the first match i made a prediction that the score will be 5:0 or 4:1,but i am not sure who would win the match. My opinion was it must be a overwhelming outcome,whether for Lee or AlphaGO. After the first match, I could say its a 99 percent winning rate and the score will be 5:0 or 4:1\u3002"], [2010, "2016-03-10T06:52:02Z", [0.9], ""], [436, "2016-03-10T06:51:23Z", [0.87], ""], [2010, "2016-03-10T06:37:47Z", [0.88], ""], [2, "2016-03-10T06:25:16Z", [0.58], "No change, but one thing the expert on the stream said was interesting.  When describing the prior efforts at a Go robot, he said that the prior efforts had weaknesses that made them amateur, but if they were allowed to play their own game, they were around 5-dan.  Which is actually much better than I thought they were.  "], [2010, "2016-03-10T06:01:09Z", [0.85], ""], [2010, "2016-03-10T05:57:00Z", [0.75], ""], [131, "2016-03-10T05:44:16Z", [0.3], ""], [14145, "2016-03-10T05:38:49Z", [0.8], ""], [2010, "2016-03-10T05:25:47Z", [0.7], ""], [13942, "2016-03-10T05:25:35Z", [1.0], ""], [102, "2016-03-10T04:59:43Z", [1.0], "http://www.nytimes.com/2016/03/10/world/asia/google-alphago-lee-se-dol.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=photo-spot-region&region=top-news&WT.nav=top-news&_r=0"], [93, "2016-03-10T04:50:45Z", [0.85], ""], [19803, "2016-03-10T04:43:28Z", [0.7], ""], [14327, "2016-03-10T04:15:10Z", [0.95], "Lee himself said the final score will be either 5-0 or 4-1.  I'll take his word for it. "], [2, "2016-03-10T04:13:40Z", [0.58], "Here's a less breathless article, plus @morrelldaniels that Lee was actually 3-3.5 pts ahead, but lost due to the handicap for going 2nd.  Just going to moderate slightly. http://www.slate.com/blogs/future_tense/2016/03/09/google_deepmind_s_alphago_ai_beats_champion_lee_sedol_in_go.html That Lee thought it was the opening that screwed him up, rather than the end-game, actually is also a good sign.  I'm not sure if Go is the same, but Chess openings are hard coded into the system, which means it's human-driven not calculation driven."], [4992, "2016-03-10T03:44:47Z", [0.9], ""], [1993, "2016-03-10T03:26:11Z", [0.5], "Lee made a mistake early on. AlphaGo also made mistakes and is clearly exploitable. I'm sure Lee learned a lot in the first game. I give him a 50/50 chance until I see the second game."], [20929, "2016-03-10T03:02:10Z", [1.0], ""], [15443, "2016-03-10T02:50:58Z", [0.95], ""], [10298, "2016-03-10T02:36:03Z", [1.0], ""], [20410, "2016-03-10T02:29:35Z", [0.75], ""], [20338, "2016-03-10T02:23:42Z", [0.65], ""], [14702, "2016-03-10T02:11:07Z", [0.99], ""], [498, "2016-03-10T02:07:03Z", [0.2], "Mr. Lee said he knew he had lost the match after AlphaGo made a move so unexpected and unconventional that he thought \u201cit was impossible to make such a move.\u201d\nhttp://www.nytimes.com/2016/03/10/world/asia/google-alphago-lee-se-dol.html?_r=0 Does anyone know which particular move Sedol is referring to? Redmond made a comment along-the-lines of \"that's an interesting move\" a couple-three times regarding an AlphaGo move, but in no case did he seem to belabor the point. He just continued his commentary in stride. Just curious which one Sedol viewed as critical."], [14137, "2016-03-10T01:30:22Z", [0.98], "Having won the first game makes it all but certain AlphaGo will win the round. AlphaGo improves every game, while a human player is more likely to be derailed by an unexpected (to them) loss."], [12479, "2016-03-10T01:20:02Z", [0.93], ""], [83, "2016-03-10T01:06:23Z", [0.98], "Lee Sedol must win 3 of 4 matches."], [3320, "2016-03-10T00:55:52Z", [0.35], "Well, a day or so ago I was giving AlphaGo only a 10% chance.  The program is obviously much stronger than I thought possible.  Lee Sedol made some doubtful moves (or so players competent to have an informed opinion write) as did AlphaGo.  The game was close.  Redmond thought that Lee won \"on the board,\" meaning that AlphaGo won because of the _komi_ (handicap to compensate for Black's first move).  I think Lee will play better, and I doubt that AlphaGo will \"learn\" that much overnight since it's database already must includes many thousands of games.  The big factor may be fatigue.  This is a pretty stressful contest for a human, and AlphaGo won't get tired. ( I'm what the commentators call a mid-kyu level player.   If I played basketball as well as I play Go, you'd be glad to have me on your team for a pickup game at the Y, and maybe I'd be a starter for a small-town high school.)  Anyway, I'm still betting on Lee Sedol but with less confidence."], [21924, "2016-03-10T00:49:57Z", [0.87], "How likely is it for Lee to not make any more mistakes?  Pretty unlikely I think, especially now that the pressure is on.\nBefore the game, Lee predicted it would be 5-0 or 4-1, and he will fight for the 5-0. Gross underestimation of opponent.  Now add the psychological pressure.  Unless you think game 1 outcome is not representative of their true skill differential - and this is unlikely vs. the hypothesis that AlphaGo is just (somewhat? a lot?) better - Lee is toast. So I predict 6 or 7:1 odds. Just .02.  (No knowledge of Go..)"], [689, "2016-03-10T00:26:41Z", [0.5], ""], [21924, "2016-03-10T00:22:40Z", [0.85], ""], [12879, "2016-03-10T00:21:41Z", [0.75], ""], [15331, "2016-03-10T00:05:15Z", [0.35], ""], [16024, "2016-03-10T00:01:40Z", [0.99], ""], [1218, "2016-03-09T22:59:20Z", [0.65], ""], [100, "2016-03-09T22:47:26Z", [0.65], ""], [14301, "2016-03-09T22:46:03Z", [0.49], ""], [1468, "2016-03-09T22:42:44Z", [0.88], ""], [1468, "2016-03-09T22:42:31Z", [0.65], ""], [138, "2016-03-09T21:36:22Z", [0.59], ""], [19306, "2016-03-09T21:22:44Z", [0.69], "First game was won by Alphago, wich seems stronger than I have guessed => Updating my forecast"], [16026, "2016-03-09T21:22:42Z", [0.86], ""], [20501, "2016-03-09T21:07:55Z", [0.8], ""], [15870, "2016-03-09T20:41:12Z", [0.55], ""], [15261, "2016-03-09T20:36:08Z", [0.8], ""], [21910, "2016-03-09T20:30:23Z", [0.85], ""], [4288, "2016-03-09T20:12:17Z", [0.81], "Despite Lee Sedol now observing how AlphaGo plays, this will be of relative help to Sedol. This is because AlphaGo is dynamical in its learning; meaning the opponent that Sedol will play will be be a different opponent each and every time. As the saying goes, 'you can never step into the same river twice'  "], [21822, "2016-03-09T19:51:27Z", [0.56], ""], [20226, "2016-03-09T19:48:50Z", [0.78], ""], [20, "2016-03-09T19:45:52Z", [0.95], ""], [19829, "2016-03-09T19:45:06Z", [0.95], ""], [14841, "2016-03-09T19:39:58Z", [0.7], ""], [17875, "2016-03-09T19:38:11Z", [0.0], "Alphago won the first game - not unexpected.  The game was beautiful. I was so happy to see AlphaGo play aggressive and against convention in many ways.  I don't know Go so I had to watch through great commentary:  https://www.youtube.com/watch?v=6ZugVil2v4w I was at 0% and I'm staying at 0% because Lee now knows it's on like Donkey-Kong.  Lee is the champion. Champion's loose from time-to-time and the title is open.  It will be up to Lee to win.  I'd change to 100% if AlphaGo were simply beyond Lee's capability to win, but that isn't the case.  This is the right moment and will be very fun to watch."], [12413, "2016-03-09T19:10:28Z", [0.75], "Increasing my prediction after first match"], [17875, "2016-03-09T19:09:15Z", [0.0], ": )"], [20247, "2016-03-09T19:05:32Z", [0.78], ""], [11885, "2016-03-09T19:02:59Z", [1.0], "Wondering how I could have forecasted 0% just y'day. My Snowie backgammon software kills me since 10Y+. & wasn't it Brad DeLong who just recently argued that we may have reached Peak-human\". I wonder when bots will make a killing in this nice good-judgement competitions."], [11885, "2016-03-09T18:54:45Z", [1.0], ""], [1500, "2016-03-09T18:46:54Z", [0.75], "Upping my forecast since AlphaGo won the first game."], [21778, "2016-03-09T18:38:13Z", [0.7], ""], [19807, "2016-03-09T18:20:42Z", [0.25], "AlphaGo won Match 1. Not going to be easy."], [14646, "2016-03-09T18:11:51Z", [0.97], ""], [3474, "2016-03-09T18:08:05Z", [0.5], "Hedging bets."], [2, "2016-03-09T18:05:26Z", [0.64], "http://www.nytimes.com/2016/03/10/world/asia/google-alphago-lee-se-dol.html?_r=0 Lee sounds mentally defeated in this article.  It sounds like a guy who's been punched in the nose for the first time. What's not clear is whether that's the NYT reporter speaking ( selectively quoting) or what Lee really thinks... \"AlphaGo posed Mr. Lee a unique challenge. In a human-versus-human Go match, which typically lasts several hours, the players \u201cfeel\u201d each other and evaluate styles and psychologies, he said. \u201cThis time, it\u2019s like playing the game alone,\u201d Mr. Lee said on the eve of the match. \u201cThere are mistakes humans make because they are humans. If that happens to me, I can lose a match.\u201d\""], [19432, "2016-03-09T18:02:54Z", [0.6], "Previous rationale plus developments of today with alpha go beating lee seedol in the first game "], [15471, "2016-03-09T17:45:13Z", [0.85], ""], [909, "2016-03-09T17:40:37Z", [0.99], "The singularity is nigh, AlphaGo won.   http://www.npr.org/sections/thetwo-way/2016/03/09/469788814/ai-program-from-google-beats-human-world-champ-in-game-of-go"], [110, "2016-03-09T17:37:57Z", [0.25], "http://www.businessinsider.com/heres-how-much-computing-power-google-deepmind-needed-to-beat-lee-sedol-2016-3 \"At the end of Wednesday's game, AlphaGo was down to 5:30 of its time, while Lee had 28:28 left on the clock.\" If I were Sedol, I would be figuring out how to get AlphaGo to exhaust its time remaining."], [14027, "2016-03-09T17:24:06Z", [0.95], ""], [12289, "2016-03-09T17:18:55Z", [0.95], ""], [20230, "2016-03-09T17:01:20Z", [0.65], ""], [20031, "2016-03-09T16:59:53Z", [0.75], ""], [18489, "2016-03-09T16:54:39Z", [0.65], ""], [1646, "2016-03-09T16:48:40Z", [0.9], "watching the game live made me want to be in a live go prediction tournament. Would such a thing be technically possible? Observers of the game predicting AlphaGo\u00b4s and Lee Sedol\u00b4s next movement, getting running Brier\u00b4s scores for it, and identifying the best predictors with possibly a chance to go against AlphaGo themselves... "], [14933, "2016-03-09T16:42:07Z", [0.68], ""], [10930, "2016-03-09T16:27:16Z", [0.75], ""], [19352, "2016-03-09T16:13:38Z", [0.92], ""], [55, "2016-03-09T16:07:36Z", [0.88], ""], [20078, "2016-03-09T16:06:11Z", [0.94], ""], [20078, "2016-03-09T16:02:33Z", [0.87], ""], [14253, "2016-03-09T15:51:53Z", [1.0], ""], [10759, "2016-03-09T15:51:08Z", [0.9], ""], [4634, "2016-03-09T15:50:38Z", [0.9], ""], [180, "2016-03-09T15:42:55Z", [0.79], ""], [1529, "2016-03-09T15:31:53Z", [0.93], "AlphaGo was able to slightly edge out a 9 DAN-LEVEL player. The complexity of the endgame was the deciding difference. Notably, AphaGo was willing and able to play aggressively at this top level. Probably most people did not expect this level of capability. Psychologically, this is a big weight that human player has to carry. For alpha, it's just in a day's work. It looks like AlphaGo has an edge in the endgame due to its massive computing power when there are less, but very critical, moves remaining."], [271, "2016-03-09T15:20:20Z", [1.0], ""], [19908, "2016-03-09T15:19:47Z", [0.95], ""], [2010, "2016-03-09T15:12:18Z", [0.75], ""], [10783, "2016-03-09T15:08:21Z", [0.3], "I still think that Lee will win.  Perhaps I'm stubborn."], [2, "2016-03-09T15:06:39Z", [0.55], "Alpha Go won the first game.  Updating from 25% to 55%. http://www.cnet.com/news/google-deepmind-hooked-us-on-go-the-geekiest-game-youve-never-heard-of/"], [180, "2016-03-09T15:01:46Z", [0.79], "AlphaGo up by one."], [20044, "2016-03-09T14:51:58Z", [0.96], "Upping my estimate slightly, from 95 - 96%. While I have long expected AlphaGo to win, I was surprised it was able to win the first game. It is important to note that AlphaGo will likely continue to improve as the matches occur, iterating through further games played against itself - only this time with the additional data gained from playing Lee Sedol himself. "], [176, "2016-03-09T14:42:18Z", [0.5], ""], [10762, "2016-03-09T14:37:39Z", [0.98], ""], [21853, "2016-03-09T14:34:39Z", [0.9], ""], [21853, "2016-03-09T14:30:43Z", [0.9], ""], [436, "2016-03-09T14:27:55Z", [0.77], ""], [20020, "2016-03-09T14:27:28Z", [0.85], ""], [21853, "2016-03-09T14:26:39Z", [0.85], ""], [12828, "2016-03-09T14:12:33Z", [1.0], ""], [4794, "2016-03-09T14:05:19Z", [0.9], ""], [6552, "2016-03-09T14:03:05Z", [0.9], ""], [19170, "2016-03-09T13:59:53Z", [0.6], ""], [13361, "2016-03-09T13:52:19Z", [0.85], "Well, I guess AlphaGo winning the first match against Lee Sedol answers the question about how good it has gotten over the last 6 months. I'll up my prediction that it will take the five-game match.\nhttp://www.kurzweilai.net/alphago-machine-learning-program-defeats-top-go-player-in-first-match"], [823, "2016-03-09T13:37:50Z", [0.95], ""], [14777, "2016-03-09T13:36:44Z", [0.2], ""], [19170, "2016-03-09T13:35:24Z", [0.45], ""], [19427, "2016-03-09T13:35:01Z", [0.95], ""], [19427, "2016-03-09T13:34:15Z", [0.94], ""], [19427, "2016-03-09T13:33:59Z", [0.92], ""], [19427, "2016-03-09T13:32:52Z", [0.97], ""], [73, "2016-03-09T13:25:33Z", [0.99], ""], [14920, "2016-03-09T13:25:28Z", [1.0], ""], [6518, "2016-03-09T13:15:54Z", [0.75], ""], [1529, "2016-03-09T13:15:09Z", [0.85], ""], [1529, "2016-03-09T13:14:34Z", [0.75], ""], [15477, "2016-03-09T13:08:57Z", [0.85], "Already won round 1"], [12874, "2016-03-09T13:07:31Z", [0.86], ""], [691, "2016-03-09T13:05:32Z", [0.76], "I expected the machine to lose the first game.  "], [14145, "2016-03-09T12:58:24Z", [0.7], ""], [10762, "2016-03-09T12:58:19Z", [0.67], ""], [11080, "2016-03-09T12:49:26Z", [0.95], ""], [10762, "2016-03-09T12:48:38Z", [0.5], ""], [10921, "2016-03-09T12:48:18Z", [0.65], ""], [19260, "2016-03-09T12:46:10Z", [1.0], "Well, I was very clearly wrong.  I didn't expect that playing itself (given the relative paucity of training data above the amateur level, and the limited scaling benefits of throwing more hardware at a Monte Carlo tree search) would genuinely let it train to high-professional levels.  Throwing all the way to 100% to balance out the long time I've had it  at 12%."], [1646, "2016-03-09T12:33:49Z", [0.9], ""], [110, "2016-03-09T12:28:29Z", [0.25], "http://www.businessinsider.com/google-deepmind-ai-alphago-defeats-human-lee-sedol-go-in-south-korea-2016-3 Uh oh. Raising on the basis of the first game. Analysts called it a close game, fiercely fought. If AlphaGo wins the second game, I'll probably go to P=1.0."], [597, "2016-03-09T12:23:53Z", [0.99], ""], [773, "2016-03-09T12:13:57Z", [0.67], ""], [20692, "2016-03-09T12:13:50Z", [0.97], ""], [20692, "2016-03-09T12:01:53Z", [0.99], ""], [19106, "2016-03-09T11:57:52Z", [0.85], "All of my suspicions have been confirmed by the first game. They seem fairly evenly matched - Lee made some mistakes but so did AlphaGo. Lee was even maybe ahead at one point (from 93 to 119), coming back from early mistakes.\nhttps://gogameguru.com/alphago-defeats-lee-sedol-game-1/ The trouble is, to win, Lee needs to make almost no mistakes. He was not in his best form in this game so there is still room for him to win, but I think his chances are severely hindered."], [1738, "2016-03-09T11:33:44Z", [0.75], ""], [12591, "2016-03-09T11:18:32Z", [0.8], ""], [19449, "2016-03-09T11:06:45Z", [0.88], ""], [212, "2016-03-09T11:04:10Z", [0.95], ""], [10039, "2016-03-09T10:59:46Z", [1.0], ""], [3363, "2016-03-09T10:27:23Z", [0.83], "I, for one, welcome our new robot overlords."], [1468, "2016-03-09T10:22:21Z", [0.96], "http://www.wired.com/2016/03/googles-ai-wins-first-game-historic-match-go-champion/"], [21800, "2016-03-09T10:21:29Z", [0.99], "Alpha has won the first game. "], [1106, "2016-03-09T09:59:28Z", [1.0], ""], [1106, "2016-03-09T09:59:07Z", [0.99], ""], [19308, "2016-03-09T09:54:45Z", [0.25], ""], [11788, "2016-03-09T09:48:03Z", [1.0], ""], [12874, "2016-03-09T09:47:35Z", [0.88], ""], [12874, "2016-03-09T09:44:10Z", [0.83], ""], [19815, "2016-03-09T09:35:17Z", [0.78], ""], [15621, "2016-03-09T09:28:58Z", [0.98], ""], [361, "2016-03-09T09:21:03Z", [0.98], ""], [15621, "2016-03-09T09:20:15Z", [0.95], "Alpha just won first game\nhttp://www.bbc.com/news/technology-35761246\n"], [20156, "2016-03-09T09:09:28Z", [0.85], ""], [19311, "2016-03-09T09:08:23Z", [0.8], ""], [19311, "2016-03-09T09:05:13Z", [0.75], "well, I didn't definitely expect this... Upping the chance sharply"], [20448, "2016-03-09T09:04:54Z", [0.75], ""], [20501, "2016-03-09T08:45:46Z", [0.96], "AlphaGo 1-0 warrants a strong update"], [17819, "2016-03-09T08:38:49Z", [0.57], ""], [20497, "2016-03-09T08:36:07Z", [0.99], ""], [20131, "2016-03-09T08:18:32Z", [0.8], "AlphaGo won the first game and so I upping the probability by 5%."], [4936, "2016-03-09T07:58:02Z", [0.85], ""], [19956, "2016-03-09T07:51:28Z", [0.6], ""], [19476, "2016-03-09T07:49:56Z", [0.73], "Upping my forecast in favor to AlphaGo after its win on the first match against Lee Sedol. \n"], [4386, "2016-03-09T07:45:40Z", [0.9], ""], [18914, "2016-03-09T07:44:22Z", [0.85], "Just to give you guys some information, after the first game, pros are still split on whether AlphaGo can win the series or not, and many are still confident in Lee Sedol. That said, this is not the distributed version of AlphaGo, so if they used that, then Lee Sedol probably will have no chance. "], [767, "2016-03-09T07:42:15Z", [0.7], "AlphaGo beat Lee SeDol on game one. Very close match but as I see it, this game was decisive. The computer will only improve in subsequent games. "], [5001, "2016-03-09T07:41:24Z", [0.65], "Wow"], [11872, "2016-03-09T07:40:26Z", [0.75], ""], [60, "2016-03-09T07:39:57Z", [0.91], "Upping from 14 to 91. AlphaGo just won the first game. Lee Sedol resigned during the \"endgame.\" The fellas on the commentary seemed to think that Sedol might have had a board advantage, but was going to lose on \"points.\" I don't know enough about Go to make an intelligent comment one way or the other, but from what I do know of Go, it looks like AlphaGo \"came to play.\" Here's hoping that Sedol was simply underestimating AlphaGo and brings his \"A\" game to the next one.  The next game is tomorrow, so if Sedol doesn't get it together and win tomorrow..."], [241, "2016-03-09T07:39:15Z", [0.25], ""], [19770, "2016-03-09T07:39:07Z", [0.95], ""], [14240, "2016-03-09T07:38:46Z", [0.95], ""], [498, "2016-03-09T07:35:58Z", [0.2], ""], [20562, "2016-03-09T07:35:31Z", [0.99], "Gonna up my estimate from 87% and add a told ya so :D."], [10759, "2016-03-09T07:33:49Z", [1.0], ""], [4634, "2016-03-09T07:33:17Z", [1.0], "AlphaGo wins the first round (Amazing!)"], [306, "2016-03-09T07:32:40Z", [1.0], ""], [19770, "2016-03-09T07:32:39Z", [0.85], ""], [1122, "2016-03-09T07:26:45Z", [0.55], ""], [2010, "2016-03-09T07:15:07Z", [0.7], ""], [2010, "2016-03-09T07:07:52Z", [0.5], ""], [15471, "2016-03-09T06:51:11Z", [0.7], ""], [20129, "2016-03-09T06:34:30Z", [0.85], ""], [2010, "2016-03-09T06:04:53Z", [0.0], ""], [2010, "2016-03-09T05:15:16Z", [0.2], ""], [2010, "2016-03-09T04:43:17Z", [0.5], ""], [498, "2016-03-09T04:06:17Z", [0.1], "Game 1 Livestream\nhttps://www.youtube.com/watch?v=vFr3K2DORc8&internalcountrycode=JP"], [1529, "2016-03-09T03:35:43Z", [0.55], ""], [19936, "2016-03-09T02:30:05Z", [0.95], "Google are putting up 1M as a prize, have a big team on it, and their machine can scale with more hardware. Additionally, Sedol has few prior games to study, the previous with Fan did not exhibit the potential of the machine because it is defensive when it is ahead. It's likely to move fast and not give Sedol time to think. Kasparov in a similar situation became very nervous and Sedol admitted that it would feel like \"playing against himself.\" The real test will be in the later matches (March 10 Korean time) as the two players adapt to each other."], [767, "2016-03-09T01:01:08Z", [0.35], "At the press conference today, Lee Sedol was a lot less confident than he has previously shown to be, and said that, after learning more about the algorithm that Deepmind is using for AlphaGo, he no longer thinks that a 5-0 win is possible. http://www.koreaherald.com/view.php?ud=20160308000967 Demis Hassabis, DeepMind's founder, admitted just last month that there is \"no clear consensus on who will win\" while speaking at the Association for the Advancement of Artificial Intelligence conference.  http://www.techrepublic.com/article/can-googles-deepmind-beat-world-go-champ-watch-live-this-week/  The same article linked above that cites Hassabi, mentions several experts in the Artificial Intelligence community who predict that ALPHAGO will prevail. \nRoman Yampolskiy, director of the Cybersecurity lab at the University of Louisville, believes ALPHAGO will win.  He says that since ALPHAGO has had five extra months to improve its game and that it can be trained to specifically against Lee Sedol's playing style.  He also thinks, as others have expressed here, that \"Google is not in the business of losing million dollar prizes while promoting an embarrassing loss worldwide.\"   I'm adjusting the forecast from 25 to 35.  Many have indicated Lee SeDol is most likely to win the first game. We will see how it goes after tonight. "], [1107, "2016-03-09T00:25:34Z", [0.99], "The algorithm has the equivalent of 80 years of experience playing the game.\nhttp://www.businessinsider.com/googles-alphago-is-making-artifical-intelligence-history-2016-3 "], [21853, "2016-03-08T22:09:02Z", [0.75], ""], [443, "2016-03-08T21:31:58Z", [0.35], ""], [767, "2016-03-08T19:12:27Z", [0.25], "We all know computers AI capability is exponentially increasing.  Despite the great team and resources behind ALPHAGO, I still think the human --this time around-- has the edge to beat it.  Much must have been learned and adjusted since ALPHAGO beat Fan Hui in October (the player ranked 537 in the world).  Fan Hui became one of the Deepmind advisors in programming ALPHAGO to teach it the game of Go.   The Scientific American article linked below discusses the reason computers have a hard time with Go:  Go players need a large pool of knowledge\u2014past experiences with the game\u2014to draw from.  In training, ALPHAGO has played against itself millions of times but does that really account for the similar type of experience gained from playing against other's people's intuition?  There are just two rules to GO, but it is still considered one of the hardest games for machines to play. It combines logic with intuition and there are more possible configurations of the board than there are atoms in the universe.   It's notable that an AI system like ALPHAGO wasn't expected to beat a human at GO for another 10 years. Scientific American quoted Deepmind's developer, Demis Hassabis, saying that in a game of chess there is an average 20 possible moves per turn whereas for Go in each turn an average 200 moves are possible. \"This means that if a computer were to search all the possible moves and outcomes in Go, it would take an enormous amount of computing power to do so, one that some say may not even be possible.\"  But this is also a great indicator that the ALPHAGO team is very clear on all of the system's vulnerabilities and weaknesses and more likely than not has had the time and resources to fix them.  AlphaGo's team solved the systems' knowledge problem with \"deep neural networks.\"  They are using two 13-layer-deep networks consisting of millions of connections, similar to the neural connections in the human brain. The team then trained those networks showing it \"more than 30 million moves from games played by human experts\" which was meant to help the system learn how the best players win, and the also had the computer play thousands of games with itself so it could discover new strategies and learn the game on its own. These two strategies presumably allowed AlphaGo to recognize patterns in the game and identify what moves gave it the best chance of winning.  The result was a new version that could beat its old self 80-90% of the time. ALPHAGO was also trained to use statistics as a shortcut to determine the best move, rather than playing out each and every possible outcome of a given move which in this game not only could take forever but it could also present a problem with the issue of timed games.   LEE SEDOL, as of February 2016, is ranked 2nd in the world. He is know for using the Broken Ladder formation method which is considered bad beginners play but he has defied all conventional wisdom in the game by making the strategy successful in his games.  His record is of 472 wins, 185 losses, 0 jigos (tied games) for a 71.8% winning percentage. I am still betting --this time-- on human intuition and on LEE SEDOL for the particular challenges to AI computers that the game of GO represents.  One has to take into account the human factors that may affect the human in the game, but he is presumably experienced (playing professionally since 1995) in this type of matches.  One thing worth considering is that the ALPHAGO team will have the time to adjust their system after each game and improve it for the next.   http://www.scientificamerican.com/article/computer-beats-go-champion-for-first-time/"], [691, "2016-03-08T18:49:42Z", [0.66], ""], [4712, "2016-03-08T17:07:10Z", [0.31], ""], [2010, "2016-03-08T17:07:04Z", [0.02], ""], [1529, "2016-03-08T16:52:20Z", [0.45], "EDIT (There can be no tie, so ignore this >>>>>(IFP requires AphaGo to \"beat\" Sedol, so a tie goes to Sedol for this IFP. Based on that aspect, lowering %) First game will be significant."], [12413, "2016-03-08T16:45:49Z", [0.65], "I have put my prediction back upwards, to 65% (originally 65%, then to 45% now 60%).  I feel I have underestimated the exponential learning capacity of the AI.  It is almost impossible to comprehend but the AI learning speed is astounding.  This is the advantage.  If you chart the learning speed of the AI, the AI can get exponentially better (the line curves upwards towards vertical) while if you took a toddler and taught him or her for a lifetime, the curve would be the opposite - starting almost vertically and then gradually pushing towards horizontal.   As humans we think too linear a fashion. "], [1106, "2016-03-08T15:38:23Z", [0.05], "According to other professional players, AlphaGo has not reach the top standard of professional player yet. it may win 1 or 2 games by chance"], [1218, "2016-03-08T15:25:18Z", [0.22], "From the American Go Journal - WATCH IT LIVE: the first Game will tell us all a lot! http://www.usgo.org/news/2016/03/alphago-vs-lee-sedol-match-schedule-and-details/ American Go E-Journal AlphaGo vs Lee Sedol: Match schedule and details Tuesday March 8, 2016 The much anticipated five game match between Lee Sedol 9P and Google DeepMind\u2019s AlphaGo begins this week, on Wednesday, March 9 2016.03.02_DeepMind - YouTube(March 8 for American viewers). Here is the match schedule, along with details of how you can watch and timezone conversions, courtesy Go Game Guru.\nThe first game in the Lee Sedol-AlphaGo match will be Tuesday, March 8, 8p PST (11p EST). The match will be livestreamed on DeepMind\u2019s YouTube channel with English commentary by Michael Redmond 9p with American Go E-Journal Managing Editor Chris Garlock.\n"], [12874, "2016-03-08T14:31:06Z", [0.41], ""], [4288, "2016-03-08T14:30:26Z", [0.69], "Based on Lee Sedol's comments. "], [21817, "2016-03-08T12:23:30Z", [0.35], "While AlphaGo reaches professional Go player level, it is not up to the level of top human players yet."], [21814, "2016-03-08T11:46:07Z", [0.4], "Comment deleted on May 11, 2020 09:00AM UTC"], [15621, "2016-03-08T11:04:49Z", [0.67], ""], [1468, "2016-03-08T10:45:59Z", [0.23], ""], [17819, "2016-03-08T09:56:49Z", [0.55], ""], [21806, "2016-03-08T07:50:07Z", [0.8], ""], [19770, "2016-03-08T06:03:27Z", [0.72], ""], [102, "2016-03-08T05:01:34Z", [0.65], "https://larswericson.wordpress.com/2016/03/08/gitrep-7mar16pm/"], [20671, "2016-03-08T04:50:29Z", [0.85], ""], [1646, "2016-03-08T00:55:27Z", [0.45], ""], [11740, "2016-03-07T22:26:36Z", [0.39], ""], [19170, "2016-03-07T22:14:52Z", [0.2], ""], [21758, "2016-03-07T21:30:25Z", [0.87], ""], [3320, "2016-03-07T20:23:10Z", [0.1], "This isn't very rational, but as a go player, I'm all too familiar with how a small difference in skill translates into consistent wins for the stronger player.  The difference in strength between Lee Sedol and the pro AlphaGo defeated is quite large.  There's no question that AlphaGo represents a huge breakthrough in AI, but a 9-dan pro can create complexities for which the games stored in the AlphaGo database offer no basis for optimizing next moves.  "], [20740, "2016-03-07T19:39:22Z", [0.8], ""], [14841, "2016-03-07T18:53:15Z", [0.55], ""], [21692, "2016-03-07T18:43:44Z", [0.87], ""], [823, "2016-03-07T18:18:06Z", [0.8], ""], [21710, "2016-03-07T17:41:56Z", [0.35], "Complex game. If Google wins wil show a remarkable growth in AI."], [21710, "2016-03-07T17:40:06Z", [0.65], "Complex game. If Google wins, it will show the incredible growth on their AI."], [21649, "2016-03-07T16:35:29Z", [0.95], "The loss is worth less to the man, than a win is worth to Google here. AKA GOOG.   Not to mention, as referenced around here, Deep Blue was how many years ago? I am unfamiliar with GO but I would not assume the complexity increase compared to Chess is insurmountable, possibly even by Deep Blue given modern programming languages."], [21687, "2016-03-07T16:08:04Z", [0.75], "They are leaders in AI (deep learning) and managed to create a champion winning chess AI."], [19170, "2016-03-07T15:55:16Z", [0.3], ""], [21653, "2016-03-07T15:28:55Z", [0.85], "Google wouldn't play this match if they weren't very confident they would win. They've had multiple months of training since last recorded ELO"], [14777, "2016-03-07T15:05:52Z", [0.05], ""], [21620, "2016-03-07T14:11:28Z", [0.78], "See Deep Blue vs. Gary Kasparov"], [691, "2016-03-07T13:15:45Z", [0.68], ""], [21590, "2016-03-07T12:55:53Z", [0.77], ""], [10762, "2016-03-07T12:22:11Z", [0.35], ""], [21566, "2016-03-07T12:13:13Z", [0.55], ""], [21562, "2016-03-07T11:51:18Z", [0.75], ""], [14702, "2016-03-07T10:43:33Z", [0.3], "I think Lee Sedol will win this time.  In the future, the computer program will learn from its mistakes and beat him.  Alpha Go beat Fan Hui, who is ranked 2-dan.  Lee Sedol is ranked 9-dan and has been world champion for a decade.  He is a stronger player than Fan Hui.  Alpha Go beat Fan Hui 5 games to zero."], [21550, "2016-03-07T09:13:36Z", [0.4], ""], [19954, "2016-03-07T09:06:03Z", [0.99], ""], [11080, "2016-03-07T06:41:46Z", [0.28], ""], [11080, "2016-03-07T06:41:28Z", [0.26], ""], [14084, "2016-03-07T05:06:14Z", [0.99], ""], [21323, "2016-03-07T03:52:30Z", [0.7], ""], [14240, "2016-03-07T02:50:57Z", [0.73], ""], [12874, "2016-03-07T00:51:49Z", [0.46], ""], [94, "2016-03-06T23:24:56Z", [0.35], "Affirming This is really weak evidence, but someone on Reddit says they've been setting up bets with AlphaGo as a 2:1 underdog (see https://www.reddit.com/r/baduk/comments/44oaf1/where_can_i_bet_on_the_lee_sedol_vs_alphago_match/)."], [19650, "2016-03-06T23:03:27Z", [0.7], ""], [974, "2016-03-06T23:01:25Z", [0.15], ""], [21427, "2016-03-06T22:23:38Z", [0.34], ""], [20698, "2016-03-06T21:51:58Z", [0.16], ""], [21415, "2016-03-06T21:27:28Z", [0.65], ""], [14327, "2016-03-06T21:21:03Z", [0.85], "http://www.tomshardware.com/news/google-alphago-vs-lee-se-dol,31142.html \"The real breakthrough was that AlphaGo learned on its own how to play Go by \u201cwatching\u201d 30 million Go moves played by human experts. AlphaGo is merely a version of DeepMind, which Google used to train the AI how to play Go, but in reality DeepMind is actually an \u201cAGI,\u201d or artificial general intelligence. Google has talked before about how it has been training it to learn arcade games from the 70s and 80s, as well as the Doom game from the 90s. The DeepMind agent is not pre-programmed to play any of those games, including Go. It actually learns everything from scratch, even the game rules or how to use \u201cbutton actions\u201d during a video game. It initially fails all the time until it eventually figures out what are the moves that \u201creward\u201d it with moving forward into the game.\" ~~~~~~~~\nFor all those worrying about the mistakes AlphaGo made in its match against Hui - this seems to suggest that those mistakes were genuinely \"errors in judgement\" and not code that needs to be re-structured.  Anyway - humans make errors, too. "], [21408, "2016-03-06T21:17:20Z", [0.79], "Seems to have experience with previous wins and time since then to develop strategy"], [21403, "2016-03-06T21:07:27Z", [0.85], ""], [10762, "2016-03-06T20:35:02Z", [0.3], ""], [21371, "2016-03-06T19:35:12Z", [0.15], ""], [21336, "2016-03-06T19:21:20Z", [0.87], ""], [21335, "2016-03-06T18:41:43Z", [0.89], "AlhaGo already has some feathers on it's cap. Moreover, it must have researched all the possibilities that can arise in Lee Sedol's mind. Be that as it may, the game offers Lee Sedol great deal of possibilities. Nevertheless the odds are stacked against him as he would only be able to go so far in tapping into the possibilities offered by Go. Hence, I wouldn't give Lee Sedol higher than 11% chance."], [21308, "2016-03-06T17:40:30Z", [1.0], ""], [5001, "2016-03-06T17:23:29Z", [0.25], ""], [21274, "2016-03-06T16:35:12Z", [0.65], ""], [21211, "2016-03-06T15:44:25Z", [0.75], ""], [21221, "2016-03-06T15:32:08Z", [0.85], ""], [691, "2016-03-06T15:27:47Z", [0.68], ""], [10039, "2016-03-06T14:08:37Z", [0.0], ""], [4354, "2016-03-06T12:57:50Z", [0.5], ""], [10759, "2016-03-06T10:32:13Z", [0.22], ""], [4634, "2016-03-06T10:32:09Z", [0.22], ""], [21169, "2016-03-06T09:20:24Z", [0.1], ""], [20802, "2016-03-06T09:18:43Z", [0.95], ""], [689, "2016-03-06T08:44:58Z", [0.45], ""], [21168, "2016-03-06T08:04:25Z", [0.4], ""], [17875, "2016-03-06T07:56:56Z", [0.0], ""], [20901, "2016-03-06T04:50:22Z", [0.75], ""], [102, "2016-03-06T04:01:33Z", [0.66], "https://larswericson.wordpress.com/2016/03/05/manual-category-assignment-work-in-progress/"], [20901, "2016-03-06T03:00:39Z", [0.75], ""], [13859, "2016-03-06T02:24:05Z", [0.95], ""], [1354, "2016-03-06T01:56:21Z", [0.5], ""], [21145, "2016-03-05T22:53:24Z", [0.85], ""], [11872, "2016-03-05T22:17:38Z", [0.25], ""], [20343, "2016-03-05T22:01:57Z", [0.6], "Need to understand what does it take to win a typical \"Go\" match and then assess the probability of a computer doing better than a human: Start with an equal 50-50 chance between human and Google's AlphaGo. Then let's looks at what is takes to win: Strategy and analyzing power: per wikipedia, Go is a strategy game with a total of 10^761 plays. Compared to the game of Chess with 10^120, Go has much more game plans to consider and evaluate. The higher the  number of play the more analytical power required. AlphaGo has the higher hand in being able to assess larger volumes of plays. So AlphaGo gets 10%. Probability is now 60-40 with AlphaGo being the winner. Next, is the consistency in analyzing and strategyzing. Again according to Wikipedia a tournament game of Go can take between 1-6 hours. That means computer has a higher hand because it maintain consistency in analysis at all time. A human can get tired and that impacts consistency and quality of analysis specially as game lasts longer. So AlphaGo gets another 10% point. Probability is 70-40 with AlphaGo being the winner. The other element that could impact winning is the role of non fact based variables. For example in Poker you need to guess things. Some other games your hand  depends on the roll of dice. In the game of Go, there are no such elements. It is a perfect information situation and again a computer have a higher hand, because computer has a higher fact-based analysis power compared to a human. I give another 10% to AlphaGo. We are not at 80-20 to the favour of AlphaGo. Now let's look at how strong is the AlphaGo's analytical power. Based on information on deepmind.com, the computer algorithms havnt been as successful as one might think. The new program that is used for the upcoming game is a new approach that \"combines Monte-Carlo tree search with deep neural networks that have been trained by supervised learning, from human expert games, and by reinforcement learning from games of self-play.\" So let's see how good are these \"neutral networks\" are? They are algorithm that are self-trained.  This means they learn from their past mistakes and get better as they go. Now, these algorithm are trying to replicate what an experienced player can do. Since they are still in development phase and I havnt seen examples of super functioning algorithms just yet (based on investopeida.com these algorithms at best have improved efficiency in decision making by 10%) I will give wold champion Lee Sedol 10% back. He is probably well a head of any algorithm from the perspective of learning from past. So score is now 70-30 still to the favour of AlphaGo. Another factor is also the AlphaGo hasnt been able to win a pro player like Lee sedol (who has 9 ranking) and AlphaGo has only won level 5 players. It means, there are moves it hasnt seen yet and as such hasn't been trained at the professional level. So I give another 10% to Lee for his professional ranking of 9 against AlphaGo's proxy ranking of 5. We are now at 60-40 to the favour of AlphaGo. I also think we need to explore if there is anything special about Lee that might impact the results? I cant find much rather than he likes to look into the opponents eyes. And that's doesnt help here. Also he is confident that he can win but also want to retire to go to the US and promote Go. So cant find much about his ability to win beyond what have stated about. So final score is 60% probability of winning by AlphaGo.\n"], [20882, "2016-03-05T21:19:02Z", [0.4], "I read Myungwan Kim's assessment on AlphaGo and am sure the Google team has been working hard to address the identified weaknesses.  Will 5 months be enough to for AlphaGo to beat Lee Se-dol?  I don't think so, however, I do expect AlphaGO to win 1 or 2 games."], [20230, "2016-03-05T21:08:30Z", [0.3], ""], [21116, "2016-03-05T20:56:32Z", [0.75], ""], [21104, "2016-03-05T18:50:02Z", [0.75], ""], [21128, "2016-03-05T17:59:38Z", [0.95], "I belive artificial intelligence has reached a level whereby cognitive abilities can mimic the human mind, but much faster.  As a result, I believe the probability of any human beating a computer is decreasing.  Google wouldn't play to lose."], [13361, "2016-03-05T17:37:01Z", [0.48], "OK, since the games take place next week (March 9-15), I'll update my forecast. It looks like the biggest question is how good has AlphaGo gotten in 6 months since October & no one except Google knows that answer & they aren't giving people any clues. The general Go community consensus was that it couldn't beat Lee in October. Now, it seems they are in the too close to call camp or giving one or the other a slight advantage. I will give Lee the slight advantage - only because AlphaGo is a black box at this point & I think it's impossible to predict how much progress it has made. https://gogameguru.com/can-alphago-defeat-lee-sedol/\nhttp://www.milesbrundage.com/blog-posts/alphago-and-ai-progress"], [73, "2016-03-05T16:10:10Z", [0.49], ""], [1474, "2016-03-05T16:02:00Z", [0.96], "European Go champion was defeated 5-0 as outlined in the Nature paper published in Jan. Reinforcement learning is a genuinely useful set of mathematical methods,  which continues to show great levels of progress. With these set of games set to receive similar levels of attention as the Kasparov / Deep Blue series of chess games in 1996/97, Demis Hassabis has every incentive to ensure that AlphaGo will defeat Lee Sedol."], [691, "2016-03-05T13:58:14Z", [0.68], ""], [21100, "2016-03-05T12:43:27Z", [0.75], ""], [15621, "2016-03-05T10:06:01Z", [0.64], ""], [15621, "2016-03-05T09:40:03Z", [0.61], "Just want to stay slightly on side of computer vs gjo predictions"], [18914, "2016-03-05T00:25:44Z", [0.85], "Being a go player, I REALLY REALLY hoped that Lee Sedol would win. One of the biggest reasons I started go was because bots could not defeat the top players in go.  But after reading more, I can only come to a sad conclusion. The only people in the world right now that have all the relevant data is the deepmind team, and they are confident of a victory.  One of the biggest potential pitfalls for the AlphaGo team is that they might have underestimated the difference between Fan Hui and Lee Sedol, and didn't understand the go world enough. Indeed, it does seem quite convoluted, and there isn't a real international ranking. Even \"Professional players\" have different standards across the world; it isn't like chess where a 2500 ELO will make you a GM. In different parts of the world, there are different ways to become a \"pro\", and so their strengths vary widely.  But after reading more, it seems that Demis is fully aware of Lee Sedol's strength. And now that they have Aja Huang and Fan Hui on the team, it's laughable to still be under the delusion that I know more about the go environment than they do. And so, pretty much the only potential for error* is covered, I have to objectively say that AlphaGo will likely win, 5-0. I really hope I'm wrong. But objectively speaking, I think go players should probably face the inevitable now. *The only other thing that might make the games more interesting is if DeepMind decides to test out the accuracy of their strength evaluation. What do I mean?  Well, let's say that they predict that Lee Sedol is around 3500 ELO, and that AlphaGo with 10,000 CPU/GPU is 4000 ELO. But there's no need for such an overkill, right? So maybe they'll choose to use 1,500 CPU/GPUs instead, which will make AlphaGo's ELO be around 3525, so still a bit higher than Lee Sedol. Then they can see just how good their ELO guessing is, since if AlphaGo just easily 5-0s, you can't really determine an upperbound for the ELO.  Then if Lee Sedol gets lucky, he might win. "], [691, "2016-03-04T20:49:23Z", [0.59], ""], [11885, "2016-03-04T20:47:14Z", [0.0], ""], [3474, "2016-03-04T18:55:55Z", [0.15], "Computers are not good at fuzzy logic."], [16272, "2016-03-04T18:43:15Z", [0.35], ""], [17007, "2016-03-04T18:35:47Z", [0.75], ""], [19199, "2016-03-04T17:56:17Z", [0.25], ""], [19198, "2016-03-04T17:52:53Z", [0.32], ""], [19200, "2016-03-04T17:52:06Z", [0.2], ""], [19197, "2016-03-04T17:28:01Z", [0.15], ""], [21066, "2016-03-04T16:19:30Z", [0.05], ""], [14841, "2016-03-04T11:44:34Z", [0.5], ""], [1468, "2016-03-04T10:12:22Z", [0.47], ""], [21012, "2016-03-04T09:54:37Z", [0.75], "In October 2015, the distributed version of AlphaGo defeated the European Go champion, and since then Google team had several months to tweak and optimize the software. Moreover the machine will not suffer from pressure potentially put on by its opponent with aggressive moves."], [20226, "2016-03-04T09:04:48Z", [0.67], ""], [13377, "2016-03-04T08:33:16Z", [0.35], "https://larswericson.wordpress.com/"], [14646, "2016-03-04T06:22:43Z", [0.74], ""], [102, "2016-03-04T05:29:29Z", [0.35], ""], [14327, "2016-03-04T04:36:32Z", [0.8], "Moving up based on @eron_gj 's / @aarongertler 's great link."], [19469, "2016-03-04T03:54:20Z", [0.07], ""], [21040, "2016-03-04T02:56:19Z", [0.65], ""], [17875, "2016-03-04T01:18:55Z", [0.0], ": )"], [14253, "2016-03-04T01:01:53Z", [0.85], ""], [436, "2016-03-04T00:45:51Z", [0.54], "Boosting my prediction based on this article, which taught me more about AlphaGo's ability to bring more processing power to bear in certain situations. I predicted earlier that Lee Sedol will think more carefully than Fan Hui did, given the world's attention on the match -- but AlphaGo may get a similar advantage if certain \"kid gloves\" exist to be taken off. http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress"], [12874, "2016-03-03T18:24:29Z", [0.52], ""], [2010, "2016-03-03T18:00:27Z", [0.02], ""], [21013, "2016-03-03T16:31:39Z", [0.7], "http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress"], [21000, "2016-03-03T14:12:23Z", [0.85], "It's not a question of processing power its a question of developing suitable heuristics and now that teams are working ones that feedback into play (e.g. learn) its just a question of time before outstripping the best humans is achieved"], [19432, "2016-03-03T10:39:14Z", [0.58], "Alpha go has, as of now won 499/500 of it games with a computer software and all of its games with humans . Thus it has. Baseline win rate of 99.8%. Lee See dol has a 71.8% baseline win rate as per Wikipedia . Thus the balance is slightly tilts towards alpha go .  Given that the game has a big human component to it, I would be cautious to start with and say that the computer has a slightly higher chance of beating lee see dol than vice versa. "], [20501, "2016-03-03T10:34:07Z", [0.6], "Here's my take as a casual interested observer who tried to read the paper in Nature to understand how AlphaGo works and challenges faced. - Number of possible different games is given by b^d\n- b: breadth is the number of legal moves per turn\n- d: depth is the number of turns\n- For chess, b=35 d=80 approx\n- For go, b=250, d=150 approx\n- Both games are huge and impossible to brute force search the entire game tree to solve but Go is astronomical orders of magnitude bigger\n- Chess engines can far surpass humans in tactical and endgame play because the breadth and especially depth of games becomes more tractable after the opening. This is favourable because there exists an edifice of book opening  theory which engines can just be fed to see them into a reasonable middlegame. Engines by themselves would be much weaker at positional play and understanding longterm positional advantages created in the opening. As a result you see chess engines not only far surpass the best human players, but do it while often finding extremely powerful moves through their own calculation that to any human would seem counterintuitive or unnatural.\n- Because Go is so much bigger, the ability to evaluate positions and find the strongest move is diminished by the practical need to consider a very limited subset of possible moves on any given turn, and look ahead fewer moves (pruning both depth and breadth to make it manageable). For the same reason the 'value function', an evaluator of a position to tell you who's winning, is also a much weaker approximator of the true theoretical value function that tells you who would win from any position if both sides played perfectly, than chess positional evaluators.\n- In order to intelligently prune breadth of search tree, AlphaGo starts with a neural network that looks at hundreds of thousands of expert level games played on KGS to learn how good humans play. After this stage its network can predict the next move a human will play from any position with 57% accuracy\n- To go from predicting human moves to its goal of winning games, it refines its neural network by playing large amounts of games against itself so it can learn by itself which moves end up doing better and which do worse. As a result of this experience it simultaneously trains another neural network, its 'value network' that learns over time to predict which side will win from any given position. In a real game it uses both these networks simultaneously, with policy network searching forward a few turns considering only 'good moves' from each side and value network approximating how good each side will be after a few turns for the position that results from a given combination of the next few moves. In effect, AlphaGo learns from and refines human play from the initial set of human games it trained from, to achieve a level that might be like the best of the human players playing on a really good day where they make no blunders and see things well. Contrast with Chess engines that can play unhumanlike and counterintuitive because they find things humans didn't see by being able to search a much higher % of the overall game tree. Commentary from the expert Go community seems to back up this view - commentators noted being surprised how human-like AlphaGo was and how they may not be able to tell which was the human and which was the computer if seeing the game without knowing. It seems the initial games trained from KGS go server were below the professional level. According to someone on Reddit they were 6-9dan KGS games, where 8-9 dan is roughly thought to translate to 1-2P (professional dan) level. Fan Hui is a 2P which would make him similar level to the best of KGS games so the program managed to learn and improve convincingly beyond the level of players it was initially trained on, but the key question is how much more? My feeling is that AlphaGo is bound by a human limit of the games it was trained on due to how it works, in a way chess engines are not. The opinions of top level 9P Go Pros carry weight here, having reviewed the games they are convinced the AlphaGo that played Fan Hui has no hope of beating Lee Sedol in a match who is considered a far far higher level. Additionally it was commented AlphaGo plays with a more passive 'Japanese style' of play which does worse in international competitions than the modern way South Koreans play. I believe that unless the AlphaGo team find better games to train the bot on, it would take much more than 5 months of 24/7 reinforcement learning on distrubted systems (could easily be more than 500 years!) to surpass Lee Sedol because of the hard limits of only mainly considering human moves made by much weaker human players that play a different style. However the main chance for AlphaGo is the very real possibility they will find much better databases of top professional level games to retrain the neural networks on. A quick search of google reveals they certainly exist. KGS games used previously are freely available where as pro games are commercially available. Considering I was able to find out the reaction of Go pros and put this together, id say it's high probability the AlphaGo guys will train with better inputs. They have a big investment in the project and the eyes of the world are on them - I imagine they'll be putting manpower and resources into thinking of ways to improve and maximise the probability of winning."], [855, "2016-03-03T08:52:18Z", [0.55], "Frankly I think it could go either way, given Sedol's level and AlphaGo's continued ability to learn.  Slight advantage to the machine...."], [20129, "2016-03-03T08:45:08Z", [0.69], ""], [20994, "2016-03-03T08:04:00Z", [0.2], ""], [102, "2016-03-03T04:53:51Z", [0.33], ""], [1450, "2016-03-03T02:51:20Z", [0.35], ""], [19352, "2016-03-03T02:19:14Z", [0.85], ""], [20981, "2016-03-03T00:14:37Z", [0.85], ""], [1646, "2016-03-03T00:06:49Z", [0.25], ""], [498, "2016-03-02T23:39:02Z", [0.1], "Per German poll cited by @dada: http://www.gjopen.com/comments/comments/155178 (-2)"], [20448, "2016-03-02T21:52:49Z", [0.7], ""], [15870, "2016-03-02T19:14:16Z", [0.45], ""], [691, "2016-03-02T18:57:52Z", [0.63], ""], [12770, "2016-03-02T18:51:23Z", [0.75], ""], [14777, "2016-03-02T18:16:12Z", [0.08], ""], [20562, "2016-03-02T16:24:53Z", [0.87], "I am increasing my estimate to 87% after this comment alleviated some potential concerns. http://www.gjopen.com/comments/comments/152961"], [138, "2016-03-02T14:56:27Z", [0.4], ""], [20892, "2016-03-02T14:49:48Z", [0.53], "HP"], [12413, "2016-03-02T14:33:27Z", [0.45], "I'm lowering my forecast to 45% from 65% this is a drastic change but after reading up on this it seems to me the weakness in Alphago right now is that it hasn't played versus many human players.  "], [19815, "2016-03-02T09:31:03Z", [0.68], ""], [18914, "2016-03-02T06:31:30Z", [0.5], "For people still skeptical about what I said in all my posts, and the difference between western and Asian players, here's a short article about a recent go tournament that gives a little idea of the go world: http://www.usgo.org/news/2016/03/korea-china-win-at-iemg-with-na-players-in-5th-place-li-scores-against-japan-pro/ I have not watched this yet, but it seems that there is a part about AlphaGo in this interview: https://www.youtube.com/watch?v=DHkkR_wM--c&feature=youtu.be EDIT: Wu Guangya, a top Chinese pro just said today that he also thinks that it's probably be 5-0 either way, he doesn't think it'll be a close, intense battle. (but he thinks it's more likely it'll be Lee Sedol who will win). "], [20919, "2016-03-02T06:04:50Z", [0.68], ""], [20912, "2016-03-02T04:45:29Z", [0.95], "AlphaGo was stronger than a third dan last fall, and it has been constantly improving since then."], [20911, "2016-03-02T03:53:36Z", [0.75], ""], [19427, "2016-03-02T00:39:01Z", [0.7], ""], [1646, "2016-03-02T00:01:11Z", [0.32], "German Poll\nhttp://computer-go.org/pipermail/computer-go/2016-March/008741.html\nYou can join here\nhttp://www.go-baduk-weiqi.de/gewinnspiel-lee-sedol-gegen-alphago/"], [691, "2016-03-01T22:25:10Z", [0.64], ""], [20889, "2016-03-01T21:42:10Z", [0.0], ""], [16025, "2016-03-01T20:21:47Z", [0.57], "C Over the course of time, even the smartest artificial intelligence devices have not been able to beat GO amateurs, however, Google\u2019s own AlphaGO has had a recent history of success, going 5-0 in a match against a European professional, and has beaten the vast majority of AI systems by a wide margin. AlphaGo has been programmed to \u201cmimic\u201d expert players and seems to be slightly favored to win the match against Lee Sedol. \nH Though an artificial system beating a human in GO has not happened as of yet, there is definite information to be discussed. Google has trained its AlphaGo system to memorize 30 million moves that were played by professional GO players, and it can now sensibly predict the next human move approximately 57 percent of the time. This was achieved through reinforcement learning, a form of learning that involves implementing new techniques and strategies through trial-and-error. \nA The most recent forecast for predicting AlphaGo\u2019s performance against world champion Lee Sedol was 55%, however the new forecast is 45%. There are several reasons for why AlphaGo may not actually have an edge in beating Lee Sedol. It appears as though Lee Sedol is a much stronger player than Fan Hui relative to AlphaGo. Elo ratings predict that Fan Hui would have a 25% chance of beating Lee in just one match. In addition, after his match against AlphaGo, Fan Hui noted that he wasn\u2019t able to play his conventional style against the computer, which may have actually contributed to his poor performance. In essence, AlphaGo\u2019s victory was impressive, however they may have a tougher time with Lee Sedol than anticipated. \nM Though no models have directly predicted AlphaGo\u2019s strength as a Go player, one may consider Fan Hui and Lee Sedol\u2019s Elo ratings in accordance to potential adjustments AlphaGo can make prior to the next match. Sedol\u2019s 2940 rating in comparison to Fan Hui\u2019s 2750 may not seem like that big of a difference, however, may professional\u2019s agree that Sedol is much stronger than a 2940 rating. AlphaGo can potentially play from a database of 100 million moves for its next match. \nP AlphaGo and Lee Sedol have not formally played their match, making it difficult to provide post-mortem analysis. For the time being, one can imagine that AlphaGo developers have been continuing to develop its intelligence through simulations against expert players. AlphaGo\u2019s continuing development should make for an interesting and exciting match against Sedol, with forecasts locked near a dead tie.\nS The most pressing questions that relate to this forecast are: How impressive was AlphaGo\u2019s win against Fan Hui? Will the match be set up to automatically favor AlphaGo? This could easily affect Sedol\u2019s playing style but may not actively reflect one of his better performances. Also, will AlphaGo\u2019s ever-expanding intelligence lead to a significant advantage over Sedol?\nK The key players in this situation have to be the Google developers. If AlphaGo play\u2019s how it played against Fan Hui (with a substantial amount of mistakes), it could have a tough match against Sedol. However, if Google developers can take the information from the match and implement it to make AlphaGo even stronger, it will make for a much better match. \nN Important policies that may swing the match in AlphaGo\u2019s favor are the rollout policy and rules on time limits. First, the match is likely to have a greater time limit for each player. This works in AlphaGo\u2019s favor because it gives the computers more time to make calculated and accurate moves. In addition, the rollout policy allows for AlphaGo developers to implement more training into their computers and use more power during the match. \nO Many articles have excluded the confirmed fact that AlphaGo lost 2 unofficial matches against FanHui. Is AlphaGo as good as people say it is? \nW Google developers have expressed confidence in their artificial intelligence systems and likely have some \u201ctricks up their sleeve\u201d for the match, which brings some unpredictability from AlphaGo\u2019s side. Everyone pretty much knows what to expect from Lee Sedol: high performance from the consensus number 1 go player for the last decade. Whether Google can significantly enhance its technology will be a driving factor in catching Sedol off guard and delivering a strong performance in March.\n"], [20766, "2016-03-01T20:11:26Z", [1.0], ""], [20206, "2016-03-01T17:39:08Z", [0.63], ""], [20833, "2016-03-01T15:12:23Z", [0.15], ""], [20692, "2016-03-01T11:01:56Z", [0.87], ""], [20844, "2016-03-01T08:23:35Z", [0.33], ""], [20834, "2016-03-01T06:00:04Z", [0.99], ""], [20833, "2016-03-01T05:04:15Z", [0.75], "Google is conscious of its own image. It is unlikely to roll this out until it can win."], [19513, "2016-03-01T03:57:08Z", [0.42], ""], [20053, "2016-03-01T03:55:42Z", [0.65], ""], [19170, "2016-02-29T23:08:04Z", [0.35], ""], [498, "2016-02-29T22:49:38Z", [0.12], " -1 on @Investmentitos' intimation that *real* Go pros are siding with Sedol, and adopting the heuristic that when you don't really know crap about an issue, it's OK to \"go with the pros\" (provided you can identify the *real* pros, and that you can adjust for whatever biases the real pros may have relative to your payout [which, as I've learned personally the hard way, is not the same as the outcome].)\nhttp://www.gjopen.com/comments/comments/153868"], [11289, "2016-02-29T22:30:31Z", [0.1], ""], [19150, "2016-02-29T22:18:42Z", [0.58], "It's hard to take an outside view on this question, because directly comparable situations don't exist. We can look at how computers have beaten world experts in chess and jeopardy, but GO is a level more complicated and tests the strength of artificial intelligence even further. In chess, it took years after the first chess-master tournament for the computer to beat the world expert. This is experience AlphaGo doesn't have.   It's also important to pay attention to the role of luck in this situation. When skills levels are sooo high, like they are between the world expert and the machine, luck begins to play a greater role in determining the outcome. By betting on the computer, I would be saying that the computer is capable of reaching a higher skill level than the best human mind that is likely to overcome the luck in the game. I decided to rule in favor of the computer, because the champion said that when he observed the October match of the computer, he thought that the computer was only slightly lower in skill level. Having heard the expert admit to only a slight advantage, and having read about how the computer has been practicing and learning since then and how the computer will use faster technology at the real match, I decided to rule in slight favor of the technology.   http://www.geekwire.com/2016/alphago-lee-sedol-whos-underdog-in-google-ai-million-go-match/\nhttp://www.wired.com/2016/02/well-know-soon-if-google-can-beat-a-super-grandmaster-at-go/\nhttp://phys.org/news/2016-02-human-champion-hell-ai-ancient.html\nhttp://www.scientificamerican.com/article/go-players-react-to-computer-defeat/"], [19476, "2016-02-29T17:47:13Z", [0.65], ""], [18489, "2016-02-29T16:04:33Z", [0.35], ""], [823, "2016-02-29T15:51:53Z", [0.8], ""], [17875, "2016-02-29T14:10:19Z", [0.0], ": )"], [12874, "2016-02-29T12:52:23Z", [0.57], ""], [12874, "2016-02-29T12:51:33Z", [0.56], ""], [14841, "2016-02-29T12:28:54Z", [0.4], ""], [20078, "2016-02-29T12:22:25Z", [0.71], ""], [691, "2016-02-29T12:19:26Z", [0.63], ""], [4972, "2016-02-29T10:44:53Z", [0.69], "Alphago beat the European champion 5-0. There is a good chance (69%) that it will beat the world champion by atleast 3/2"], [361, "2016-02-29T07:52:46Z", [0.77], ""], [19936, "2016-02-29T07:06:13Z", [0.85], "Alpha Go seems to scale with bigger hardware and Google has plenty"], [20766, "2016-02-29T04:24:35Z", [1.0], ""], [20766, "2016-02-29T04:11:40Z", [0.7], ""], [436, "2016-02-29T02:43:53Z", [0.39], "My newest question is: To what extent can the AlphaGo team move to correct specific deficiencies in the program's play? (As opposed to merely giving it more material to learn from?)  Given that the mistakes seem to follow a general pattern -- a pattern Sedol can study -- I'm not feeling as enthusiastic about AlphaGo's chances as I had been. But then again, maybe development and focus have been increased leading up to this match. And the secretary-general of the World Go Federation apparently thinks the matchup is \"50/50\": http://www.nature.com/news/go-players-react-to-computer-defeat-1.19255 Which doesn't mean much when said casually, and may have been merely political, but still, I'd expect her to be on the side of the humans for this one. Certainly the programmers seem confident. But the more I think about this, the more I believe it will be difficult to gain those \"last few points\" of strength, and the more I realize that Fan Hui was probably much less prepared than Lee Sedol will be. (Unless Lee Sedol will make Fan Hui's mistake in underpreparing/\"testing\" his opponent -- but given the amount of honor he stands to gain or lose, I think he'll keep his head in the game.)"], [14487, "2016-02-29T02:12:37Z", [0.6], ""], [14035, "2016-02-29T01:11:37Z", [0.95], ""], [6628, "2016-02-28T21:03:52Z", [0.95], "Go seems like something a computer would be great at. "], [14933, "2016-02-28T20:25:47Z", [0.56], ""], [16940, "2016-02-28T19:23:26Z", [0.6], ""], [20703, "2016-02-28T19:01:09Z", [0.99], ""], [20737, "2016-02-28T16:20:36Z", [0.6], ""], [20724, "2016-02-28T12:50:47Z", [0.7], ""], [691, "2016-02-28T12:44:07Z", [0.63], ""], [11885, "2016-02-28T09:17:45Z", [0.0], ""], [20715, "2016-02-28T09:10:05Z", [0.1], ""], [11038, "2016-02-28T03:25:17Z", [0.65], "I am thinking 3-2 in the favor of AlphaGo.  Predicting the number of  number of games that it would take either to win would make for side wager"], [20698, "2016-02-27T20:08:00Z", [0.15], "Not yet: it will be too soon to beat the best."], [18530, "2016-02-27T18:14:13Z", [0.78], ""], [20692, "2016-02-27T16:31:50Z", [0.82], ""], [11080, "2016-02-27T16:25:47Z", [0.25], ""], [691, "2016-02-27T14:15:59Z", [0.63], ""], [1468, "2016-02-27T10:29:54Z", [0.34], "Updating positively based on debate at: http://www.metaculus.com/questions/112/"], [20562, "2016-02-27T10:12:59Z", [0.82], "As per @balakirev's additional insights into Go rankings and play-styles (http://www.gjopen.com/comments/comments/148985), I am lowering my prediction by another 8%. I might be going too low given that this is a neural net trained on absurd quantities of data. But then again, I might be going too high based solely on the knowledge that this is a neural net trained on absurd quantities of data."], [20684, "2016-02-27T09:22:58Z", [0.74], ""], [498, "2016-02-27T07:42:41Z", [0.13], "AlphaGo creator remains confident\nhttp://koreajoongangdaily.joins.com/news/article/article.aspx?aid=3015421&cloc=rss|news|joongangdaily OK. My lack of understanding of AI may hurt me greatly on this forecast. In the interview, HassabisIn says: \"In Go, you need to combine pattern recognition with a plan.\" Now this statement is not in reference to Alphago, but to the game of Go (as played by humans). My limited understanding of AI caused me to assume that AlphaGo was just optimizing each incremental move (given some state determined by pattern recognition) and that the ability to generate \"a plan\" was not (yet) designed into AlphaGo. Even though Hassabisin's statement is not a direct claim for AlphaGo possessing that capability, his appreciation of that aspect of the game causes me to doubt my original assumption that AlphaGo does not contain that capability. (+3)"], [10039, "2016-02-27T05:29:21Z", [0.8], ""], [12828, "2016-02-27T03:52:44Z", [0.98], ""], [1646, "2016-02-26T23:33:02Z", [0.32], ""], [20562, "2016-02-26T19:57:27Z", [0.9], "Found this article where Se-dol apparently states that he's been practicing against AlphaGo for an hour or two each day.  http://english.chosun.com/site/data/html_dir/2016/02/23/2016022301813.html This is literally the only source claiming Se-dol has said this, and it's the english version of a non-english publication, so it might be a mistranslation. If it isn't a mistranslation, then it's not clear as to whether he said this before or after his own estimate that he would win 5-0 or 4-1.  If Se-dol's estimate came after testing the AI for himself, then I'd bring my prediction down from what I'd previous rounded up at 99.8% down to 3%. But, it's really weird that this is the only source reporting this, and even if accurate it's not clear when the prediction was made, so, 90%. Still keeping my estimate quite high is this quote from one of the engineers: Hassabis said most Go players are giving Sedol the edge over AlphaGo. \u201cThey give us a less than 5 percent chance of winning \u2026 but what they don\u2019t realize is how much our system has improved,\u201d he said. \u201cIt\u2019s improving while I\u2019m talking with you.\u201d"], [20487, "2016-02-26T19:39:07Z", [0.75], ""], [11053, "2016-02-26T17:44:53Z", [0.65], ""], [19078, "2016-02-26T17:33:32Z", [0.75], ""], [13601, "2016-02-26T16:46:09Z", [0.7], "Yes. This is due to belief that AlphaGo will learn more than Sedol in the timeframe up until their match. I think it will win 3-2"], [20572, "2016-02-26T15:21:46Z", [0.7], ""], [15166, "2016-02-26T13:13:39Z", [0.55], "Adjust and update forecasts when appropriate: AlphaGo recently beat the three-time European champion of Go, who, although not as strong as Sedol, is a significant first step for the AI.  This suggests that AlphaGo is indeed strong enough to compete against champions. Other perspectives should inform your forecasts: Also, opinions from those involved in the International Go Federation suggest that although they don\u2019t expect the computer to totally blow Sedol away, it at least has a chance at learning enough to overcome him in this first match. As such, I adjust my forecast up to 55% that AlphaGo will beat Sedol."], [691, "2016-02-26T12:43:55Z", [0.62], ""], [20129, "2016-02-26T09:22:21Z", [0.62], "AlophaGo beat the European Go Champion a few weeks ago 5-0. And it does not stop learning like the real Go masters. I believe in sheer crunching power over heuristics therefore I increase my forecast"], [20634, "2016-02-26T08:31:16Z", [0.6], ""], [102, "2016-02-26T05:42:34Z", [0.37], ""], [498, "2016-02-26T05:12:09Z", [0.1], "Per @eron_gj's demonstration of the confidence of Google engineers, who clearly know far, far more about both AI and Go than I do. (+5)"], [20613, "2016-02-25T23:14:52Z", [0.65], ""], [15298, "2016-02-25T20:46:31Z", [0.3], ""], [19924, "2016-02-25T20:07:08Z", [0.99], "Recent research in machine learning makes it very difficult for humans to compete in the 'games' arena. Learning algorithms can see and deal with uncertainty a lot better than average humans, and I suspected that in the Go game, machines will beat top players from now on on a frequent basis.\nNature 529, http://www.nature.com/news/digital-intuition-1.19230\n"], [20247, "2016-02-25T19:53:01Z", [0.58], ""], [15298, "2016-02-25T19:08:50Z", [0.4], ""], [20588, "2016-02-25T18:04:02Z", [0.85], "The difference between Fan Hui proficiency and Lee Sedol's can't be all that much. If AlphaGo beat Fan 5-0, it'll likely beat Lee 3-2 or 4-1. Then again, there just isn't enough of a winning streak to be sure. This is only the second official match like this. "], [11586, "2016-02-25T18:01:47Z", [0.09], "Nope."], [20586, "2016-02-25T16:01:21Z", [0.74], ""], [12874, "2016-02-25T14:29:20Z", [0.47], ""], [12874, "2016-02-25T14:27:00Z", [0.49], ""], [691, "2016-02-25T12:55:55Z", [0.62], ""], [20562, "2016-02-25T12:42:14Z", [1.0], "Actually, f* it. It would just be flat out way too weird for Google to challenge the top player if AlphaGo had tapered off at 3,100 elo. I can't seem to do 99.8% so I'm just doing 100%. Edit: Something to consider for the people who estimated low odds of AlphaGo winning (though it'll be apparent that I'm not taking my own advice with my own very high prediction): Do you think the odds AlphaGo's engineers place on AlphaGo winning are lower or higher than the odds you placed on AlphaGo winning? \nIf so, do you know more about either AlphaGo, or the game of Go, than do AlphaGo's engineers?  If you think the engineers have studied Go more carefully than you have, and you think they have a better idea of AlphaGo's capabilities than you do, then it seems more reasonable to place your prediction more in line with what you imagine their prediction would be."], [20562, "2016-02-25T12:27:25Z", [0.95], "IBM was a company. It made Deep Blue. IBM had a lot of very smart engineers, and it had a very smart PR department, and the very smart engineers had to convince the very smart PR department that Deep Blue could beat Kasparov. Deep Blue beat Kasparov (though I grant this was after an off-by-one year error).  IBM made Watson. Watson was developed by a lot of very smart engineers, who had to convince a very smart PR department, that Watson could beat Ken Jennings. Watson beat Ken Jennings (this time, IBM was much more careful about making sure it would win).  Google made AlphaGo. AlphaGo was developed by a lot of very smart engineers, who had to convince a very smart PR department, that AlphaGo could beat Lee Se-dol. Also, the research papers are really impressive. Like, by any measure I can't think of any especially compelling reason but plain old meat-chauvinism as to why AlphaGo shouldn't win this thing, and I *can* think of quite a few as to why it should.  Leaving the estimate at 95% though, because I can't find data on the rate at which AlphaGo is improving. Its last estimated ranking was 3,100 elo, which is higher than Sedol's lower bound elo (though, as far as I can tell his upper bound is unknown?). There's some chance that AlphaGo's improvements tapered off near there, but, it strikes me as unlikely that Google would challenge the top player if AlphaGo's skill level looked like it was going to taper off around 3,100 elo. When it comes to neural nets, tapering off isn't something you can just shrug off with a \"re-match after we throw more hardware at it next year.\" Tapering off means that your algorithm isn't capable of learning much else. Throwing more GPUs at it doesn't make it significantly better unless you're increasing the number of games you teach it faster than the system's returns are diminishing. If you want to re-match next year, you need to be confident that you're going to make a groundbreaking advancement in deep learning next year."], [20497, "2016-02-25T12:15:29Z", [0.75], ""], [4064, "2016-02-25T10:48:41Z", [0.2], ""], [16768, "2016-02-25T10:24:35Z", [0.55], ""], [498, "2016-02-25T07:22:05Z", [0.05], "I know nothing about AI, but this video provided me with some insight into how DeepMind approaches it. If I understand it correctly, it is responding to no more than a spatial configuration of reality. That is, it responds to no more than \"the pixels on the screen.\" It seeks to optimize some objective based on the current configuration of pixels. Well, that's the best I can do with my explanation so far. There's probably more to it. If there's any truth to my initial understanding of AI as implemented by DeepMind, I'm thinking that first, it is vulnerable to a novel configuration. How does it handle a configuration of reality it has never seen before? Second, it seems to lack any \"theory of mind.\" It responds to the what, not the why of the what. I'm not sure, but I'd think this lack of insight into the why of a move might make it susceptible to being tricked. Well, that's my 15 minute analysis of AI and my justification for my forecast.\nhttps://www.youtube.com/watch?v=rbsqaJwpu6A"], [17875, "2016-02-25T06:54:36Z", [0.0], " \u00af\\_(\u30c4)_/\u00af "], [1333, "2016-02-25T06:04:52Z", [0.5], ""], [1529, "2016-02-25T03:38:58Z", [0.55], "Not changing estimate, but for those interested in watching the past games between AlphaGo and Fan Hui.....\nhttp://www.usgo.org/news/2016/01/alphago-beats-pro-5-0-in-major-ai-advance/"], [18969, "2016-02-25T00:54:25Z", [0.28], "Slight reduction (-5%)  Just saw the video (https://www.youtube.com/watch?v=SUbqykXVx0A) which has a bit more insight into the scale of complexity of chess vs. Go.  It looks like the number of possible moves is about an order of magnitude higher in Go.  I think this give further credence to my principle notion that more time is needed for development in order to defeat a top human player - based on previous developments in chess.  Not only is the development time less than for chess, but the complexity is higher.  I think complexity will increase development time.  "], [20523, "2016-02-25T00:20:55Z", [0.15], ""], [15311, "2016-02-24T23:07:47Z", [0.35], ""], [1646, "2016-02-24T21:57:17Z", [0.48], ""], [691, "2016-02-24T21:50:20Z", [0.58], ""], [15621, "2016-02-24T19:39:47Z", [0.58], ""], [20508, "2016-02-24T18:50:58Z", [0.85], "AlphaGo is probably machine learning and training all the relevant documented Go games in history while Lee sleeps."], [19106, "2016-02-24T17:52:56Z", [0.5], "I'm upping my estimate, after a great deal of thought and research in the course of writing an article on the upcoming match (actually mostly prompted by this GJ question though!). My prediction for AlphaGo's chances have gone from about a 20% chance to 50%. The approach google are using has the potential to be really, really powerful. Having a value function that starts to approach the value function for optimal play is pretty unprecedented, and if I'm not mistaken they can always get it to converge closer by running more reinforcement learning to build out the knowledge encoded in the monte carlo search tree, and then backpropogate that to the policy, and ultimately value, networks. I'm now thinking the only troubles it will face are to do with whole-board strategy, subtle non-local interactions, and maybe some '5 dan mistakes', though I suspect they will be '7 dan mistakes' by March, if it makes them at all. Long, complicated expansion on these points below. --- Many professionals expect the Korean professional to demonstrate the ongoing superiority of man over machine. \nhttp://www.nature.com/news/go-players-react-to-computer-defeat-1.19255\nhttp://www.usgo.org/news/2016/01/chinese-professionals-react-to-the-historic-alphago-win/ But are they right? Or does AlphaGo have more of a chance than they think? DeepMind certainly seem to be more confident than the consensus predictions \u2013 at a Korean press conference Demis Hassabis suggested it would be \u201ctoo close to call\u201d - 50/50 odds. David Silver\u2019s comments that he would be \u201cvery disappointed\u201d if they lost suggests even stronger confidence. Deepmind is full of strong amateur Go players, as well as pioneers in Go AI, so we\u2019d expect them to be reasonable judges. What do they know that the pros don\u2019t? Well one thing they, and no-one else, knows is what they\u2019ve been doing since October. There are several potential improvements that might give AlphaGo extra strength, from most to least likely to be carried out.\n\u2022\tImproving the accuracy of the value function\n\u2022\tTweaking model parameters\n\u2022\tAdding more hand-crafted features to the rollout policy\n\u2022\tRetraining the supervised learning network on professional data One of the things that separates professionals from strong amateurs is their ability to look at even a complex board position and tell who is ahead. This question of \u2018value\u2019 of a board position has been a non-trivial problem in computer Go since inception, and DeepMind\u2019s solution to it is the main thing separating its program from other Go AIs. Deterministic, zero-sum games actually have an objective value function across all board positions, but Go has too many combinations to ever calculate this precise value. AlphaGo uses a neural network model to approximate the value function, and this model was created in three steps, building two other models along the way: 1.\tA \u2018policy network\u2019 (ie a model giving a probability distribution over possible moves) built using \u2018supervised learning\u2019 (SL) (where we get the model to make a prediction, then we give it the answer and it adjusts the model to \u2018learn\u2019 from the answer) to predict a human\u2019s move, given a board position. AlphaGo\u2019s supervised learning policy network successfully predicted human moves 57% of the time, when trained on 160,000 6-9dan KGS games having 30 million board positions. 2.\tAnother policy network, built by \u2018reinforcement learning\u2019 (RL) - taking the supervised learning network and getting it to play subsequent versions of itself and learn from the game outcomes, to predict the move most likely to result in a victory. The reinforced learning policy played 1.28 million games against different versions of itself, resulting in a very strong policy network for selecting moves. 3.\tFinally, the \u2018value network\u2019, which was built by supervised learning & regression over board positions and values generated from the SL and RL networks, and predicts the expected value (ie probability of a victory) of a board position. To do this AlphaGo generated 30 million games, playing the first n-1 moves with the SL network, then selecting a random legal move, and then using the RL network to select all moves until the game ends and a value (ie win/lose) is known. The value network was then trained on just one board position from each game \u2013 the one subsequent to the first RL network move \u2013 to minimize the error in predicted value. This complex process resulted in a value function that is closer to the \u2018real\u2019 value function for Go than anyone has ever achieved before. As a result of this, the value function is one area where DeepMind might be able to easily gain extra strength to close the gap with Lee Sedol. By doing more reinforcement learning to build a better move policy, and then generating a much larger corpus of games and board positions and using them to retrain the value function, they could further improve its accuracy. All that is really required to do this is time and computing power. Another area where I\u2019d expect DeepMind to at least experiment and perhaps obtain modest improvement, is in some of the structural aspects of their model. AlphaGo uses a hybrid approach that combines the more traditional Monte Carlo Tree Search technique of semi-random playouts with the above-described value function to assess moves. At the moment those two techniques are given equal weight, but the optimum balance may differ from that. There are also several other modelling constants where trial-and-error tweaking is inexpensive and could improve results. Like other Monte Carlo based AIs, AlphaGo builds up a set of complete games from the current board state by playing lots of fast random(ish) games all the way to the end to see who wins. But it is only randomish, because they use a \u2018rollout policy\u2019 to select which moves it\u2019s more likely to explore. AlphaGo\u2019s rollout policy is built in a similar way to the SL policy, but is designed to be much (~1000x) faster. Like the SL policy it uses some handcrafted features, such as whether a move is an atari, ladders, and a \u2018pattern book\u2019 of 3x3 patterns, to decide the move probabilities. Though not as likely as the previous two options, DeepMind might try adding other such features, or tweaking the existing rollout policy to improve the Monte Carlo search. One other possibility suggested by several observers is that DeepMind could go use kifu from professional games to get an edge. This would involve going back to the start and retraining their supervised learning network, but instead of the KGS games, using kifu from pro games. There are at least 80,000 pro games out there, around half the volume of KGS games used to train AlphaGo\u2019s SL network. However this approach seems unlikely, not only because it requires re-doing a lot of work, but also because if there were no obstacles, Deepmind would have used it to begin with. Whether due to copyright uncertainties or some other reason, they will probably stick with the KGS dataset they\u2019re already using. All of that may not be enough to beat Lee if professionals\u2019 observations about AlphaGo\u2019s weaknesses reflect inherent limitations in its approach or structure, rather than simply not yet having learnt from enough board positions. The critiques all touch on similar themes, which centre around a lack of whole-board awareness, or high-level play. Among the weaknesses suggested are a lack of understanding of sente, no useful conception of aji, and a lack of \u2018creativity\u2019, or following common patterns (albeit patterns often used by strong amateurs or even pros) where the specific context calls for deviation. Myungwan Kim\u2019s comment about \u201c5 dan mistakes\u201d seems particularly prescient.  It may make sense that AlphaGo struggles with whole-board interconnectedness given the structure of its underlying models. Convolutional neural networks (CNNs) are typically local by nature, and don\u2019t build a good understanding of the whole board. According to Remi Coulom, AlphaGo\u2019s architecture uses 1 layer of 5x5 convolution, and 11 layers of 3x3 convolution, meaning \u201cit can propagate information at a distance of 13 points, but not more\u201d. This is remarkable when compared to all previous Go-playing CNNs, but may still present a limitation for certain positions with important non-local interactions. So after all this I find myself relying on Demis Hassabis 'too close to call' statement, and saying 50%."], [10921, "2016-02-24T15:18:26Z", [0.14], "Incresing my estimate given the confidence displayed by google that it is 50/50."], [19306, "2016-02-24T12:22:27Z", [0.21], "Following WJK questions, I m modifying my forecast to take into account the possibility that I maybe wrong  about my estimation of AlphaGo ELO. "], [20483, "2016-02-24T10:47:18Z", [0.43], "I believe the human mastery is yet to be matched by any algorithm. "], [19974, "2016-02-24T08:07:41Z", [0.75], "I read a number of commentaries on the AlphaGo vs Fan Hui match. The official results was very decisive, but they played a couple of unofficial games and Fan Hui won two of those. Hence, it seems that AlphaGo was not better by an order of magnitude. Other factors to consider: since the beginning of computer science, progress of AI was generally slower than expected, so betting against AI was historically a good bet. On the other hand, how likely is it that AlphaGo is vastly better than human players, but we didn't have a chance to see it yet? Also, how much better can AlphaGo get between October and March? Judging these factors, I move to from 50-50 to 75%..."], [17875, "2016-02-24T07:11:33Z", [0.0], ":)"], [19626, "2016-02-24T05:29:14Z", [0.75], "Read links based on articles about projected ELO ratings. Find these atgents quite convuncing, but believe the probability of Google substantially improving the algorithm by extensive training before the match to be quite good. Essentially being bullish on the Google staff given their considerable resource and how high profile this game has become."], [691, "2016-02-23T21:28:27Z", [0.56], ""], [3320, "2016-02-23T20:31:33Z", [0.27], "I've gone back and forth on this one.  But Lee Sedol is so strong that he'll be able to capitalize on any sub-optimal move AlphaGo makes.  However, that may be true of AlphaGo as well.  But I doubt that AlphaGo can win three of five games against one of the world's best players."], [16024, "2016-02-23T20:28:54Z", [0.5], ""], [20448, "2016-02-23T17:39:51Z", [0.42], ""], [20428, "2016-02-23T11:48:24Z", [0.05], ""], [689, "2016-02-23T08:47:10Z", [0.0], ""], [20129, "2016-02-23T08:02:52Z", [0.58], ""], [17875, "2016-02-23T05:05:24Z", [0.0], ": ) "], [20410, "2016-02-23T02:32:27Z", [0.6], ""], [19770, "2016-02-23T00:29:11Z", [0.7], ""], [3363, "2016-02-22T22:09:02Z", [0.42], ""], [19427, "2016-02-22T21:00:39Z", [0.65], "\"Park Chi-Moon, the vice-chairman of Korea\u2019s Go association, injected a note of caution for humanity. \u201cDeepMind said they believe AlphaGo has a 50% chance of winning \u2026 we believe they are in fact more confident,\u201d he said.\" http://www.theguardian.com/technology/2016/feb/22/google-deepmind-go-alphago"], [691, "2016-02-22T19:12:42Z", [0.59], ""], [20363, "2016-02-22T14:34:56Z", [0.55], ""], [20078, "2016-02-22T14:20:52Z", [0.68], ""], [11016, "2016-02-22T14:20:51Z", [0.95], ""], [15133, "2016-02-22T12:16:43Z", [0.85], ""], [212, "2016-02-22T05:57:09Z", [0.35], ""], [20341, "2016-02-22T03:31:14Z", [0.75], "Even though there is no method for playing Go, the computer is capable of keeping a big numbers approach to the game. This gives the computer and edge beyond the random component of the game. Assuming the game is 50% chance and 50% ability the computer has a 100% in the ability side."], [14137, "2016-02-22T02:49:43Z", [0.75], "AlphaGo's first victory was against a far less skilled opponent than Lee Sedol, however the program is a continuously learning program. Given a 5 month period in which to improve, AlphaGo could emerge incredibly skilled.  Unfortunately it really is difficult to have any certainty on this question because key details are unknowable.\nMost importantly, how much continued learning is AlphaGo conducting? It is already primarily playing against itself. After discussing the learning algorithm with a friend who works on neural networks, the learning environment seems to leave open either incredibly rapid learning, or re-enforcing existing weaknesses against a human player. It wont be possible to know until after the match. I discount the possibility of fatigue in the human player because the games are played over multiple days. It shouldn't dramatically reduce Lee's concentration. He is described as emotional however, so if things begin to go against him he may struggle.  None of that changes the underlying fact that AlphaGo's performance at the last match was rated far below Lee's level, but we aren't able to know how quickly AlphaGo can continue to learn. Having watched a neural network learn to play Mario Bros, I think the learning rate should be incredibly fast, so long as the feedback works properly. My prediction is essentially based on the assumption that performing at the already demonstrated level precludes major weaknesses in the learning algorithm. "], [4139, "2016-02-22T01:16:51Z", [0.1], "Not yet, a few more years."], [12937, "2016-02-22T00:49:08Z", [0.68], "So when AlphaGo won the game 5\u20130, it was a big deal.\n"], [19547, "2016-02-21T23:54:10Z", [0.9], ""], [20020, "2016-02-21T22:59:34Z", [0.75], ""], [18965, "2016-02-21T22:47:29Z", [0.72], ""], [1078, "2016-02-21T21:33:12Z", [0.7], ""], [16742, "2016-02-21T20:34:45Z", [0.85], ""], [17875, "2016-02-21T17:46:47Z", [0.0], "@cdob63  is correct about the awesome destructive force of innovation.  Car's weren't a better horse, Eli Whitney & Abigail Adams made a swell cotton gin, and Gutenberg's press wasn't bad either.   It's difficult to anticipate the specific tipping point but once it happens everything after it isn't the same. \n--\nThe reason I'm at 0% and not 59% is reflected in a John Maynard Keynes quote:  \u201cThe market can stay irrational longer than you can stay solvent.\u201d  For the last 12,000 years a computer hasn't bested man at Go until last October.  That's a pretty awesome feat on both accounts! Perversely, I hope I'm totally wrong on this question.  I'd happily trade my irrationality for an awesome achievement of mankind, but I have to endure the pain of being proven utterly and indiscriminately wrong to do it : ))"], [20317, "2016-02-21T17:30:36Z", [0.1], "If AlphaGo beat Lee Sedol, it would mean Go programs progressed faster in the past six months than at pretty much any other time. There's a big difference between beating a 2p and beating the world champion."], [691, "2016-02-21T15:55:05Z", [0.59], ""], [15471, "2016-02-21T14:44:58Z", [0.65], "Going a bit lower due to ELO ratings, still believing in ML though"], [19306, "2016-02-21T13:45:36Z", [0.08], "Alpha Go current ranking estimated #279\nIf progress up to #95 rank, ELO would be 3306\nProbability of winning a game against Lee Sedol (ELO=3524) is 0,22 (http://www.bobnewell.net/nucleus/bnewell.php?itemid=279)\nProbability of winning 3, 4 or 5 time on 5 game against Lee Sedol = 0,074 rounded up"], [212, "2016-02-21T13:09:39Z", [0.15], "I don't think AlphaGo will win considering the European champion AlphaGo beat was just 2p while Sedol is 9p, a big difference. While we don't know how much AlphaGo can learn in six months, the belief among a few of the people in the Go industry seem to think that AlphaGo just doesn't have enough experience to beat Sedol. Also, AlphaGo would have to win three of five games to win, which seems like a bit of a jump from just beating a 2p player 5-0 to beating a 9p player.\nhttp://www.nature.com/news/go-players-react-to-computer-defeat-1.19255\nhttps://www.quora.com/Can-Google-AlphaGo-beat-world-Go-champion-Lee-Sedol-in-March But again, the crucial thing is how much AlphaGo can learn by the time it plays against Sedol."], [424, "2016-02-21T03:53:45Z", [0.33], ""], [424, "2016-02-21T03:44:11Z", [0.45], ""], [19427, "2016-02-21T02:59:17Z", [0.6], "More weight on DeepMind CEO and the computer scientists' statements. I believe them."], [19306, "2016-02-21T02:51:13Z", [0.19], "Elo difference gives Lee Sedol 93% of winning and he must win 3 times on 5"], [19306, "2016-02-21T02:07:38Z", [0.07], "ELO 3140 vs 3515 has 0,0342 ow winning, doubled considering previous progress made by AlphaGo"], [17875, "2016-02-20T20:07:12Z", [0.0], ": )"], [14933, "2016-02-20T18:00:38Z", [0.56], ""], [17819, "2016-02-20T07:06:57Z", [0.52], "I have a bit of I bias toward machine learning algos and as such I have been second guessing my intuitive feeling that AlphaGo will take it. I do however feel that the plans that the google team have started implementing after the last game (getting better help and letting AlphaGo play more games i.e. \"learn more\") might push the odds in favour of the machines. This is a very hard question to answer as there is pretty much no way of knowing how strong AlphaGo has gotten since the game against Fan Hui.  "], [20194, "2016-02-20T06:40:57Z", [0.45], ""], [12479, "2016-02-20T03:43:43Z", [0.79], ""], [17875, "2016-02-20T03:09:12Z", [0.0], ": )"], [20265, "2016-02-20T02:35:40Z", [0.35], ""], [94, "2016-02-19T23:56:39Z", [0.35], "According to [1], AlphaGo has at most a 50% chance of winning, and a normalized 45% chance of winning.  I'm increasing my odds in accordance with this. [1] https://bitbet.us/bet/1249/alphago-will-defeat-lee-sedol-overall-in-march/"], [915, "2016-02-19T23:28:50Z", [0.02], ""], [11437, "2016-02-19T23:03:02Z", [0.8], ""], [20193, "2016-02-19T21:57:25Z", [0.5], "It is too early for the computer to have all the moves. Hence giving 50 % "], [19476, "2016-02-19T21:46:59Z", [0.49], ""], [100, "2016-02-19T20:04:18Z", [0.45], ""], [20247, "2016-02-19T19:42:19Z", [0.68], "Lee Sedol is ranked 5th in the world, meaning AlphaGos victory over Fan Hui (who despite being European champion, is 633rd in the world) doesn't provide as much information as it first appeared. That said, the victory was comprehensive, and the alphago team have since added FanHui to their advisory team. In addition, AlphaGo is playing around a million games every day, improving with each one. Furthermore, the alphago team are laying their own cables for the upcoming match, further increasing their computing power. Finally, the alphaGo team reports it can beat the best rival computing programs with a 4 stone handicap, something even the top professionals have been unable to do. While it is tempting to draw comparison to Kasparov vs DeepBlue (with DeepBlue losing the first game), there are vast differences. DeepBlue's previous game was a loss in an AI tournament, leading to a tie for second place. Its code was based on hard-coded logic which was being manually updated. And computing power was far less back then. AlphaGo is coming off the back of an impressive victory against a human professional, is improving every day, and has significantly more computing power, meaning it is likely to improve exponentially.  "], [20246, "2016-02-19T19:03:41Z", [0.75], ""], [12874, "2016-02-19T14:55:13Z", [0.37], ""], [12874, "2016-02-19T14:54:04Z", [0.4], ""], [12828, "2016-02-19T14:34:50Z", [0.7], ""], [691, "2016-02-19T14:26:21Z", [0.59], ""], [19801, "2016-02-19T13:58:00Z", [0.92], "After reading Google's paper, and realizing that AlphaGo distributed has thousands of cores and GPUS, it is clear that this program will be at least 240-220x stronger than the original AlphaGo which was already able to beat a human 2 Dan pro. Neural networks have linear scaling on GPUs, and Monte Carlo Tree Search has almost linear scaling on CPU cores. Those are the two main techniques that AlphaGo uses. This argument is based on lots of computers and video cards, a brute force argument. One human brain doesn't match up well to all this power, no matter how efficient."], [19170, "2016-02-19T13:53:39Z", [0.2], ""], [20226, "2016-02-19T12:09:50Z", [0.65], "chess has 10^120 permutations, wei qi has 10^170, computing power has grown exponentially since deep blue beat kasparov. "], [18915, "2016-02-19T10:47:22Z", [0.22], ""], [17819, "2016-02-19T06:48:03Z", [0.46], ""], [19247, "2016-02-19T06:09:31Z", [0.67], ""], [19170, "2016-02-19T05:30:43Z", [0.3], ""], [5001, "2016-02-19T04:01:42Z", [0.35], ""], [19770, "2016-02-19T03:52:17Z", [0.62], ""], [14841, "2016-02-19T01:05:24Z", [0.3], ""], [20217, "2016-02-18T23:49:09Z", [0.26], ""], [20217, "2016-02-18T23:49:00Z", [0.23], ""], [15362, "2016-02-18T22:56:55Z", [0.25], ""], [15331, "2016-02-18T21:20:32Z", [0.15], ""], [19642, "2016-02-18T21:14:17Z", [0.15], ""], [15311, "2016-02-18T21:06:48Z", [0.25], ""], [15331, "2016-02-18T20:20:12Z", [0.35], "https://www.quora.com/Can-Google-AlphaGo-beat-world-Go-champion-Lee-Sedol-in-March"], [15314, "2016-02-18T18:59:08Z", [0.45], ""], [10039, "2016-02-18T18:34:53Z", [0.57], ""], [18915, "2016-02-18T18:34:19Z", [0.55], ""], [12413, "2016-02-18T18:08:56Z", [0.75], ""], [12413, "2016-02-18T18:05:13Z", [0.68], "The previous match between AI and the Euro GO champion was 5-0 for the AI.  This champion would not win a match vs the upcoming opponent, so that takes me to  suggest it will be at least a close match.    In fact Fan Hui wasn't just beaten he was thoroughly trounced in his match he stated \"The problem is humans sometimes make very big mistakes, because we are human. Sometimes we are tired, sometimes we so want to win the game, we have this pressure...The  programme is not like this. It's very strong and stable, it seems like a wall. For me this is a big difference. I know AlphaGo is a computer, but if no one told me, maybe I would think the player was a little strange, but a very strong player, a real person.\""], [5803, "2016-02-18T17:17:45Z", [0.2], ""], [10759, "2016-02-18T16:14:56Z", [0.25], ""], [4634, "2016-02-18T16:14:37Z", [0.25], ""], [19007, "2016-02-18T16:04:18Z", [0.75], ""], [691, "2016-02-18T13:59:29Z", [0.58], ""], [17819, "2016-02-18T08:13:15Z", [0.48], ""], [361, "2016-02-18T07:24:43Z", [0.55], ""], [20156, "2016-02-18T07:21:31Z", [0.47], "Beating the best human player in GO is much harder than in chess. It took IBM years to battle the latter, and it will take time to really succeed at the former. It will beat Sedol once or twice but probably not more this time. "], [11312, "2016-02-18T06:44:51Z", [0.78], ""], [17875, "2016-02-18T02:57:22Z", [0.0], " : )"], [20167, "2016-02-18T00:39:56Z", [0.67], ""], [691, "2016-02-18T00:30:56Z", [0.58], "I do not bet against machines any more on this sort of Q.  Still, comes the revolution, I expect I will be in the first group the machines line up against the wall--too ambivalent, so not a reliable ally for them."], [20164, "2016-02-17T22:39:11Z", [0.25], "Like IBM for DeepBlue, Google will need this first match to collect feedback/lessons learned and update its Go engine...then it'll be ready for the second match and beat Lee Sedol (or someone of his caliber)...yet this is still an incredible achievement by Google thus far...."], [20153, "2016-02-17T22:00:21Z", [0.25], ""], [20154, "2016-02-17T20:19:46Z", [0.97], ""], [20142, "2016-02-17T14:54:15Z", [0.75], ""], [20044, "2016-02-17T14:14:55Z", [0.95], "Assigning an arbitrary value of 10 to the computational power of the first computer to defeat a grandmaster in 1988, Moore's law implies that Deep Thought had computational power of 226 on the the same arbitrary scale when it defeated Gary Kasparov in 1997. In other words, the difference between a grandmaster and one of the best chess players was about 216 units of computational power. (The difference in ELO ratings between a grandmaster and a top player like Gary Kasparov appears to be similar to the difference in ELO ratings between Fan Hui and Lee Sedol, as given by this Wired article: http://www.wired.com/2016/02/well-know-soon-if-google-can-beat-a-super-grandmaster-at-go/).  Continuing the exponential growth in computing power on the same arbitrary scale results in computational power of roughly 109,000 when AlphaGo defeated Fan Hui in October of 2015. The same exponential growth implies computational power of roughly 126,000 when AlphaGo plays Lee Sedol in March 2016, or an increase of almost 17,000 computational units. Even assuming that the difference between top Go players and top chess players is significantly larger (but still linear), it appears incredibly unlikely that AlphaGo would not be able to improve enough over the course of 5 months to defeat Lee Sedol.  There is some confounding of the increase in raw computational power and the increases in playing ability that come from reinforcement learning, but I believe the analysis still holds. Lastly, I would also add that as recently as less than 2 years ago the general consensus was that it would be at least 10 years before a computer would even have a realistic chance of defeating a grandmaster without a handicap (http://www.wired.com/2014/05/the-world-of-computer-go/)."], [20137, "2016-02-17T13:15:26Z", [0.73], ""], [17819, "2016-02-17T12:31:42Z", [0.45], "https://www.gjopen.com/comments/comments/137262 If this is true then I AlphaGo might not just be ready for a master yet."], [10039, "2016-02-17T12:19:53Z", [0.55], ""], [19807, "2016-02-17T11:33:30Z", [0.2], "Upon reading the following articles, i am turning 20 to yes. https://www.reddit.com/r/baduk/comments/43g2jl/synopsis_of_myungwan_kims_analysis_of_fan_hui_vs/ (an analysis by Myungwan Kim a 9 dan American professional )  https://britgo.org/files/2016/deepmind/BGJ174-AlphaGo.pdf (an analysis by a jon Diamond, the British amateur that served as ref on the Fan Hui - AlphaGo match) On the conclusions Jjon Diamond doesn't go very deep in the reasons why he thinks Lee Sedol will defeat AlphaGo.  Myungwan Kim points the following weaknesses and strengths in AlphaGo: AlphaGo's strengths:\n   - It's not afraid of Ko.\n   -  Reading might be AlphaGo's strength. AlphaGo's weaknesses:\n    - Doesn't understand sente, aji and gote [advanced concepts of the game]\n    - At times too obsessed with following common patterns, when the specific situation might require creative deviation from those patterns..\n    - AlphaGo has difficulty, or even doesn't at all, evaluating the value of specific stones. It's good at making moves which directly gain territory for itself, but tends to miss moves which reduce the value of the opponent's stones.\n   -It can make really high level moves at times, but it doesn't understand those moves. Which it displays by making the right moves at the wrong time. I dismiss Kim's comments on the insularity of AlphaGo's learning method (playing millions of times against itself) as that method includes records from a massive  database of past  6 dan+ masters.   Of course those conclusions, both by Jon diamond (he's on Google team's payroll after all :-) ) and Myungwan Kim (an \"expert\" with all the goods and the bads that come with this condition) must be viewed with some skepticism but both point in the same direction: AlphsGo may not be ready to win against a world champion. The only doubt is whether alphaGo can learn until match day how to avoid the pitfalls reported above."], [20135, "2016-02-17T11:17:40Z", [0.68], "The input from many different parties should outweigh the strategic thinking of one."], [20082, "2016-02-17T10:46:46Z", [0.75], ""], [17819, "2016-02-17T10:42:31Z", [0.56], "I personally feel that machine learning algorithms are a very powerful tool and that the odds are in favour of AlphaGo winning. The algorithm used by it (while having some drawbacks) is very good at finding underlying relationships- taking into consideration both different strategies and the \"score\" of the position. I don't know exactly what the Google team did after the last game so I don't know what the chances are that the machine will beat a much stronger opponent. But I'd like to think that the machine will exploit any weakness that Lee makes. So for a preliminary forecast, let's start with a safe 56%. Will update after I've done a bit more research.  "], [175, "2016-02-17T08:29:48Z", [0.2], ""], [175, "2016-02-17T08:29:42Z", [0.25], ""], [175, "2016-02-17T08:25:52Z", [0.1], ""], [20131, "2016-02-17T07:52:44Z", [0.75], "First of the reasons is the AlphaGo program beat the European champion by 5-0. This makes it a very strong contender in the March series. If not by 5-0, the program can at least win by 3-2. The reasons for not giving the absolute certainty of the program winning i.e the remaining 25% chance of not winning is it is possible that the Lee Sedol (a world champion) might be a tougher opponent than Fan Hui (just an European champion). It is also possible that the intermittent time allows Lee Sedol to come to know the weaknesses in the AphaGo's play.\nOverall, it is almost a certain thing that once a computer program crosses a threshold it will keep on improving and would not stop. A simple explanation is the very fist calculators could have been as fast or as slow as fastest human mathematics wizards. But, slowly they moved up the ladder and as of today no human beings is anywhere close enough. So it will be the same case even with the game Go. "], [17875, "2016-02-17T06:10:21Z", [0.0], ": )"], [4794, "2016-02-17T05:46:10Z", [0.65], ""], [767, "2016-02-17T03:06:46Z", [0.2], "Sounds like AlphaGo is just getting the hang of it.  The new computer program may need more experience playing masters in this type of game. http://www.scientificamerican.com/article/computer-beats-go-champion-for-first-time/"], [20117, "2016-02-17T00:51:04Z", [0.85], "Computers programmed thoughtfully will prevail."], [19352, "2016-02-17T00:41:45Z", [0.75], ""], [2586, "2016-02-16T23:55:26Z", [0.0], "Good Judgement is currently 53% yes, but maybe only 0.1% of forecasters even understand the rules of Go let alone have played it. They have heard alot about Google though, and about chess computers, so forecasters naively think Go is a solveable computer problem. So right here there is an over-optimistic bias for AlphaGo. BitBet gives AlphaGo only about 33% chance.   The question is whether Go is bounded, or solveable problem for the computer. I think there is too much subtlety to Go for a computer to grasp long-range strategic meaning and follow through. You can study what happened in games all you like but they represent only the thinest of lines on a page. What I think is missed is this: Sedol is likely orders of magnitude stronger than the European who lost. Meanwhile the computer's ability to improve might prove aymptotic - so I'd guess maybe the computer guys have already nearly maxed out how strong a computer can become. Also would depend on how much time per move is allowed."], [691, "2016-02-16T23:39:21Z", [0.58], ""], [691, "2016-02-16T23:14:48Z", [0.58], ""], [691, "2016-02-16T22:46:28Z", [0.58], ""], [12663, "2016-02-16T22:08:59Z", [0.66], ""], [19044, "2016-02-16T20:54:06Z", [0.82], ""], [1354, "2016-02-16T20:19:57Z", [0.4], "Analysis of AlphaGo's game suggests that at its previous level it would lose (that is was ranked in the 200's, and Lee Sedol is ranked much higher). Lee will also have time to prepare; on the other hand, AlphaGo will too, and its team would presumably not take the match if they didn't think thy could win."], [15872, "2016-02-16T18:33:27Z", [0.35], ""], [15872, "2016-02-16T18:30:18Z", [0.65], ""], [15165, "2016-02-16T18:29:39Z", [0.52], ""], [20053, "2016-02-16T17:39:02Z", [0.8], ""], [19807, "2016-02-16T15:08:27Z", [0.4], "I am correcting my forecast. I meant 40% yes :-) "], [19807, "2016-02-16T14:48:54Z", [0.6], "According to goRatings as of 02-14-2016  (http://www.goratings.org/ ) Lee Sedol, World Champion, is the number 4 in the world.\nFan Hui, the European Champion, defeated by AlphaGo is number 475 (and the top seeded not coming from China, Japan or Korea) . This puts some perspective on AlphaGo's victory... on the other hand, AlphaGo beat FanHui quite categorically by 5-0. So;\n - I am assigning a slight advantage to Lee Sedol. on the basis that he is a much better player than Fan Hui."], [20092, "2016-02-16T14:45:00Z", [0.75], "Track record already existing for AlphaGo"], [20078, "2016-02-16T13:35:27Z", [0.71], ""], [20089, "2016-02-16T13:26:23Z", [1.0], ""], [20087, "2016-02-16T12:53:00Z", [0.25], ""], [241, "2016-02-16T12:29:49Z", [0.05], ""], [20085, "2016-02-16T12:22:14Z", [0.7], ""], [20080, "2016-02-16T11:21:08Z", [0.95], "the alphGo beat the european champion by a country mile. no reason why he won't at least beat the world champion in a five-game series. "], [19990, "2016-02-16T07:44:50Z", [0.65], "I don't think Deepmind would have entered the challenge if they didn't have some confidence that they will at least win a few games."], [2088, "2016-02-16T05:58:25Z", [0.33], ""], [823, "2016-02-16T01:54:14Z", [0.8], ""], [20042, "2016-02-15T21:22:14Z", [0.8], "I just can't imagine a professional in the east being so much better than a champion in the west. If the European Go champion was beaten 0-5 and AlphaGo has months to improve its play I think Lee Sedol's chances are slim. Also I heard Lee Sedol has been on the top for years, so perhaps his time has come and he is not so sharp anymore. Humans playing a charismatic champion will often fail to recognize a blunder for what it is, but a computer will punish the slightest error relentlessly.\nOf the two big computer chess engines Stockfish optimizes its parameters by playing lots of games, but the world champion Komodo has been programmed with higher positional principles by a chess grandmaster renowned for his positional play. \nIn the same way you'd expect Lee Sedol to have a positional feeling that AlphaGo cannot acquire by playing itself. How broad and general are the patterns that AlphaGo can recognize in and of itself? In chess tactics always play a larger role, in Go that would be to conquer a group of stones. Go is more of a positional play than chess so it is really interesting to see if AlphaGo has provably acquired such higher positional principles. I would be disappointed if AlphaGo just defeats Lee Sedol with tactical play, but I don't know the game of Go very well so the commentators have to tell me that."], [17875, "2016-02-15T20:23:42Z", [0.0], ": ) "], [138, "2016-02-15T19:07:49Z", [0.05], ""], [20036, "2016-02-15T19:06:17Z", [0.45], ""], [20031, "2016-02-15T18:13:19Z", [0.3], ""], [13055, "2016-02-15T17:27:34Z", [0.35], ""], [974, "2016-02-15T14:02:53Z", [0.2], ""], [1106, "2016-02-15T11:56:34Z", [0.96], "Human get tired, Lee perhaps may beat AlphaGo in one of the first to third game, but in the five-game match, it is rather difficult for a human player to beat a cold machine 3/5 games. moreover, Lee can beat other human by studying other master player's style in previous matches, but a computer has no style, just go for probablities, which make it hard for human to beat, the human can allow no mistake in front of a computer search algorithm"], [10039, "2016-02-15T10:29:25Z", [0.51], ""], [1403, "2016-02-15T09:37:33Z", [0.98], "Deep Mind has generally been blowing people out of the water. Plus so much dev time will mean likely success."], [19106, "2016-02-15T09:29:57Z", [0.2], "I would put the chances of the October version of AlphaGo beating Lee Sedol in a game at 10%, based on google's estimate of its ELO (3140) and GoRatings' estimate of Lee Sedol's ELO (3500). That means the probability it wins a match is only about 0.1% if we believe the ratings. I think even the October AlphaGo's chance are perhaps slightly better than that. However, 6 months is a long time when you're dealing with a convolutional neural network. I expect AlphaGo to be much, much stronger than the version that played Fan Hui. And DeepMind said at the Korean press conference in Jan that they believe it will be 'too close to call' - that is even odds. Since they are the only ones with the inside information to know their chances in March, this makes me significantly revise up my probability. It is tempting to predict as high as 40% based on their estimate of 50% and revising downwards for some overconfidence.  There are some things that will make it hard for AlphaGo that I doubt merely making the neural net better, or tweaking the balance between policy and value, or rollout and value, will alter. In particular, all MCTS based AIs suffer from the fact that they optimize for a half-point win, so may miss opportunities to extend an advantage. To the extent that they then make mistakes (which Alphago made several of in all its games) this limits their ability to create a buffer. Though against Lee Sedol it would be impressive if it were even in a position to create such a buffer. Myungwan Kim also noted that it doesn't understand the importance of sente, and that it misses many subtleties when there are connections between non-local positions that matter, or when a pro might have spotted an unconventional move, where it falters. "], [17875, "2016-02-15T07:37:27Z", [0.0], "the most important questions I have are:\n1) learning rate of AlphaGo since October\n2) perceived benefit of using professional games for training on the actual performance  (e.g. identifying circumstances of opportunity, ability to recognized strategic decisions, balancing chance of winning vs best possible move at the time, aggressive vs passive play)\n--\nupdate: I LOVE that I got 2 downvotes, lmao : ))  "], [60, "2016-02-15T05:09:53Z", [0.14], "Raising from 8 to 14%. I still think Sedol will win, but given we're still pretty far from the first game of the match, I'd like to be a *touch* closer to the consensus. Feb 14: \"AlphaGo vs. Lee Sedol: Odds favor machine over man in $1 million Go match\" --> http://www.geekwire.com/2016/alphago-lee-sedol-whos-underdog-in-google-ai-million-go-match/ \u201cI\u2019m honored to play against an AI invented by Google,\u201d Sedol said in a statement. \u201cI regard this to be an important match in the history of Go, so I accept the challenge. I\u2019m confident that I can win the match.\u201d Hassabis said most Go players are giving Sedol the edge over AlphaGo. \u201cThey give us a less than 5 percent chance of winning \u2026 but what they don\u2019t realize is how much our system has improved,\u201d he said. \u201cIt\u2019s improving while I\u2019m talking with you.\u201d Online predictions favor the machine over the human, but just barely: The current odds on BitBet are slightly tipped toward AlphaGo, and the just-for-fun prediction from GoodJudgment gives AlphaGo just a bit more than a 50-50 chance of winning. For Hassabis, the AlphaGo project is about much more than beating one of the world\u2019s best Go players. The principles that are being put to work in the program can be applied to other AI challenges as well, ranging from programming self-driving cars to creating more humanlike virtual assistants and improving the diagnoses for human diseases. \u201cWe think AI is solving a meta-problem for all these problems,\u201d Hassabis said. Even if AlphaGo doesn\u2019t prevail next month, it\u2019s likely to be just a matter of time before the computer wins. That\u2019s how it went for Deep Blue, which lost to Kasparov in 1996 before triumphing a year later. But we can take solace in Hassabis\u2019 assessment that it will be decades before AI programs match humans in what\u2019s known as \u201cgeneral intelligence\u201d \u2013 that is, the ability to handle a wide variety of life\u2019s intellectual challenges rather than individual specialized tasks. ~~~ COMMENT: It's interesting to note that it seems like Hassabis almost has the attitude like (simply showing up) will be a big win for AI. Can you really lower expectations for a computer? Anyway, if I were reading the tea leaves on this one, I'd be inclined to think that this quick news hit is supportive of @GJDrew's point (https://www.gjopen.com/comments/comments/118716) that in the beginning, sponsors are willing to take the loss for publicity. But, having been burned (BADLY!) on some of these Just For Fun IFPs, I'm tempering my 'extreme' forecast."], [14327, "2016-02-15T04:49:28Z", [0.75], "Putting the odds back up.  The learning curve of this system is exponential - as it learns, it teaches itself more because it plays itself even as it observes the best moves on games provided by its database.  >>>Hassabis said most Go players are giving Sedol the edge over AlphaGo. \u201cThey give us a less than 5 percent chance of winning \u2026 but what they don\u2019t realize is how much our system has improved,\u201d he said. \u201cIt\u2019s improving while I\u2019m talking with you.\u201d Great quote from this article:\nhttp://www.geekwire.com/2016/alphago-lee-sedol-whos-underdog-in-google-ai-million-go-match/ The computer is teaching itself better and faster than anyone has in history. \nAnd if Lee Sedol makes a single mistake, I'm almost certain the program will exploit it. "], [19936, "2016-02-15T03:32:25Z", [0.75], "My friend George works on it and is working hard"], [17942, "2016-02-15T03:26:35Z", [0.88], ""], [20007, "2016-02-15T02:44:22Z", [1.0], ""], [17875, "2016-02-15T02:11:56Z", [0.0], ": )"], [1028, "2016-02-15T01:55:31Z", [1.0], ""], [18031, "2016-02-15T00:46:08Z", [0.8], ""], [335, "2016-02-15T00:23:21Z", [0.4], ""], [20000, "2016-02-15T00:15:36Z", [0.55], ""], [9823, "2016-02-14T23:23:21Z", [0.55], ""], [16989, "2016-02-14T23:03:50Z", [0.52], ""], [8183, "2016-02-14T22:17:23Z", [0.15], "I do not believe that AlphaGo  is intuitively intelligent in any way shape or form and the game has a lot of intuitive intelligence in it that Sedol has. I didn't rate the chance down to zero, because accidents can happen."], [1218, "2016-02-14T21:48:45Z", [0.2], ""], [10785, "2016-02-14T20:45:20Z", [0.75], "Go is a mathematical game with a limited set of solutions, therefor possible to calculate the winning moves. A - well programmed - computer can do this better/faster than a human being."], [746, "2016-02-14T20:17:35Z", [0.99], ""], [19979, "2016-02-14T17:45:15Z", [0.25], ""], [19965, "2016-02-14T15:41:33Z", [0.08], "The degree to which Lee Sedol is better than Fan Hui suggests to me that the AI can't be improved that much in the period of time between the matches.  But it's also not clear how much better AlphaGo is than Fan Hui."], [691, "2016-02-14T15:24:53Z", [0.58], ""], [3702, "2016-02-14T14:07:28Z", [0.3], ""], [19380, "2016-02-14T10:50:42Z", [0.22], "Decision points... \n1. Is Go a type of computing problem where a computer can conceivably be better at than all humans?\n2a. If so, has AlphaGo reached that level of capability yet? \n2b. If not, has it at least reached a level to beat Lee Sedol? 1. Most likely yes. (No strong evidence otherwise).\n2a. Unknown... that's why they're holding the match\n2b. Beating a particular player may be an easier problem, if the algos could be tuned to play against a player of his profile, or, could be a harder problem if algos are general purpose Go playing with goal of beating most players most of the time. (That is, if there some discontinuity in approaches to learning how to beat less skilled players vs. the very most skilled players.) There's some other edge conditions here, too, like AlphaGo is possible of winning if more computing resources dedicated to the problem, in which case another factor is what emphasis Google is placing on this project. In all, there are lots of things that have to go just right, so my best guess is Sedol is likely to win 4-1."], [19956, "2016-02-14T08:16:56Z", [0.4], ""], [909, "2016-02-14T05:45:41Z", [0.85], "They've already beaten Fan Hui, another Go master. I don't think this next match going any different. People forget that mental fatigue is a thing.  Garry Kasparov lost to Deep Blue when he got tired, the same thing will happen here as well.  For the match against Fan Hui: \nhttp://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html"], [709, "2016-02-14T04:50:50Z", [0.7], "I don't know much about Go, but I do work in machine learning.  Google's DeepMind team was acquired by Google for their work in building systems that could learn to play video games. This had previously been considered impossible (or rather - had been considered so hard that no one even bothered trying it at all). Since then they have developed programs that can outscore humans on a wide variety of video games, by considerable margins. I would be extremely confidence that AlphaGo would win, but the commentary from experienced Go players about the number of mistakes AlphaGo made gave me pause. However, reading the transcripts of other high-level games it appears mistakes are quite common(?!) in high level play (eg, move 130 in https://gogameguru.com/go-commentary-lee-sedol-vs-gu-li-jubango-game-7/). I don't completely grasp how significant this is, so I'm moderating my confidence. "], [12467, "2016-02-14T03:24:19Z", [0.8], ""], [19951, "2016-02-14T03:24:16Z", [0.23], "Articles I have read indicate that the level of the Go program is not high enough yet. I have little knowledge of Go, but not much reason to disbelieve the articles."], [19945, "2016-02-14T02:46:39Z", [1.0], ""], [14301, "2016-02-14T00:44:15Z", [0.45], ""], [231, "2016-02-14T00:24:56Z", [0.15], ""], [18011, "2016-02-13T23:43:43Z", [0.99], ""], [19941, "2016-02-13T23:02:06Z", [0.65], ""], [17875, "2016-02-13T21:40:29Z", [0.0], ": )"], [128, "2016-02-13T16:55:33Z", [0.4], ""], [691, "2016-02-13T16:24:04Z", [0.58], ""], [14258, "2016-02-13T14:45:07Z", [0.1], ""], [19918, "2016-02-13T12:53:30Z", [0.25], ""], [19903, "2016-02-13T06:41:19Z", [0.95], ""], [12595, "2016-02-13T05:36:57Z", [0.5], ""], [19427, "2016-02-13T01:34:58Z", [0.4], "Re-weighing DeepMind teams' statements as being stronger evidence that they will win."], [16592, "2016-02-12T23:12:07Z", [0.85], ""], [3320, "2016-02-12T21:54:49Z", [0.45], "I continue to rate AlphaGo's chances slightly better but still not quite at the Lee Sedol level.  Here's a useful link from a Go player with a lot of experience in programming computers to play the game.  https://smartgo.com/blog/lee-sedol-vs-alphago.html.  Here's his conclusion, stripped of a graphic showing a board position: March version of AlphaGo Google has five months to improve AlphaGo. So what can they do? During the Deep Blue match, engineers could make adjustments to Deep Blue\u2019s algorithm between games, such as fixing a bug after game 1. For AlphaGo, it may not be as easy. For example, move 31 in game 2 (shown below) was a mistake by AlphaGo (see Myungwan Kim\u2019s analysis). It\u2019s the right local move under the right circumstances, but AlphaGo doesn\u2019t have a sufficient understanding of that board position. How can they fix that issue? Feeding that position to the neural network won\u2019t help balance out what it has learned in 30 million other positions. There\u2019s no quick fix for that kind of mistake. Avoiding specific mistakes is easier. AlphaGo was not using an opening library in October, but Google could easily add that by March, making it at least possible to adjust play between games so Lee Sedol won\u2019t be able to exploit a particular joseki mistake in multiple games. There are many other improvements Google can make before March: Google can refine AlphaGo\u2019s neural networks. They used 30 million positions to train the value network for the Fan Hui match \u2014 they can use 100 million for the Lee Sedol match.\nThey can add extra training for the rollout policy. And then feed the improved rollout policy into the training of the value network.\nThey can fine-tune the balance between rollouts and the value network.\nThey can throw more computing power at the match itself. And the match will likely have longer time limits, so AlphaGo can calculate more during the opponent\u2019s time.\nAnd more. Google has a strong team that wants to win, and they\u2019ll have other ideas up their sleeves.\nGoogle has also expressed confidence, and they chose the opponent and timing for this next match. So I\u2019d expect the March version of AlphaGo to be significantly stronger than the one that played last October. Its reading is going to be even better. Its assessment of the global position will be improved. The main question is whether those improvements will be enough to remedy or at least balance out the weaknesses seen in the October version. Google made a huge leap forward with AlphaGo, creating a qualitative difference in computer play, not just a quantitative one. It\u2019s hard to tell what another five months of work and neural network training can do. Conclusion The AlphaGo from last October was very strong, but probably not strong enough to beat Lee Sedol. With five months of work, I think AlphaGo is going to be a different beast in March, and it will be a very exciting match. If I had to bet? Lee Sedol will lose a game or two but win the match.\n"], [12874, "2016-02-12T21:53:52Z", [0.32], ""], [691, "2016-02-12T19:43:45Z", [0.58], ""], [19218, "2016-02-12T18:10:03Z", [0.2], "Readjusting my estimate downwards to reflect: 1. Reading Go players' accounts suggests that AlphaGo is significantly weaker than Sedol. 2. Most high estimates seem to rely on naive ELO calculation, which seems full of bugs and much less reliable than an expert assessment of the computer's strength."], [19883, "2016-02-12T17:49:55Z", [0.15], ""], [18294, "2016-02-12T17:04:20Z", [0.45], "I don't see a 5-0 win for the computer in this match.  But the world champ has played enough matches that the program can be optimized to some degree for the match in question.  That is enough of an edge that I think the machine has a slight edge."], [915, "2016-02-12T16:38:55Z", [0.06], ""], [17875, "2016-02-12T16:19:15Z", [0.0], "I was originally at 75% given what I know about AI. I changed to 0% to over compensate for being a little high early on"], [19871, "2016-02-12T14:49:25Z", [0.75], ""], [19862, "2016-02-12T11:42:57Z", [0.8], ""], [19863, "2016-02-12T11:42:27Z", [1.0], ""], [10039, "2016-02-12T10:24:40Z", [0.29], ""], [19857, "2016-02-12T08:28:23Z", [0.85], "Merit so far"], [17875, "2016-02-12T02:26:45Z", [0.0], "I was originally at 75% given what I know about AI. I changed to 0% to over compensate for being a little high early on"], [19427, "2016-02-12T01:50:21Z", [0.36], "Simple Bayes' theorem calculation: 0.20 = Estimated prior probability of AlphaGo winning a hypothetical March match against Lee Sedol given that AlphaGo just defeated Fan Hui in October 0.95 = Estimated probability that upon publishing their paper in January, DeepMind would challenge Sedol in the manner they did, assuming the AlphaGo goes on to *win* (i.e. there's a 5% chance that even if AlphaGo was strong enough to defeat Sedol that DeepMind wouldn't have challenged him) 0.80 = Estimated probability that upon publishing their paper in January, DeepMind would challenge Sedol in the manner they did, assuming that AlphaGo goes on to *lose* ==> Posterior probability that they will win given that they challenged Lee Sedol to a March match = 0.20 * (0.95/0.80) = 0.24 Updating again, for the statements that DeepMind made about being \"quietly confident,\" '\"even chances\" and \"very disappointed\" if they lose the match:\n8** ==> Posterior probability that AlphaGo will win given the statements DeepMind made = 0.24 * (0.90/0.60) = 0.36 = **36% prediction** This exercise doesn't seem to be very useful since it's very difficult to pin down accurate \"(0.95/0.80)\" and \"(0.90/0.60)\" values and the final probability varies significantly depending on which values I select. Nevertheless, I'm going to revise my estimate down from 45% to 36%. -------- A better methodology seems to be to estimate a probability distribution for AlphaGo's ELO in the March match, do the same for Lee Sedol's rating (or just take his ELO as a fixed value), and then calculate the expected probability of AlphaGo winning. The only problem with this methodology preventing me from using it to make my estimate is that I don't know how to do the math and I don't have enough of a growth mindset to make the effort to teach myself right now. But I note that I think methodology is better since I would trust my intuitive estimate of the probability distribution for AlphaGo's ELO much more than I trust my estimate of the values used in the above two Bayes' theorem calculations."], [19841, "2016-02-12T01:12:37Z", [0.75], ""], [691, "2016-02-12T00:38:56Z", [0.61], ""], [16136, "2016-02-11T20:02:08Z", [0.4], ""], [421, "2016-02-11T19:39:28Z", [0.75], ""], [19829, "2016-02-11T19:36:59Z", [0.85], ""], [3320, "2016-02-11T16:08:54Z", [0.42], "I'm increasing my bet just slightly in favor of AlphaGo.  I'm impressed by the arguments that AlphaGo can continue to learn faster than any human, so can be expected to be even stronger in March.  I've thought about the report that AlphaGo was trained using only amateur games.  If so, why might that be, given that so many professional games have always been available?  I can think of one reason.  Amateurs make errors whose adverse consequences are often obvious after only one or a very few moves.  So if you're showing someone what not to do, review of games by weaker players makes sense.  Professionals almost never make outright blunders.  They lose by making suboptimal moves whose consequences may not be obvious and often not even identifiable without careful post-game analysis.  However, AlphaGo may well be at the point where it can learn from subtler mistakes.  If that's plausible, the program may well win. That said, what leads me to continue to think Lee Sodol will win are reminders that he's a lot stronger than Fan Hui and will be able to exploit subtle errors in AlphaGo play."], [19815, "2016-02-11T15:41:58Z", [0.63], ""], [241, "2016-02-11T14:58:50Z", [0.1], ""], [19170, "2016-02-11T13:47:03Z", [0.2], ""], [14948, "2016-02-11T12:42:53Z", [0.57], ""], [19803, "2016-02-11T08:22:15Z", [0.31], ""], [19805, "2016-02-11T08:19:48Z", [0.85], ""], [19801, "2016-02-11T06:30:06Z", [0.69], "Checkers was solved in 2007, by Chinook. Tic-Tac-Toe and Connect 4 are also solved games. Stockfish's strength is beyond compare at chess. It was only a matter of time before Go fell, and AlphaGo is demonstrating an approach that can deliver the goods. The combination of neural networks and Monte Carlo has already made Alpha Go the strongest Go program that ever existed. Out of 500 games, AlphaGo lost only one. Google has the hardware, and a large database of professional games to train on. Lee Sedol has only 5 games of Alpha Go's to train on. Lee's entire professional career, and his human errors can be easily exploited by a cryogenic calculator that knows no fear or time pressure. Lee can't compete.\nIn the end, a human will hit his genkai against a machine, but a machine has no limits. Lee won't be able to find any weakness in AlphaGo's impervious armour, but the same isn't true vice versa."], [19799, "2016-02-11T05:39:45Z", [0.75], ""], [773, "2016-02-11T03:03:08Z", [0.15], "Bsed on @einsteinjs comment\nhttps://www.gjopen.com/comments/comments/129537 am reverting to my prior analysis but excluding crowd impact (so 15 instead of 25).  Given his track record his opinion should carry weight."], [90, "2016-02-11T01:29:59Z", [0.4], ""], [19777, "2016-02-11T00:05:38Z", [0.25], ""], [16002, "2016-02-10T23:44:09Z", [0.75], ""], [498, "2016-02-10T23:13:16Z", [0.05], ""], [19770, "2016-02-10T22:56:05Z", [0.6], ""], [691, "2016-02-10T20:56:58Z", [0.61], ""], [6518, "2016-02-10T17:28:35Z", [0.7], ""], [17875, "2016-02-10T17:22:47Z", [0.0], "I was originally at 75% given what I know about AI.  I changed to 0% to over compensate for being a little high in the beginning and need to stay here a couple more days. "], [19706, "2016-02-10T16:16:47Z", [1.0], ""], [19712, "2016-02-10T16:05:42Z", [0.85], ""], [16160, "2016-02-10T15:34:56Z", [0.8], "AlphaGo already has beat Fan Hui, the European 5/5 games in a tournament format.  "], [357, "2016-02-10T14:56:49Z", [0.55], ""], [14777, "2016-02-10T14:17:23Z", [0.09], ""], [19680, "2016-02-10T14:08:04Z", [0.67], ""], [15166, "2016-02-10T13:18:06Z", [0.45], ""], [19652, "2016-02-10T12:47:36Z", [0.75], ""], [15590, "2016-02-10T12:30:11Z", [0.09], "Update: Did the probabilities, messed up subtraction.\nAssuming a probability of wining of about 87% if the 5-0 match was not a fluke, but a 50-50 result. We could estimate the elo rating of Alpha Go to be about 3300, a bit higher than stated in the original paper. Chess programs improve about 70 elo per year, so it COULD be estimated to have improved about 30 points by then. The first win was expected to 2020, but Deep Mind managed it in 2016  That is 3330 vs 3522 for Sedol. Still a 192 point diference. \nBut: the biggest uncertainty is how much can AlphaGo improve in this few months. I does seem that throwing more raw computer power can increase ELO a bit, but it is not clear by how much. I believe I can be underestimating the rate of improvement. let's say it can improve 5 times faster. That would be a decent ELO 3450 vs 3522\n72 point difference translate in about 25% per match. = 9% per game"], [10039, "2016-02-10T12:14:33Z", [0.51], ""], [15964, "2016-02-10T12:01:55Z", [0.0], ""], [14327, "2016-02-10T05:59:08Z", [0.52], "Thank you, @rfgarcia and @WJK for your comments concerning scores and probabilities. \n@adamgbell and @johnfdhartman also have excellent points.  http://www.usgo.org/news/2016/01/chinese-professionals-react-to-the-historic-alphago-win/\nVery interesting interview with some top players.  A large part of my re-assessment downward is a reassessment of purpose. \nSpecifically - why is Google hosting this competition?  -----Detour-----\nAlgorithms and machine learning can occur relatively quickly. You can build a simple algorithm in seconds. \nA more complex algorithm may be built over the course of several hours. \n\"Learning\" takes seconds. \nA model can change radically between iterations depending on what kinds of data are fed into it.  THUS \nIf it is true that AlphaGo was trained on amateur games ( @WJK ), then training on professional games could radically change the program. \n-----End Detour----- This brings me back to the question of purpose.  It seems to me that the overriding purpose of the match is likely to be:\nHow strong exactly is this algorithm? Their question likely isn't \"can this computer beat Lee Sedol?\" but \"How powerful is this algorithm that we have constructed?\"  That being said, additional questions that may be answered in March include:\n1. How fast can machines learn? \n2. Did Google use professional games to train its latest iterations? If so - how much of a difference does that make to training on amateur games? While the Google team has \"expressed great confidence\" in their creation, it doesn't sound to me like their purpose is to win and I feel I need to take this into account in forecasting. "], [5001, "2016-02-10T05:32:05Z", [0.25], "Previously, I underestimated the uncertainty about how and how fast AlphaGo can improve. So correcting. "], [19427, "2016-02-10T03:38:33Z", [0.45], "Updating from 40% to 45%. I think that back in October I would have thought it unlikely that AlphaGo's strength would increase by enough by March to have a good chance at winning a match against Lee Sedol at that time and so would have estimated that the probability of AlphaGo winning such a match at that time to be low, around 20%. I adjust this 20% prior probability upwards significantly because I see the act of DeepMind challenging Lee Sedol in early March (rather than later in the year) with a $1 million prize as evidence that they are stronger than the prior. I adjust my prediction up further because these two comments also seem to be evidence that AlphaGo is at least at Lee Sedol's strength. It seems unlikely that either of them would have said these things if they didn't believe that AlphaGo had good chances: David Silver (DeepMind computer scientist): \"I haven\u2019t put any money on AlphaGo winning, but I do think we have a lot of reputation riding on this bet. So let\u2019s just say we\u2019ll be very disappointed if we lose the match in March. But you never know, anything is possible. Humans inevitably have a lot of tricks up their sleeve that we\u2019re not able to train against.\"\nhttp://www.nature.com/news/go-players-react-to-computer-defeat-1.19255 Demis Hassabis (Deepmind CEO): \"I think it is 50/50. So, we'll have to wait and see. We are quietly confident, but I know Mr. Lee is as well, so I would say it's even chances.\"\nhttps://youtu.be/_r3yF4lV0wk?t=27m10s All this said, I'm still not updating to 60+% because I think that the prior does deserve some weight."], [19623, "2016-02-10T02:41:21Z", [0.85], ""], [18888, "2016-02-10T02:15:37Z", [0.02], "Thanks to @rfgarcia for the catch from the original paper: http://www.gjopen.com/comments/comments/129458 Basically copying my reply to his comment here: I used the same proxy as @rfgarcia for AlphaGo's Elo compared to Fan Hui's, but couldn't find a reliable source for Fan Hui's absolute Elo rating. Don't know how I missed the original paper's evaluation of both AlphaGo's (3140) and Fan Hui's (2940) Elo ratings. In my first analysis, I didn't take into account 5 additional, unofficial games of which AlphaGo only won three. Taking those into account, AlphaGo's Elo (assuming it wins 8/10 games 50% of the time) is 3181 (241 greater than Fan Hui's), which is almost spot on with Google's estimate. I think splitting the difference makes sense (3160), then I'll add 40 Elo as AlphaGo is learning faster than chess programs do (who historically improve ~40 Elo/year), making AlphaGo's Elo rating 3200. Lee SeDol's Elo rating is 3522, like you said. Lee SeDol thus has an 86% chance of winning a game, and since it's a five-game series, that means Lee SeDol has a 98% chance of winning the series. Huge swing for me, who started at 90% the other way and reduced to 76%. My major flaw was assuming Fan Hui is MUCH stronger than he actually is, as @balakirev had warned me. The probability of Fan Hui winning a given game against Lee SeDol is about 3%, not 31% as I had initially estimated. I guess there really is a huge difference in strength between European Go and Chinese/Korean Go that I didn't account for. Future changes would depend on whether or not AlphaGo improves even more than double the speed of chess computers or if the analysis of Lee SeDol's and AlphaGo's Elo ratings cannot be compared for some reason."], [15261, "2016-02-10T01:47:09Z", [0.3], ""], [19617, "2016-02-10T01:27:41Z", [0.65], "Based on what AlphaGo has achieved, I'm quite optimistic that it will win. "], [7042, "2016-02-10T00:19:55Z", [0.62], ""], [15590, "2016-02-09T23:47:53Z", [0.2], "Assuming a probability of wining of about 87% if the 5-0 match was not a fluke, but a 50-50 result. We could estimate the elo rating of Alpha Go to be about 3300, a bit higher than stated in the original paper. Chess programs improve about 70 elo per year, so it can be estimated to have improved about 30 points by then. that is 3330 vs 3522 for Sedol. Still a 92 point diference. That is about 20% per match. "], [12591, "2016-02-09T21:59:45Z", [0.65], ""], [14268, "2016-02-09T21:43:11Z", [0.4], "Go is a hard game, and there's a big difference between 700th and 1st ranking in playing it. The team may be able to bridge that gap in a few months, but Deep Blue lost the first match with Kasparov and I think it's likely the top player here will win also, "], [19218, "2016-02-09T21:28:35Z", [0.4], "Before I said 27%, reasoning (a) that Google was silent and Sedol was confident and (b) Google's incentives are if anything to challenge before it's likely to win.  But WJK says Google is saying \"50/50\" and \"quietly confident\". Sedol still seems more confident, and (b) remains valid. "], [1007, "2016-02-09T21:20:06Z", [0.85], ""], [1111, "2016-02-09T19:55:02Z", [0.9], "Quite certain that it will. "], [19390, "2016-02-09T19:34:20Z", [0.3], "Based on expert game analysis, AG seems to have approx 7th dan level. According to ELO ratings, the chance of Sedol winning would be around 80%. But, AG has probably become stronger over the last 5 months (since defeating Fan Hui). So I subtract 10% (wild guess) in order to adjust for this learning curve"], [17875, "2016-02-09T18:53:23Z", [0.0], "AlphaGo's ability to scale on hardware and Googles ability to make it scale on programming has really given AlphaGo a unique chance to increase it's learning rate against tougher professional opponents where it is counter-intuitively likely to do even better than against worse players (it's optimized for winning % not the best move).  So I'm capitulating that I lost this question but I'll take one for the team and improve others scores"], [4794, "2016-02-09T18:50:00Z", [0.35], ""], [19381, "2016-02-09T17:22:45Z", [0.3], "https://www.quora.com/Can-Google-AlphaGo-beat-world-Go-champion-Lee-Sedol-in-March"], [18315, "2016-02-09T17:05:06Z", [0.65], ""], [12828, "2016-02-09T16:49:59Z", [0.65], ""], [11080, "2016-02-09T15:13:17Z", [0.2], ""], [3322, "2016-02-09T14:47:13Z", [0.61], ""], [10930, "2016-02-09T14:32:21Z", [0.35], ""], [10930, "2016-02-09T14:29:17Z", [0.45], "@nicholas found better information on the ELO ratings, via hacker news (https://news.ycombinator.com/item?id=10981679) : \"AlphaGo played Fan Hui using 1202 CPUs and 176 GPUs.\nIt's strenght was assessed to be 3140 ELO on the scale used by www.goratings.org/ (BTW, this is different from the European ELO system)\nThat would put it at #279 in the world. Fan Hui is number 633 at 2916. AlphaGo's next opponent will be Lee Sedol, #5 at 3515.\nThe single computer version of AlphaGo is estimated to be 2890 ELO, which would be #679 in the world. It's is closer to what we might be playing against on our laptops and phones but still 48 CPUs and 8 GPUs.\" This would suggest that the version of alphaGo from the paper would have less than 10% chance.  Based on that I'm going to adjust down from 55 to 35."], [691, "2016-02-09T14:13:46Z", [0.61], "The thing about AI, I am reliably informed, is that sometimes the machine gets smart even faster than its programmers expected it to.  And sometimes it's slower.  Even though this one hasn't played officially in a while, it has presumably been gulping down data, including game play-by-plays, much faster than you or I care to know about.  In such matters I have taken to siding with the machine.  Luckily I shall be safely dead before we all find out whether they make a better job of it than we have.  "], [14841, "2016-02-09T14:06:37Z", [0.24], ""], [16292, "2016-02-09T14:04:29Z", [0.45], ""], [19135, "2016-02-09T13:46:41Z", [0.12], ""], [19538, "2016-02-09T13:19:42Z", [0.15], "Go software is competing at professional level for quite a while now (~ 4 Dan), but is still quite a bit away from the very top. In chess, it took longer from competing at grandmaster level to beating Kasparov. That said, Google is throwing a lot of money at AI research, and has exceeded expectations in the past."], [19453, "2016-02-09T11:00:29Z", [0.12], "AlphaGo's current rating is widely reported as around 3140- this bears out, as it won 5 out of 7 matches against Fan Hui (Whose 2970 ELO would make him about 27% likely to win any game against a 3140 opponent, making winning 2 out of 7 a fairly reasonable outcome). Lee Sedol's ELO appears to be 3515, which means that he has a roughly 90% chance of expected victory against AlphaGo in any one game. Stringing these probabilities together, the odds of AlphaGo winning 3 games out of a five game series is about 1%. I have adjusted upwards for the possibility that AlphaGo is being trained specifically against Lee Sedol / professional dan matches and will be markedly improved in the 5 months between the Fan Hui match and the Lee Sedol match as well as the fact that the crowd wisdom average is much higher than mine. "], [18683, "2016-02-09T09:52:55Z", [0.85], ""], [919, "2016-02-09T08:26:12Z", [0.75], ""], [10039, "2016-02-09T06:51:37Z", [0.43], ""], [436, "2016-02-09T04:50:26Z", [0.53], "I found an interesting analysis from a Go professional, who critiqued (and complimented) AlphaGo's play in the Fan Hui match. There's a decent chance that Fan Hui could've won a game or two had one single move been different (the sort of thing Lee Sedol would never miss). https://www.reddit.com/r/MachineLearning/comments/43fl90/synopsis_of_top_go_professionals_analysis_of/ Another good comment in that thread explained the vast gulf between Hui and Sedol in a way that helped me better understand what AlphaGo will have to accomplish in order to improve to Sedol's level. Amateur chessplayers can jump from 1200 to 2000 in a few years without doing anything remarkable, but the jump from 2000 to 2800 may take decades (or simply be impossible). I'm not sure how well Go ratings correspond to this increasing difficulty, nor how AlphaGo's progress will compare to that of a human Go player. So I'm adjusting my prediction downward. But I think I'd still bet even money on AlphaGo, because this has been a bigger mainstream news story than I expected (who cares about Go?), which makes me think that AlphaGo's creators will be pouring a lot of resources into the program's training over the next few months (and perhaps even testing it against other human masters). I wonder whether they can hook it up to an online Go network and find human opponents that way? Aside: I wouldn't put much emphasis on Lee Sedol being \"nervous\" or \"fatigued\" against AlphaGo. It doesn't seem as though Fan Hui suffered much in those ways -- his games were almost playful at times, as though he were testing AlphaGo in a casual way. Go also feels less high-pressure than chess, in that moves are based on intuition and it's easier to recover from a minor mistake (this is based on the limited reading I've done on Go, plus a lot of chess experience -- I could be wrong)."], [19527, "2016-02-09T04:25:14Z", [0.2], "AlphaGo clearly isn't there yet.  I'm skeptical it can traverse the rest of the difference in 5 months; especially since the previous big leap was insight-based rather than brute-force-based.  To keep up that speed of improvement they'd need another insight; I don't think that massively parallel power will be enough.  I don't yet have reason to believe AlphaGo's programmers are in that special class that only says \"We expect to win\" in possible worlds where they're mostly set to win - that's a rarefied level of honesty and rationality.  AlphaGo may well win in 2017 or 2018, but a 0-5 shutout would not surprise me here."], [19170, "2016-02-09T03:38:17Z", [0.3], ""], [16025, "2016-02-09T03:24:06Z", [0.55], "Computers have played Go and beaten amateurs but, before Google's victory against the French champion, experts had predicted that it would take another 10 years until a computer could beat the world's best Go professionals. This prediction was clearly wrong and people are underestimating Google's AlphaGo."], [773, "2016-02-09T03:10:26Z", [0.25], "In Gary Markus discussion of the hybrid neural net / decision tree approach being used:\nhttps://backchannel.com/has-deepmind-really-passed-go-adc85e256bec#.bz6s0q2c7 He points to an interesting discussion taking place in the Hacker News boards:\nhttps://news.ycombinator.com/item?id=10981679 In which a commenter \"ponta\" offers following (citing a Nature paper):\n\"AlphaGo played Fan Hui using 1202 CPUs and 176 GPUs.\nIt's strenght was assessed to be 3140 ELO on the scale used by www.goratings.org/ (BTW, this is different from the European ELO system)\nThat would put it at #279 in the world. Fan Hui is number 633 at 2916. AlphaGo's next opponent will be Lee Sedol, #5 at 3515.\" Plugging 3140 and 3515 into the ELO win probability calculator with weighting factor \"50\" (not positive that is correct weighting factor so perhaps this estimate could be improved upon with a better understanding of this tool)\nhttp://bzstats.strayer.de/bzinfo/elo/?lang=en The calculator gives AI a win probability of 10% for this matchup.   Noting that the AI is likely still under development and it is more likely that the AI will improve materially prior to the match than the human doing so, I will generously add another 5% to win probability (15%). Finally noting that the GJ crowd is currently at 55% and the crowd is likely smarter than myself so I'll bump it up to 25% chance of AI win."], [19515, "2016-02-09T01:35:39Z", [0.9], "The tech giant can spare plenty of resources to ensure its victory."], [19476, "2016-02-08T23:43:38Z", [0.15], "- The last significant evidence suggests supercomputers don't always win at the first challenge (Kasparov vs Deep Blue 1996), meaning they are perfectible. \n- Most Korean professionals favor Lee Sedol (they can better assess the difficulty of programming Go algorithms than I do).\n- Lee Sedol is confident he can win.\n- Go isn't chess (10^761 vs 10^120 possible games).\n- Lee Sedol has actual evidence on the power of AlphaGo since the computer already played Fan Hui and can accurately estimate its capability of beating it."], [10930, "2016-02-08T23:11:22Z", [0.55], "Some Experts:\nFan Hui has the rank of 8 dan, 10 levels below the top professionals such as Lee Sedol 9P. Lee should have no trouble winning in March. - Edward Cherlin So let\u2019s just say we\u2019ll be very disappointed if we lose the match in March. But you never know, anything is possible. - Creator [In the March match], no offence to the AlphaGo team, but I would put my money on the human. Think of AlphaGo as a child prodigy. All of a sudden it has learned to play really good Go, very quickly. But it doesn\u2019t have a lot of experience. What we saw in chess and checkers is that experience counts for a lot.\" - Computer scientist at the University of Alberta, Edmonton,  I would say each has a 50% chance. - Hajin Lee The AlphaGo from last October was very strong, but probably not strong enough to beat Lee Sedol. With five months of work, I think AlphaGo is going to be a different beast in March, and it will be a very exciting match. If I had to bet? Lee Sedol will lose a game or two but win the match. - https://smartgo.com/blog/lee-sedol-vs-alphago.html Google's own estimates of AlphaGo's strength is around the ~5p level. If that's correct then Lee Sedol should be able to clean up nicely. \" The way AlphaGo works interesting: it doesn't actually try to maximize points, but rather to maximize the probability it will win. There's a subtle difference there: when it is ahead, it will play \"safe\" to keep its lead rather than try to get more points. It would rather have a 99.9% chance of winning by half a point than a 99.85% chance of winning by 1.5 points. Because of this, it won't actually \"try\" against a weaker person, but will actually hold back. It will take a strong player to bring out its true strength.\" - reddit ELO based Answer: Based on conventional Elo rating distributions, the theoretical probability of Fan Hui winning a single game against Lee is around 25%.  Including informal matches, Fan Hui won 20% of the time against Alpha Go.  Of course, that is a very small about of data, but that would give Alpha Go slightly higher ELO rating"], [19218, "2016-02-08T21:12:33Z", [0.27], "1. Sedol thinks he can win, and he has much better information than I have. 2. Google hasn't said it thinks it's going to win. 3. I can't infer from Google's challenging Sedol that it thinks it will win. Google would probably get more publicity with a loss followed by a win, as IBM did with Deep Blue."], [19453, "2016-02-08T20:40:27Z", [0.12], ""], [5083, "2016-02-08T19:48:29Z", [0.7], "I give a creative well-trained human a 30 per cent chance. ingenuity can crap on a computer."], [19486, "2016-02-08T19:19:37Z", [0.65], "Beat a lower level pro who had not played professionally in a few years. Lost 2 informal games before 5-0 formal."], [16223, "2016-02-08T18:51:06Z", [0.99], "The effort was making a machine that can win. They have done that. At 5 games, I give the edge to the machine because it is more likely Lee Sedol makes a mistake. At one game I would be less convinced (though still think the machine would take it). "], [19481, "2016-02-08T18:40:39Z", [0.05], ""], [19481, "2016-02-08T18:40:16Z", [0.99], ""], [16726, "2016-02-08T17:47:26Z", [0.58], ""], [17883, "2016-02-08T17:42:43Z", [0.6], "Deep Blue beat Garry Kasparov 3.5/6 games in 1997. Kasparov had a chance to win but made a mistake. The Komodo chess engine recently beat US #1 Hikaru Nakamura 2.5/4 games. The average winning chances of the machines in both the Kasparov and Nakamura matches was 60%. It seems likely that Google's AlphaGo will do something similar: Edge out Lee Sedol by capitalizing on one of his few mistakes and turning it into a decisive advantage 2/3rds of the way into the match."], [15102, "2016-02-08T17:36:23Z", [0.44], ""], [19472, "2016-02-08T17:08:53Z", [0.67], ""], [18762, "2016-02-08T17:05:30Z", [0.67], "Lee Sedol has a 72% winning percentage. Assume Sedol's losses are to slightly better than average expert players. Clearly Alphago is better than an average expert, having beaten Fan Hui 5-0.  How much better? maybe only slightly better, but also up to many multiples better or even unbeatable. the math of Sedol probability losing may matter, but for me the more important question is how good is AlphaGo compared to him and the range out outcomes is a) large and b) the majority of these outcomes are that AlphaGo is better and c) some portion of these outomes makes it unbeatable by Sedol. "], [1824, "2016-02-08T16:48:05Z", [1.0], ""], [1150, "2016-02-08T16:29:22Z", [0.75], ""], [19470, "2016-02-08T16:23:41Z", [0.1], "Looking at the games, Alphago showed some serious mistakes that won't be easy to fix.  While it's general fighting is likely to be stronger than Lee Sedol's, it makes too many bad exchanges."], [19427, "2016-02-08T16:22:47Z", [0.4], "I made a $500 bet at even odds that Lee Sedol will win the match against AlphaGo. If I win, I intend to donate my winnings to one of GiveWell's top charities (probably the Against Malaria Foundation). At the time of the bet I would have assigned a ~30% probability estimate to AlphaGo winning, however I have since learned that AlphaGo was trained on amateur games rather than professional games (6d-9d games rather than 6p-9p games). Learning this raises my estimate for two reasons: (1) there is a possibility that Google will train AlphaGo on higher-level games (professional games) before the March match and (2) even if AlphaGo is not trained on higher level games, the fact that AlphaGo played at the level it did in October despite being trained on these lower level games shows that it is capable of learning and improving more by playing itself than I thought it could, so there is more potential for it to learn enough to reach or surpass Lee Sedol's level. I note that @Dima-K updated their prediction from 5% to 10% upon learning this: \"Going up because I just realized that a training set is likely to be based on pro games, not on the amateur online database they used earlier.\" I think it warrants a greater update, hence my 30% to 40% update. Now, backtracking, I couldn't find any good base rates to start my prediction at. I noticed that a confident consensus seemed to be that October-AlphaGo was much weaker than Lee Sedol, so the question \"How much can AlphaGo improve by March?\" is really the main factor that affects my probability estimate. After learning more about the difference in skill level between Fan Hui and Lee Sedol, it initially seemed likely to me that  AlphaGo would improve by some amount, closer to Lee Sedol's level, but wouldn't be able to improve by the rather-large-seeming amount necessary to reach or surpass Lee Sedol's level. Reading some of the rationales that people wrote here, there were a few things I noticed: (1) Those who wrote long rationales (which I assumed correlated with how much time they spent thinking about this, and gathering information) tended to have lower forecasts. (2) The few \"superforecasters\" I stumbled into all had <50% forecasts, some much lower than 50% (Update: @einsteinjs 8% https://www.gjopen.com/comments/comments/123061), @Jean-Pierre 10% https://www.gjopen.com/comments/comments/118530 and @GJDrew 25% https://www.gjopen.com/comments/comments/122608). (3) Most of those who made forecasts of >60% seemed to only mentioned reasons to justify their high probability estimate without mentioning counter-arguments (confirmation bias). For example, @Abusawyer said \"Google wouldn't agree to the deal if it was not reasonably confident of winning\" and predicted 75%. Other plausible reasons why Google might offer / agree to the deal: (1) Test to see how good AlphaGo actually is / better determine AlphaGo's playing strength. (2) Learn something else from the match against the strong Lee Sedol (2) They may have decided to challenge him earlier near October as soon as they saw that AlphaGo was stronger than Lee Sedol. Why wait more than 5 months to try against the next top professional? Even if they weren't confident that they would win against Sedol, it still seems plausible to me that they would challenge him because it's the next logical step. IBM challenged Kasparov before Deep Blue was strong enough to defeat him in the match. Google may have even less pressure than IBM had to do well. Although I note that @Abusawyer has a great Brier score: 0.072 (median: 0.203). Also see: https://www.gjopen.com/comments/comments/118626 I also note that the consensus probability has been going down over time. I wonder if this is due to the naive view that \"AlphaGo won 5-0 in October, so it will probably win in March too\" being more prominent initially. Finally, my disagreement with those informed forecasters who are predicting ~10% just stems from general uncertainty. I just don't feel that I can be that confident. Although, at the same time I am not confident that I shouldn't be assigning a probability that low. I am not a Go player. I played a couple dozen blitz games online once after learning the rules a couple years ago on KGS Server, but don't think I ever got below ~15-20k. A thread I made on the Go subreddit asking: \"How likely is AlphaGo to win 5-0 against Lee Se-dol in March?\" https://www.reddit.com/r/baduk/comments/43xret/how_likely_is_alphago_to_win_50_against_lee_sedol/"], [1614, "2016-02-08T15:34:21Z", [0.75], ""], [19464, "2016-02-08T15:29:53Z", [0.97], "1. Google has vast resources in the form of it's server farms and the machine learning expertises of it's employees.\n2. Google has a long history of getting things right when it comes to ML.\n3. AlphaGo already won 5-0 against a 3-time European champion. That's not even close, it's exponentially more meaningful than winning 3-2. Both Google and Fan Hui have a strong incentive to protect their reputation, so despite the aforementioned game being behind closed doors, there was probably no payoff involved."], [19253, "2016-02-08T15:27:24Z", [0.2], ""], [17875, "2016-02-08T14:50:05Z", [0.01], "@bpoppe  thanks for your analysis today, it was helpful to understand why someone would be at 70%.  Please explain further how you decided on 70% and not 65%-75%."], [15318, "2016-02-08T14:06:59Z", [0.7], "Given what we've seen from Google, IBM, etc. in machine-learning, and the fact that AlphaGo recently beat the 3-time European champ.  From what I can find, Lee Sedol has not played Fan Hui (the European Go champ which AlphaGo recently defeated), which would make this analysis easier in my opinion.  I did not go to the lengths of Sedol beat X who beat Fan (sports enthusiasts would tell you this doesn't matter anyway).  I still think it's reasonably likely, given that the European champ recently went down."], [15043, "2016-02-08T13:28:04Z", [0.75], ""], [691, "2016-02-08T13:06:35Z", [0.71], ""], [17695, "2016-02-08T12:29:04Z", [0.45], "It's Google, they're good, but they never totally win the first time."], [19408, "2016-02-08T11:58:51Z", [0.0], ""], [19449, "2016-02-08T09:45:24Z", [0.45], "AlphaGo's current world rank is estimated in double digits. A win for AlphaGo is plausible, but I can believe the world's top Go player has faculties it can't yet accommodate."], [16932, "2016-02-08T09:19:52Z", [0.31], "early days, and so far it has beaten only #250-odd ranked player..."], [18871, "2016-02-08T07:55:57Z", [0.79], "There is pattern when humans play which can be learned by the AI which it 79% chance to win. But given that the AI has not been pre-programmed to win there s a 21% chance for Sedol to win."], [12482, "2016-02-08T07:29:07Z", [0.05], "(1) It usually takes several trials before a computer bits the human champion.\n(2) The IBM program that bit Kasparov used a lot of historical data about games. As far as I know there is no similar database in regard to Go.\n"], [17875, "2016-02-08T07:16:29Z", [0.05], ""], [17054, "2016-02-08T06:52:00Z", [0.7], ""], [18679, "2016-02-08T06:50:01Z", [0.25], ""], [11756, "2016-02-08T05:10:50Z", [0.95], ""], [12107, "2016-02-08T03:49:17Z", [0.75], "The use of instinct to make a choice from a very large pool of choices is over-rated. The Google 'AI's' ability to learn its opponent's 'instinctual plays' should give it an advantage."], [12467, "2016-02-08T03:44:47Z", [0.9], ""], [18056, "2016-02-08T03:13:18Z", [0.4], ""], [13055, "2016-02-08T02:32:02Z", [0.75], ""], [6903, "2016-02-08T02:13:06Z", [0.55], ""], [18489, "2016-02-08T01:57:04Z", [0.25], "See http://www.numerama.com/sciences/143423-le-grand-combat-de-lia-de-google-contre-le-champion-de-go-sera-diffuse-en-direct.html. Also : http://www.wired.com/2014/05/the-world-of-computer-go/"], [16215, "2016-02-08T01:36:59Z", [0.89], ""], [11242, "2016-02-08T00:42:48Z", [0.55], "The speed of AlphaGo's learning should prepare it."], [19050, "2016-02-08T00:38:00Z", [0.35], ""], [14133, "2016-02-08T00:07:28Z", [0.75], "Google wouldn't agree to the deal if it was not reasonably confident of winning."], [15785, "2016-02-07T23:47:08Z", [0.08], "I read the person, who was beat isn't that good. It took longer for Deep Blue to win than the first match against Kasparov. I expect a similar outcome."], [8873, "2016-02-07T23:40:37Z", [0.25], "Software isn't good enough yet."], [1623, "2016-02-07T23:24:54Z", [0.8], ""], [19429, "2016-02-07T23:19:44Z", [0.8], ""], [3320, "2016-02-07T23:13:57Z", [0.36], "There is, by the way, a slight ambiguity in the word \"beat\" in the question.  I'm interpreting it as \"win the match -- i.e., win at least 3 of 5 games.\"  However, if \"beat\" means \"win even one game,\" I'd change my \"yes\" percentage to about 70.  Perhaps the GJ monitors should clarify. It's hard for non-Go players to realize how much stronger someone like Lee Sedol is than even mid-level pro that AlphaGo beat.  But a 5-0 win against any professional is a remarkable AI achievement, and the AlphaGo approach is different from the approaches available years ago against chess masters.  Go isn't just computational.  The number of possible moves is too large for brute-force solutions, even with immense computing power.  (One common way of emphasizing that  goes like this.  If you were to have played an entire game during every second that's elapsed since the Big Bang, you wouldn't nearly have exhausted the number of theoretically possible games.)   However, as I understand it, AlphaGo learns from experience by reviewing the outcome of many thousands of games.   For many years it's fairly easy to program a computer to excel in a tactical battle in a small area of a Go board.  Similarly, it's not too hard to identify the so-called \"big\" (strategically promising) moves around the board, especially early in a game.  What's hard -- for computers and for humans -- it's to know when to switch from one mode to another.  It's like a market-share problem, involving (for example) competing fast-food restaurants.  Fail to get options on enough high-traffic corners while investing too much in winning a local fight, and you end up a mom-and-pop operations vs. MacDonalds.  Conversely, get all those desirable options but fail in developing a few of them properly, you end up like some would-be billionaire whose ambition exceeds his ability.   Players like Lee Sedol have an astonishingly well developed instinct for making moves whose strategic significance is huge but not immediately obvious, combined with an encyclopedic knowledge of winning end-game tactics (which are indeed pretty much computational).  But so, apparently, does AlphaGo.   And it's no doubt already replayed all of Lee Sedol's games to date, trying out hundreds if not thousands of possible \"what if\" variations at what look like pivotal points.   If I learn that the 5-game match is to be played fast (e.g., a game every day), I'll probably put more chips on the tireless computer."], [10762, "2016-02-07T22:48:01Z", [0.13], ""], [12345, "2016-02-07T22:34:43Z", [0.78], ""], [5001, "2016-02-07T22:28:26Z", [0.1], "Going up because I just realized that a training set is likely to be based on pro games, not on the amateur online database they used earlier."], [19423, "2016-02-07T22:13:42Z", [0.75], ""], [1222, "2016-02-07T21:49:29Z", [0.45], ""], [14307, "2016-02-07T21:27:03Z", [0.7], "I looked at the timeline of the progression of chess-playing computer programs, and am losing confidence in how fast a Go program might progress. "], [131, "2016-02-07T21:16:37Z", [0.02], "Lee Sedol is going to smash AlphaGo. "], [3467, "2016-02-07T20:50:13Z", [0.6], "It seems like the game is based mostly on numerical calculations."], [19413, "2016-02-07T20:41:56Z", [0.4], "The first time a chess computer beat a pro human was in 1989. It beat the world champ in 1997. \nJust because a go computer won once, does not mean that it will win again. On the other hand, computers advance MUCH quicker than they did before, so it won't be 8 years before AlphaGo can beat Lee Sedol. This program is also using an algorithm that analyzes thousands of go games to select the best possible move that will work out well in the long term. But that won't stop players who use unconventional strategies. I'd have to estimate it's still most likely that Lee Sedol would still win, but not by much."], [19413, "2016-02-07T20:40:01Z", [0.1], ""], [991, "2016-02-07T20:34:38Z", [0.25], ""], [1450, "2016-02-07T20:16:49Z", [0.2], "I put the prior somewhere below 10% based on what some other commentators have stated about the relative Elo ratings.  I then adjusted for the level of uncertainty and AlphaGo's learning since those games.  "], [7523, "2016-02-07T19:57:37Z", [0.85], ""], [19403, "2016-02-07T18:35:01Z", [0.55], "I have read several compelling analyses about the skill difference between the prior opponent and the current opponent.  The consensus for them seems to be that Lee Sedol is not merely a slightly better player, but an entirely different class. This is tempered by the fact that the defeat of the prior master was so convincing (5-0, with no one describing the games as close-won)."], [12879, "2016-02-07T17:54:06Z", [0.6], ""], [15844, "2016-02-07T17:36:17Z", [0.65], ""], [241, "2016-02-07T16:28:06Z", [0.05], ""], [6552, "2016-02-07T14:07:40Z", [0.25], ""], [2004, "2016-02-07T13:57:47Z", [0.98], ""], [12874, "2016-02-07T13:50:09Z", [0.24], ""], [19386, "2016-02-07T12:54:50Z", [0.85], "!"], [691, "2016-02-07T12:34:39Z", [0.76], ""], [1629, "2016-02-07T12:00:38Z", [0.67], "The AlphaGo looking very strong, very good chance it will beat Lee Sedol in March, but untill I find out more about Go I'm going for the high 60ies untill I find out more. http://www.theguardian.com/technology/2016/jan/27/google-hits-ai-milestone-as-computer-beats-go-grandmaster http://www.theguardian.com/technology/2016/feb/05/google-ai-alphago-world-no-1-lee-se-dol-live-broadcast"], [15621, "2016-02-07T10:29:36Z", [0.59], "As it learns by playing itself the rate of improvement should be very rapid. Google unlikely to have offered challenge unless they are happy that it will at a minimum be competitive. They probably have some idea what level Sedol is at, but we have really no idea how good or potentially good AlphaGo is."], [19377, "2016-02-07T07:31:10Z", [0.27], ""], [11312, "2016-02-07T07:26:59Z", [0.77], ""], [5001, "2016-02-07T06:28:56Z", [0.05], "Affirming. A 9p-level professional not very impressed: \nhttps://www.youtube.com/watch?v=NHRHUHW6HQE#t=1h20m00s \"Easy money [for Lee Sedol]\", \"among top professionals, no one kind of worries right now\", \"I hope she can play stronger than ... this\""], [18845, "2016-02-07T02:48:40Z", [0.42], ""], [14643, "2016-02-07T02:33:35Z", [0.22], "I thought AlphaGo's result against Fan Hui (counting points difference at end of each game) was quite close, way closer than the 9p vs 2p gap. I'm guessing the next match will be 4-1 or 3-2, Lee. AlphaGo probably has processed all the real game data out there already, and improvements through playing itself I think will be much slower than learning from real past games."], [18845, "2016-02-07T02:29:48Z", [0.68], "Based on the stats located in the Google blog post about the topic, AlphaGo can predict the moves of an opponent 57% of the time.(http://www.siliconbeat.com/2016/02/05/googles-alphago-to-take-on-world-champ/) This led me to an initial baseline.  Along with this,  I used the information from an article published in the Nature International Weekly Review of Science that stated  the AlphaGo program boasts a 99.8% winning rate against other Go programs.(http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html) "], [14841, "2016-02-07T02:12:26Z", [0.33], "The player beaten 5-0 is quite a way below the elite level of GO, however not that far behind, so I rate AlphaGo with 1 in 3 chance."], [14877, "2016-02-06T23:43:41Z", [0.3], "Lee Sedol is the best player in the world. He has the advantage of knowing that there is a computer program capable of playing at elite level (Fan Hui was very surprised at being beaten). One could surmise that Lee will be ready for AlphaGo. On the other hand, AlphaGo will have improved in the meantime."], [19144, "2016-02-06T22:25:28Z", [0.18], ""], [17875, "2016-02-06T21:23:38Z", [0.05], "AlphaGo plays better against tougher opponent and Lee knows his opening is the weakest part of his game. Should be fun to watch"], [961, "2016-02-06T21:12:58Z", [0.85], ""], [10039, "2016-02-06T20:31:48Z", [0.85], ""], [2010, "2016-02-06T20:16:13Z", [0.0], ""], [14613, "2016-02-06T20:07:06Z", [0.79], ""], [18914, "2016-02-06T19:48:24Z", [0.35], "https://www.youtube.com/watch?v=dmR0BiGvXUw (Chinese Pro commentary on 2 of the games between AlphaGo and Fan Hui in Chinese) To summarize:\n1 In AlphaGo's best game of the five, it played quite well, at a normal professional level.\n2 AlphaGo DOES make mistakes; and even relatively simple mistakes.\n3 Fan Hui performed VERY poorly throughout the 5 games. He did not play at his best at all. \n"], [19348, "2016-02-06T19:45:58Z", [0.22], ""], [19343, "2016-02-06T19:02:02Z", [0.6], "I don't think Google would enter the match if it didn't thing it had better than even odds of winning. I also think that the system performed extremely well in its last match, and Google will take the intervening time to port it onto yet more powerful GPU clusters, deepening its raw search performance.   "], [19333, "2016-02-06T18:33:42Z", [0.4], "AlphaGo won eight of its ten games against Fan Hui.\nLee Sedol would probably not lose one game in ten against Fan Hui; AlphaGo (as of its match against Fan Hui) is notably weaker than Sedol. Starting guess: about 20% chance per match to win. AlphaGo has several months to grind knowledge at a rate that Sedol cannot hope to match. If Sedol hasn't learned something about Go in his years of playing by now, he probably won't make sudden leaps in improvement. Update, 10% increase to win per match. Total, 30%. AlphaGo has no emotional clouding or fatigue and will function at maximum for the duration of the event. Sedol has to account for nerves, pride, and fatigue. AlphaGo will also not give him any benefit of the doubt on poor play or question whether something is a trap, so any mistake (that AlphaGo has the capacity to exploit) will not go unpunished. Knowing THAT could also make Sedol play worse. Increase by 15%, total, 45% chance per match. (Side thought: AlphaGo grinds massive numbers of matches and applies its current knowledge to them. I don't know how it's coded; is it possible there is an exploit here where Sedol can begin a situation that SEEMS weak, which AlphaGo would only have low-level data on? Specifically, if AlphaGo begins by studying positions that only poor players would start, and does so at the start of its \"career,\" then it may have limited data on how a high level player would function from those positions. If Sedol intentionally steps outside AlphaGo's experience, AlphaGo will not necessarily have the tools to play correctly, but Sedol might. Such an exploit could drastically reduce AlphaGo's performance and give Sedol the ability to cheese through a match. This can happen to Chess players who primarily work from memorization, could it happen to AlphaGo? If true, and if Sedol knows to exploit it, then AlphaGo's winrate drops. I will assume this is not the case, but I'm writing it down as an interesting possibility). AlphaGo's team might update its programming to be more efficient. I am unfamiliar with neural networks and how plausible this is, but AlphaGo can still be modified. Even a minor improvement would yield massive gains in skill over the course of months of simulations. Update by 5%. Well, I accidentally ended up at 50/50. I'm going to adjust down slightly for the possibility that Sedol would have more experience and ability in unorthodox situations, and may choose to try one as an intentional gambit. Let's go back down to 45% winrate per game. Binomial CDF says AlphaGo has roughly a 40% chance of winning at least 3 games in a 5 game set, so I'm going to say 40% chance of winning; 1% of that is a 5-0 blowout, 12% chance for a 4-1 in favor of AlphaGo, 27% chance of a 3-2 victory for AlphaGo."], [19331, "2016-02-06T17:21:17Z", [0.11], ""], [19332, "2016-02-06T16:59:43Z", [0.99], "It has more capacity for process and thought - in a manner of speaking - Skynet?"], [740, "2016-02-06T16:46:32Z", [0.61], ""], [12828, "2016-02-06T15:32:41Z", [0.75], ""], [19311, "2016-02-06T15:25:42Z", [0.35], "There is really not enough info on this available (or I didn't found it). AlphaGo reportedly didn't use recordings of professional games for training before, but it would be really strange if Google didn't try to improve it in that way. Although Lee Sedol is much stronger than Fan Hui, I read that he's quite emotional and random losing in one of two first games could really hurt him to the following games..."], [691, "2016-02-06T14:39:43Z", [0.76], ""], [12874, "2016-02-06T12:34:50Z", [0.27], ""], [19308, "2016-02-06T11:24:38Z", [0.12], ""], [14394, "2016-02-06T09:57:56Z", [0.08], "- Lee Sedol is a much stronger player than Fan Hui. Fan Hui has a very low chance of winning a single game against Lee Sedol\n33% There's a chance AlphaGo is actually not (much) stronger than Fan Hui despite the 5-0 win. 0% win chance if this is the case for AlphaGo\n67% AlphaGo is indeed much stronger than Fan Hui split as below:\n      54%  I still find it far more likely than not that the AI is not yet on par with  Lee Sedol. 0% win chance if this is the case for AlphaGo\n      9% AlphaGo is roughly on par with Lee Sedol. 50% win chance if this is the case\n      4% AlphaGo is (much) stronger than Lee Sedol: 95% win chance."], [15471, "2016-02-06T09:54:34Z", [0.75], "Maybe this prediction is mostly a reflection of my believe in technology vs intuition. But google tends to do a good job at such \"AI\" projects."], [436, "2016-02-06T08:50:36Z", [0.62], "As has so often happened, I wish I truly understood Go, and could tell how close the matches were between AlphaGo and Fan Hui. Fan Hui is 600 Elo points below Lee Sedol, so the match isn't as informative as I'd expected (I knew Go was regionally lopsided compared to chess, but this is ridiculous). I do wish that I knew how much \"worse\" AlphaGo played in that match than we'd expect Lee Sedol to -- it's possible that its true strength will only reveal itself against a proper opponent. The thing about a Go program that can learn by playing itself: It can run through thousands of games against itself, a progressively more skilled opponent, in the six months between the Fan match and the Lee match. Deep Blue improved substantially against Kasparov in 15 months (despite Kasparov also getting the chance to adjust and prepare). AlphaGo should be developing skill much faster than Deep Blue ever could (imagine the machine-learning of IBM 1996 vs. what Google can muster today!). Lee may be a totally different level from Fan, but AlphaGo 2.0 should also be a totally different level from AlphaGo 1.0. I'd give the computer a slight edge, but in my heart I'm rooting for Lee."], [19096, "2016-02-06T06:42:45Z", [0.43], "Go is considerably more difficult than chess for computers due to exponentially larger number of positions, so it's remarkable that AlphaGo has already defeated a 2 dan player just 5 months ago 5-0. But Lee Sedol is a 9 dan world champion, so that, plus the game's complexity makes me think the human has the edge. "], [17875, "2016-02-06T05:32:21Z", [0.05], "Goratings.org puts Lee sedol's ELO at 3519 and a couple posts on this IFP point out the Google paper estimates AlphaGo ELO rating at 3140 back in October. Programmer friends tell me there's a plateau point at which throwing better hardware at a program doesn't make it stronger.   AlphaGo's programming apparently scales well on supercomputing and that's a big strength. That Google can train on multiple instances against itself is a pretty large difference than previous attempt at Chess or Go.  AlphaGo will reach a similar plateau with the program too.  I'm sure Google knows what that is and I guessing since the dates are settled they don't foresee that being an issue,\n"], [19293, "2016-02-06T05:04:54Z", [0.05], ""], [60, "2016-02-06T04:26:49Z", [0.08], "No change. TIL --> This IFP could have resolved on a technicality (i.e. in the negative, as Sedol could have declined the invitation?) Feb 5: \"Google AI Will Compete Against Go World Champion Lee Sedol On YouTube\" --> http://gizmodo.com/google-ai-will-compete-against-go-world-champion-lee-se-1757289813 It\u2019s official: The world champion of Go, Lee Sedol, will face off against Google Deepmind\u2019s powerful artificial intelligence, called AlphaGo. A week ago, Google DeepMind\u2019s team claimed to have built the best AI for the game in a scientific paper, and issued a public challenge to Sedol. DeepMind hopes it can prove that a powerful artificial intelligence is capable of beating the best human player in one of the most mathematically complicated board games ever created. The match between Sedol and DeepMind will be broadcast live on YouTube beginning March 9. The prize for the event is $1 million.\n[...]\nAlphaGo already successfully defeated three-time European Go champion Fan Hui in a series of five matches, but Sedol is considered to be a different challenge entirely. Sedol has dominated the game of Go throughout his professional career, earning him recognition as one of the great players of the modern era. His match against AlphaGo will be one of the most difficult challenges of his life.\n~~~~ And this tweet is important --> https://twitter.com/demishassabis/status/695379217870008321 Match days: 9, 10, 12, 13, 15 March - will be livestreamed on YouTube. More details soon. We are very excited to be coming to South Korea! ~~~ COMMENT: Not that a forecast changed on March 9th will really have a major effect on one's score for this IFP, it'll certainly be telling to see how that first game falls (and if this ends up going to the \"fifth\" game before a winner can be determined -- it might help one's score to update one's forecast at the conclusion of that game)."], [19288, "2016-02-06T03:35:53Z", [0.73], "Google is very good at this sort of thing. "], [10475, "2016-02-06T03:24:27Z", [0.63], ""], [93, "2016-02-06T01:41:24Z", [0.25], ""], [19260, "2016-02-06T01:22:29Z", [0.12], ""], [10921, "2016-02-06T00:44:51Z", [0.09], ""], [2010, "2016-02-05T23:59:49Z", [0.0], ""], [19273, "2016-02-05T23:53:24Z", [0.65], "Since it won 5-0 against the European champion, and since Google has some reputation riding on this, I'd say they'll make sure they've got good odds"], [19255, "2016-02-05T22:43:43Z", [0.08], "Google estimates the elo rating of AlphaGo to be 3140. Goratings.com estimates the elo rating of Lee Seedol to be 3516. Assuming that these ratings are accurate, there is about a 10% chance that AlphaGo will win each individual game and less than 1% chance that it will win the five game match. Elo ratings tend to be quite accurate, but accounting for the possibility of error in the ratings and the possibility that Google can significantly improve AlphaGo through algorithm changes, additional learning, or by running it on much more powerful hardware, I will make a quite liberal estimate that AlphaGo has an 8% chance to win the match."], [19264, "2016-02-05T22:34:01Z", [0.8], ""], [632, "2016-02-05T22:15:03Z", [0.95], ""], [14761, "2016-02-05T21:50:16Z", [0.35], "Given the unpredictability and range of possible moves the computer needs more experience against top rated players to consistently beat them."], [2, "2016-02-05T21:25:17Z", [0.25], "No change.  Just stimulated by other people's comments.  I see a lot of forecasters cite additional improvement/learning by AlphaGo.  It's interesting what the learning rates of AlphaGo is.  As I understand it, on chess computers, the improvements mainly came from throwing more processing power at the program.   For Deepminds, like AlphaGo, what do you think the constraints are on its learning rates? Is it just more time for it to play itself?  Get a bigger collection of past games?  Is it time for the engineers to analyze the data and tweak the neural net architecture?  Is it still throwing more CPUs at the problem?"], [100, "2016-02-05T21:21:12Z", [0.73], ""], [19250, "2016-02-05T20:59:40Z", [0.6], ""], [19252, "2016-02-05T20:51:06Z", [0.46], ""], [19122, "2016-02-05T20:50:59Z", [0.95], "The difference between 2 dan and 9 dan is 4 stones ( if i got this correct ) , and AlphaGo has beaten other programs with just this much handicap ( according to their website ) . Now since Zen ( other go program) has beaten a 9 dan at 4 handicap , its highly likely that AlphaGo will beat Lee Sedol . I have to detract some percentage because of AlphaGo`s conservative playstyle .\n"], [19240, "2016-02-05T20:42:13Z", [0.85], ""], [706, "2016-02-05T20:30:27Z", [1.0], "It already beat a human and its been 'learning' since then.  Mr. Sedol has no chance."], [14307, "2016-02-05T19:16:05Z", [0.94], ""], [14307, "2016-02-05T19:15:45Z", [0.9], "One of my friends who is a borderline professional go player mentioned that he is confident that the program is going to win, based off of how the other professional match went, and based off of the kind of tactics the computerized player has available to it.  "], [12289, "2016-02-05T19:13:06Z", [0.9], ""], [14993, "2016-02-05T19:08:34Z", [0.7], ""], [19226, "2016-02-05T18:56:59Z", [0.8], ""], [651, "2016-02-05T18:53:32Z", [0.1], "The differences in ratings, even granting the program's learning potential, suggests a low probability for the March match.."], [73, "2016-02-05T17:50:05Z", [0.36], "-3, part 1 is right 57% of time up from previous 44%.  Taking three off, assuming (without any facts to back me up) it needs to be at 70% to beat 9d player, and the 1m games it plays a day are not enough to get there.\n\"The program involves two neural networks, software that mimics the structure of the human brain to aggregate very simple decisions into complex choices, running in parallel. One, the policy network, was trained by observing millions of boards of Go uploaded to an online archive. Using those observations, it built up a predictive model of where it expected the next piece to be played, given knowledge of the board and all previous positions, that could accurately guess the next move of an expert player 57% of the time (compared to a previous record of 44.4% from other groups). This \u201csupervised learning\u201d was then backed up by a bout of \u201creinforcement learning\u201d: the network was set to play against itself, learning from its victories and losses as it carried out more than 1m individual games over the course of a day. The policy network was capable of predicting the probability that any given move would be played as next, but the system also needed a second filter to help it select which of those moves was the best. That network, the \u201cvalue network\u201d, predicts the winner of the game given each particular board state.  from http://www.theguardian.com/technology/2016/jan/27/google-hits-ai-milestone-as-computer-beats-go-grandmaster/"], [691, "2016-02-05T17:39:19Z", [0.76], ""], [19185, "2016-02-05T16:50:39Z", [0.3], ""], [19183, "2016-02-05T16:44:08Z", [0.75], "The previous matches were at a bit of a handicap, but it won 5/5 and has made tremendous progress in a short period of time without the kind of active management of the Deep Blue chess program.  I suspect it's simply learned to be a better Go player than humans are capable of."], [10298, "2016-02-05T16:33:08Z", [0.75], ""], [19173, "2016-02-05T16:30:06Z", [0.24], "The recent high-profile computer-go victories have been against players ranked a ways from the top.  Computer-go is developing, more quickly than anticipated, but such development in such short time would be pretty astounding."], [19178, "2016-02-05T16:25:55Z", [0.65], ""], [19159, "2016-02-05T16:00:27Z", [0.22], ""], [19177, "2016-02-05T15:59:31Z", [0.6], ""], [4288, "2016-02-05T15:47:10Z", [0.64], "This is difficult - I am an avid Chess player, but not a Go player; so my forecast is likely misinformed. But based on how the AlphaGo is programmed to process information, Lee Sedol will be playing the accumulated expertise of all Go players (based on a historical archive of previous Go matches). This is similar to IBM's Deep Blue approach, but Google's is more advanced, as its not programmed through brute force (inputting every game), rather is utilizes a technique of Deep Learning that enables the program to 'learn' from this historical data set AND identify new emerging patterns that may have not already been recorded or observed in a Go Players' strategy.  Additionally AlphaGo does not have a stylistic form of play, making Sedol's preparation for the program difficult. I think @KillerDucky forecast is important to observe"], [19170, "2016-02-05T15:41:00Z", [0.35], ""], [18967, "2016-02-05T15:22:49Z", [0.2], "Google's paper estimates the strongest version of AlphaGo in October had an Elo rating of 3168, calibrated to the scale at goratings.org. Lee Sedol is 3516, giving AlphaGo a 12% chance in each game, less than 10% to win a BO5. The big question is how much did it improve since October, and that's hard to say.\n"], [19168, "2016-02-05T15:20:56Z", [0.75], ""], [19165, "2016-02-05T15:09:11Z", [0.64], ""], [12874, "2016-02-05T15:03:21Z", [0.23], ""], [19155, "2016-02-05T14:57:40Z", [0.65], ""], [19148, "2016-02-05T14:41:32Z", [0.3], ""], [19139, "2016-02-05T13:52:34Z", [0.05], ""], [110, "2016-02-05T13:34:18Z", [0.05], "Low chance of this happening right now. Fan Hui is 2nd dan. Lee Sedol is 9th dan. If you play go, you may have a feel for this. The level of play ramps up exponentially, not linearly, at the peak. They'll get there eventually, but I seriously doubt they'll do it now."], [19127, "2016-02-05T12:09:51Z", [0.4], "Sedol elo 3600 ago current elo 3100. Optimistic to make another leap."], [19126, "2016-02-05T12:06:09Z", [0.65], "AlphaGo learns fast"], [19125, "2016-02-05T11:52:49Z", [0.75], ""], [19121, "2016-02-05T11:30:11Z", [0.15], ""], [19116, "2016-02-05T10:44:22Z", [0.6], ""], [10039, "2016-02-05T10:15:06Z", [1.0], "Alpha go plays thousands of games in a day and learns in the process. Alpha go at the end of the tournament will not be same as the alpha go at the beginning."], [18920, "2016-02-05T09:47:03Z", [0.1], "I found a really good blog post yesterday on this very subject by Anders Kierulf. His perspective is interesting because, next to being a decent go player, he has been involved with computer go programming for many years. He offers a few good links as well, in particular an overview in .pdf from the Britisch Go Association. Please read: https://smartgo.com/blog/lee-sedol-vs-alphago.html I changed my forecast to give the AlphaGo team a bit more credit but I still think Lee Sedol is a strong favorite."], [19100, "2016-02-05T08:54:18Z", [0.22], ""], [19102, "2016-02-05T08:50:44Z", [0.2], ""], [19101, "2016-02-05T08:42:13Z", [0.32], ""], [19095, "2016-02-05T07:58:26Z", [0.45], ""], [19092, "2016-02-05T07:58:25Z", [0.41], ""], [19094, "2016-02-05T07:56:11Z", [0.6], ""], [19091, "2016-02-05T07:32:28Z", [0.65], ""], [19091, "2016-02-05T07:31:55Z", [0.65], ""], [19083, "2016-02-05T06:31:30Z", [0.42], ""], [19080, "2016-02-05T06:28:41Z", [0.38], ""], [19074, "2016-02-05T06:06:37Z", [0.05], "The gap between the level of skill in the player that AlphaGo beat vs Lee Sedol is absolutely enormous. I think it's highly unlikely that AlphaGo can beat one of the top-ranked pros."], [19078, "2016-02-05T06:04:26Z", [0.65], ""], [19077, "2016-02-05T05:58:33Z", [0.66], ""], [19076, "2016-02-05T05:54:04Z", [1.0], ""], [14296, "2016-02-05T03:11:20Z", [0.9], ""], [19055, "2016-02-05T02:51:31Z", [0.65], ""], [1993, "2016-02-05T02:31:09Z", [0.35], ""], [19053, "2016-02-05T01:38:47Z", [0.62], ""], [689, "2016-02-05T01:30:31Z", [0.49], ""], [17820, "2016-02-05T01:17:52Z", [0.55], "good start by the program beating teh european champ.  Just too much horsepower for a human brain and AI technology has come a long way."], [12879, "2016-02-05T01:09:45Z", [0.57], ""], [15914, "2016-02-04T23:41:22Z", [0.7], "Google mines the human brain. They have his info already."], [2010, "2016-02-04T23:41:00Z", [0.0], "easiest prediction of my life"], [17875, "2016-02-04T23:18:00Z", [0.05], "AlphaGo having information about the Lee while Lee knows very little about AlphaGo is probably the largest disadvantage. AlphaGo can effectively play Lee a million times just to find ANY pattern, any tendency, or any strategy that will result in a better opportunity for winning. For example:  Let's say two players, A and B, play each other 100 times.  In the beginning, A is winning 75% of the games, but after 10 games, B realizes A has a flaw in their play and starts to turn the tables, winning 75% of the games from game 11-100.  So, who is the better player? So, imagine AlphaGo gets to play Lee Sedol's entire career of games weighting most recent games patterns and tendencies more than older game.  And it gets to play those games only 10,000 times.  That's not a situation IN A GAME, that a pattern across game, pattern across his lifetime, a pattern Lee exhibited early in his career and doesn't any more which is an advantage unto itself.  And, Lee doesn't get that?  Lee sits down and plays having never played against AlphaGo before!!   Um, that's intimidating just to think about !! --\nAlphaGo doesn't actually maximize points, but rather to maximize the probability it will win. There's a subtle difference there: when it is ahead, it will play \"safe\" to keep its lead rather than try to get more points (99.9% win @ a point vs 99.85% win @2 points).   Meaning  it actually plays better against a strong player and as it gets better playing against itself will actually bring out its best play. Here's the frustrating part about how AlphaGo works: there's no way to know if it's trying or not!  In some sense, it's reminds my the \"bug\" that Garry Kasparov got so pissed about when Deep Blue played a random more (programmed to do so under that condition) and Kasparov was throw off by the arbitrariness and accused the programmers of cheating. AlphaGo has also been playing non-official games that were scheduled before the October games.  And Google's been playing games giving handicap stones to make it more difficult.  The distributed version of AlphaGo is significantly stronger, winning 77% of games against single machine AlphaGo, and they use the distributed version to train. "], [18888, "2016-02-04T23:03:35Z", [0.76], "This is an update from my previous prediction of 91%. I have found that there 5 additional, unofficial matches played between AlphaGo and Fan Hui, resulting in a 3-2 series favoring AlphaGo. This was played under a different, faster time control, but given the limited information, I think it's important to account for this unofficial series. Furthermore, while Fan Hui might be affected by playing \"official\" vs \"unofficial\" games, AlphaGo should be completely indifferent, further leading to my desire to use these games as evidence. Using the same Elo Calculators as before, an 8-2 total score against someone would be an Elo difference of 241. This leads to an Elo difference between AlphaGo and Lee SeDol of 108, leading to a probability of AlphaGo winning a given game against Lee SeDol of 65%. Over a five game series, this means that AlphaGo has a 76% chance of victory. http://gooften.net/tag/alphago/ End Note: My analysis of Lee SeDol only being 133 Elo ahead of Fan Hui has been questioned. I'll keep looking into it to see if and how to revise that difference."], [16292, "2016-02-04T22:46:53Z", [0.54], ""], [14413, "2016-02-04T21:44:52Z", [0.85], ""], [14240, "2016-02-04T21:41:10Z", [0.74], ""], [691, "2016-02-04T21:32:20Z", [0.76], ""], [14777, "2016-02-04T20:51:15Z", [0.12], ""], [19031, "2016-02-04T20:32:07Z", [0.75], "Go is a game where most moves are known to be good or bad.  The process is to not miss a good move and avoid making a bad move.  This skill is better performed by a routine that structurally examines every possible move and ranks them.  Humans tend to move before exhausting analysis of all possible alternatives.  "], [15016, "2016-02-04T20:21:58Z", [0.95], ""], [16272, "2016-02-04T19:22:40Z", [0.25], ""], [138, "2016-02-04T18:50:21Z", [0.0], ""], [94, "2016-02-04T18:28:13Z", [0.15], "My sense from other commenters is that AlphaGo's victory over Fan Hui says almost nothing about how likely it is to win against Lee Sedol, given the difference in quality between the two players.  At the same time, knowledgeable commenters (such as @einsteinjs, @Jean-Pierre, @GJDrew, and @Dima-K) seem to be giving low estimates.  My sense is that AlphaGo could win, but it would take a large improvement that is unlikely to occur. I am hoping some bookmaker will give odds for this question...\n"], [837, "2016-02-04T17:23:37Z", [0.75], ""], [130, "2016-02-04T16:43:56Z", [0.59], ""], [10759, "2016-02-04T15:29:57Z", [0.35], ""], [4634, "2016-02-04T15:29:30Z", [0.35], ""], [17875, "2016-02-04T14:45:37Z", [0.74], "To take a straight outside -> in Tetlock best practices approach to forecasting this question I think @balakirev has hit the nail on the head.  \n--\nWe need great questions, the responses to which have the propensity for fairly accurate forecasts: https://en.wikipedia.org/wiki/Precision_questioning\n--\nBig Picture\n  --> AlphaGo was created by DeepMind which was purchased by Google 2014.  It wasn't till 2015 that they started in earnest on this problem.\n  --> I'm not an AI expert but sufficiently knowledgeable to claim:  Historical examples such as chess don't uses current AI paradigms therefore are not comparable.\n  --> I'm suggesting we call the \"modern era\" for comparable cases to be those after 2010 for evaluating learning rate of AlphaGo (i.e.Watson, or otherwise)\n  -->  AlphaGo's original corpus and methodology to begin was mimicking human play by attempting to match the moves of expert players, using a database of around 30 million moves from recorded historical games (AlphaGo wiki). Then AlphaGo played itself over 1,000 million times as non-supervised reinforced learning  so that's the base or bare minimum established knowledge basis upon which will improve.\n  --> That process culminated in a BENCHMARKING game against the European Champion so as to determine, evaluate, and focus it's learning path\n  --> The next step is to begin SUPERVISED reinforced learning targeting specific facets of it's game to improve performance, which is probably occurring now\n  -->  AlphaGo has a record of 8-2, started in 2015, and is scheduled to play in 2 months\n--\nNarrowing down to a forecast @balakirev comments are helpful types of information we need.  What question's answers have the propensity to give us that information?     --> how efficiently can the program study?\n  --> Can it improve an average of 1 point per 1 million games? Or would it take 10 million games? 100 million games?\n  --> How much harder does it become for the next point?\n  --> Would it require twice the number of games as the previous point? 5x? 10x? or perhaps only 1.1x? I think there are two units of measure here, each with advantages, and both with different implications for the result: \n --> CALENDAR DAYS \n..................as time in which training could have occurred  (smooths out learning rates over time so as to create statistical result ) \n..................Google may have a an even better learning strategy than is faster ( incorporates the fact that Google/Alphabet has options we don't know about )  -->   GAMES  PLAYED\n...................reinforced learning against instances of itself  \n.................................. this is has a greater margin of error  ( we know it's at least 1,000,000 times and it's played equivalent to 10,000 years worth of Go as of Oct 15 )\n...................................we don't know the base learning rate given a number of instances, the optimal number of instances given it's projected learning rate, etc.\n..................................there's no ceiling since Alphabet/Google has sufficient resources to have played a seemingly infinite # of games 1x10e1,000,000 if it wanted\n...................games against other professional players in specific game scenarios in which to improve\n...................................as has been pointed out this is likely to improve those weaknesses identified in the bench-marking game\n...................................this may or may not drastically improve the caliber, style, and performance given key insight, concepts, & methods of playing out alternatives\n..................................IT'S  SO  EASY  to simply weight this type of learning iteration and then reinforcement as higher so as to really maximize this rich information\n...................all the moves the next opponent has ever played or could have\n.................................AlphaGo is clearly advantaged in that it can access every situation the opponent has ever encountered in every game ever played as a pro\n.................................AlphaGo is also clearly advantaged in that it can identify extremely complex patterns in the game play of an opponent that was unforeseen\n................................AlphaGo is also able to anticipate the pattern changes over time as an opponent has developed, re-engineer their learning patterns, and anticipate the takeoff's an opponent is anticipating given a unique state of circumstances -- @balakirev  original comment:  @Wund I think it can progress pretty far. I mean if you think about it, when humans invented the game, they were probably like 30k. Heck, they probably didn't even know about ladders. I'm even willing to bet that the ko rule wasn't even considered at the beginning. It probably started from a pure capturing game, when they realized that some groups cannot be captured. Then they realized you can surround whole areas and stay alive at the same time. And so on. So even without professionals to teach, humans were able to slowly progress to the point that they are at now. I guess it goes back to how efficiently can the program study? Can it improve an average of 1 point per game with an input of 1 million games? Or would it take 10 million games? 100 million games? And how much harder does it become for the next point? Would it be twice as hard as the previous point? 5x? 10x? or perhaps only 1.1x?"], [357, "2016-02-04T14:05:31Z", [0.55], ""], [10039, "2016-02-04T14:05:19Z", [0.34], ""], [1646, "2016-02-04T13:56:08Z", [0.25], ""], [1468, "2016-02-04T10:51:17Z", [0.13], ""], [18992, "2016-02-04T07:13:53Z", [0.05], "AlphaGo won a pro who would not stand a chance against Lee Sedol at all, so basically there no real information available this. But we don know some issues with MCTS programs have not been solved and Lee Sedol is the player to steer game in huge whole board battle with constant threat of multiple ko-fights.   AlphaGo is more like Depp Touhght thatnb Deep Blue at the moment"], [4729, "2016-02-04T06:58:30Z", [0.85], ""], [13897, "2016-02-04T06:56:44Z", [0.45], ""], [10783, "2016-02-04T05:41:57Z", [0.0], "Ten years all of a sudden crammed into this one tournament? \nTwo computer scientists in the article say they are confident that Sedol can beat AlphaGo. \nSedol himself is confident. \nThe tools are not new but the integration of it is. \nI think in the future it is possible. But it just seems as though both computer experts and Go Players are confident Sedol can win. \n"], [11461, "2016-02-04T04:41:15Z", [1.0], ""], [15443, "2016-02-04T04:22:22Z", [0.92], "The neural networks provided in this algorithm already has defeated a European champion. Neural networks nowadays have spooky ability to learn quickly and learn well."], [17875, "2016-02-04T03:25:17Z", [0.75], "Does anyone think it's odd that the closing date for this is  APRIL FOOLS day? http://thenextweb.com/google/2015/04/01/roundup-all-of-googles-jokes-for-april-fools-day-2015/#gref"], [18969, "2016-02-04T01:20:19Z", [0.33], "It took many years for chess computers to beat very strong human players (late 1960's to 1980's).  I am going to assume creating an computer program (AI?) to beat a human at Go is similarly complex to creating a computer program (AI?) to defeat a human at chess.  Even accounting for a Moore's law type of effect which may accelerate AI development in the intervening years, I don't think even Google can go from beating its first human who was a strong player, to beating a player on the level of Lee Sedol.   I found this link useful: \nhttp://www.nature.com/news/go-players-react-to-computer-defeat-1.19255  To quote from Jonathan Shaeffer in the above article: Jonathan Schaeffer\nComputer scientist at the University of Alberta, Edmonton, Canada, and designer of Chinook, the program that solved draughts (checkers) in 2007 \"This is not yet a Deep Blue moment [when the computer beat world champion Garry Kasparov at chess in 1997]. The real achievement will be when the program plays a player in the true top echelon. Deep Blue started regularly beating grandmasters in 1989, but the end result was eight years later. What I see from these numbers is that the gap between where AlphaGo is and where the top humans are has shrunk enormously, and it\u2019s quite possible that with a bit more work and improvements, and more computing power, within a year or two they could do it. [In the March match], no offence to the AlphaGo team, but I would put my money on the human. Think of AlphaGo as a child prodigy. All of a sudden it has learned to play really good Go, very quickly. But it doesn\u2019t have a lot of experience. What we saw in chess and checkers is that experience counts for a lot.\""], [14253, "2016-02-04T00:39:11Z", [0.5], "While working today I had https://www.youtube.com/c/USGOWeb on in the background. A video in which Myungwan Kim 9p analyzes some of the games between Fan Hui and Alpha Go. Thanks for that link, @Wund.  From what I heard and took in he thinks AlphaGO has almost no chance based on how it played. He did seem to be quite impressed with how it \"read\" the game. I'm coming down from near certainty, based on someone I trust, to 50%. Will wait for more top Go players to weigh in."], [13361, "2016-02-04T00:34:05Z", [0.15], "Google's AlphaGo sounds like an amazing development in AI that will only get more impressive over time (Wired: http://www.wired.com/2016/01/googles-go-victory-is-just-a-glimpse-of-how-powerful-ai-will-be/). That said, it beat the European champion who was ranked 633rd in the world without a handicap. That doesn't mean it's good enough to beat the guy ranked 5th in the world. Programming AI is challenging, & a lot can go wrong (e.g. AlphaGo needs an internet connection & a lot of processing power - Google's laying its own fiber). Given all the hype - and that everyone points out how experts predicted an algorithm to beat humans at Go was 10 years away, I think Sedol has the advantage, at least in 2016. I wouldn't be surprised if AlphaGo wins one of the five matches - but it would have to improve considerably between now and April to take 3+ out of five."], [15477, "2016-02-03T23:22:46Z", [0.35], "No handicap.  "], [14027, "2016-02-03T22:14:56Z", [0.8], "Google will not want to lose this one.  And they have some VERY smart people."], [14027, "2016-02-03T22:14:13Z", [0.8], ""], [12118, "2016-02-03T21:59:27Z", [0.79], ""], [1972, "2016-02-03T21:43:49Z", [0.85], ""], [691, "2016-02-03T21:18:40Z", [0.78], "Speaking strictly as a human, it's been fun being the smartest entity on the planet for a long time, but them days is over.  The consolation, decades or so from now, will be that we outsmarted ourselves and were not outdone by anyone else.  I suspect the machines will on the whole do a better job than we have of running things, but I fully expect to be dead before that question is definitively settled. (Once again, total lack of accountability triumphs over common sense, or personal preferences, or humanity's hopes and dreams, or anything else.)"], [11788, "2016-02-03T20:24:54Z", [0.85], ""], [4936, "2016-02-03T20:23:27Z", [0.63], ""], [18950, "2016-02-03T20:08:03Z", [0.9], ""], [16292, "2016-02-03T19:58:25Z", [0.64], "AlphaGo is a *really* good algorithm. As in, it's beaten every other Go algorithm in every game they've played. It also beat the European champion Fan Hui *5-0*, which is totally unprecedented. Previous algorithms could only beat strong amateur players, and were totally outclassed by top players. Then again, Lee Sedol is a *really* good player. He reached 1st Dan when he was 12, and 2nd Dan at 15. Fan Hui is as good as Lee Sedol was when he was a *child*. So the relevant questions are these: How much is Lee Sedol better than Fan Hui? And how does this compare to the extent that AlphaGo is better than Fan Hui? I'll start with the Human Players' Elo ratings. Lee Sedol is a 9-dan professional, which gives him an Elo of ~2940, while Fan Hui's 2-dan gives him an Elo of ~2720. This Go page (http://senseis.xmp.net/?EloRating) provides a table of probabilities of game out comes given Elo ratings. With Sedol ranked ~220 points higher than Hui, we see that Sedol has a 78% chance of winning this hypothetical game. (I'm not sure how well elo ratings convert to probabilities at pro level play. If someone knows more, please let me know). So, assuming a bunch of ugly probabilistic assumptions, we'd expect Sedol to beat Hui 4-1, in a game of 5. This is less than the 5-0 with AlphaGo, so we can naively assume that AlphaGo is at least a bit better than Sedol. Let's give it a prior 60% chance of winning. Let's modify it for the following reasons: - AlphaGo will probably improve a *lot* between these games, by both observing human games and by playing against itself. If it can discover and beat 2-dan professional strategies by itself, then it's pretty reasonable that it can do the same for higher level pro strategies. Bump up 15%. - There is probably a *lot* of room for Go strategies so high level that humans just aren't smart enough to think of them - so I'd say that the probability distribution for AlphaGo's skill level has a bloody huge fat tail that goes way over Lee Sedol's head. It could play at the level of a ridiculously smart human, or it could play at near omnipotence. We don't really know yet. Bump up 4%. - Before now, Go algorithms were really really bad, and pro players didn't take them very seriously. So when Fan Hui played against a computer in a scientific sort of environment, there's a good chance his head wasn't really \"in the game\". He might not have prepared properly, or something. I don't know. Bump down 3%. - In the comments, Wund argues that AlphaGo plays far below Sedol's level. I don't know anything about Go, so I can't really assess that opinion, but I'll take the outside view for a moment and bump down 10% accordingly. - Deep learning is *really* cool, and has provided totally game changing algorithms in a number of areas already (such as image recognition or video game AIs). For a while now I've believed that deep neural network effectively mimics a fundamentally human way of thinking about things, but do so with a much larger capacity to observe evidence. It might not be the end-game algorithm, but I'll still bump up the chance 5%. - I might have totally misunderstood how Elo probabilities work, and the difference between Sedol and Hui may be much greater than I thought. Bump down 7%. Which gives me an ugly and totally non-conclusive 64%. Ugh."], [16292, "2016-02-03T19:53:07Z", [0.56], "AlphaGo is a *really* good algorithm. As in, it's beaten every other Go algorithm in every game they've played. It also beat the European champion Fan Hui *5-0*, which is totally unprecedented. Previous algorithms could only beat strong amateur players, and were totally outclassed by top players. Then again, Lee Sedol is a *really* good player. He reached 1st Dan when he was 12, and 2nd Dan at 15. Fan Hui is as good as Lee Sedol was when he was a *child*. So the relevant questions are these: How much is Lee Sedol better than Fan Hui? And how does this compare to the extent that AlphaGo is better than Fan Hui? I'll start with the Human Players' Elo ratings. Lee Sedol is a 9-dan professional, which gives him an Elo of ~2940, while Fan Hui's 2-dan gives him an Elo of ~2720. This Go page (http://senseis.xmp.net/?EloRating) provides a table of probabilities of game out comes given Elo ratings. With Sedol ranked ~220 points higher than Hui, we see that Sedol has a 78% chance of winning this hypothetical game. (I'm not sure how well elo ratings convert to probabilities at pro level play. If someone knows more, please let me know). So, assuming a bunch of ugly probabilistic assumptions, we'd expect Sedol to beat Hui 4-1, in a game of 5. This is less than the 5-0 with AlphaGo, so we can naively assume that AlphaGo is at least a bit better than Sedol. Let's give it a prior 60% chance of winning. Let's modify it for the following reasons: - AlphaGo will probably improve a *lot* between these games, by both observing human games and by playing against itself. If it can discover and beat 2-dan professional strategies by itself, then it's pretty reasonable that it can do the same for higher level pro strategies. Bump up 15%. - There is probably a *lot* of room for Go strategies so high level that humans just aren't smart enough to think of them - so I'd say that the probability distribution for AlphaGo's skill level has a bloody huge fat tail that goes way over Lee Sedol's head. It could play at the level of a ridiculously smart human, or it could play at near omnipotence. We don't really know yet. Bump up 4%. - Before now, Go algorithms were really really bad, and pro players didn't take them very seriously. So when Fan Hui played against a computer in a scientific sort of environment, there's a good chance his head wasn't really \"in the game\". He might not have prepared properly, or something. I don't know. Bump down 3%. - In the comments, Wund argues that AlphaGo plays far below Sedol's level. I don't know anything about Go, so I can't really assess that opinion, but I'll take the outside view for a moment and bump down 10% accordingly. - Deep learning is *really* cool, and has provided totally game changing algorithms in a number of areas already (such as image recognition or video game AIs). For a while now I've believed that deep neural network effectively mimics a fundamentally human way of thinking about things, but do so with a much larger capacity to observe evidence. It might not be the end-game algorithm, but I'll still bump up the chance 5%. - I might have totally misunderstood how Elo probabilities work, and the difference between Sedol and Hui may be much greater than I thought. Bump down 7%. Which gives me an ugly and totally non-conclusive 64%. Ugh."], [18924, "2016-02-03T19:27:14Z", [0.63], ""], [1529, "2016-02-03T19:23:31Z", [0.55], "An excerpt from the Scientific American article in the IFP background, that leapt out at me..... \"The researchers trained these networks with two methods: For one network, they showed the computer more than 30 million moves from games played by human experts (this helped the system learn how the best players win); and for both of the networks, the researchers had the computer play thousands of games with itself so it could discover new strategies and learn the game on its own. These two training strategies allowed the computer to recognize patterns in the game and identify what moves gave it the best chance of winning.\" 30 Million....that's a lot of moves. One would assume some of those refer to Lee Sedol. Also assume researchers will be analyzing Sedol's past games, even more deeply. People thought a computer could \"never\" beat the chess champion or beat the Jeopardy champion. "], [1646, "2016-02-03T18:48:48Z", [0.4], "I suggest we have an virtual game watching party for gjopen forecasters the day of the first game. Anyone care to organize it ?"], [14779, "2016-02-03T18:03:15Z", [0.36], ""], [12029, "2016-02-03T17:59:36Z", [0.65], ""], [17875, "2016-02-03T17:49:17Z", [0.76], "I'm starting around 75% as an anchor.  I'm really interested in the question.   My experience in the JGP tournament taught me to stay away from the edges and actively manage your movements as a propensity toward an overall score.  If you're new to the game, I'd use that sentiment or something as simple as DON'T GO 0% or 100% as a very hard earned lesson because it's really difficult to come back from successfully.    I guess I state that to convey (in my mind) my % is compartmentalized from my reasoning/propensity in the rationale section but they evolve together over time. \n--\nside note:   Changed my user name from I_think_therefore_I_am cuz  I thought user names around \"doubt\" would be taken but I'm happy to have this one!"], [1500, "2016-02-03T17:29:10Z", [0.5], "I don't think Google would go for the match if they didn't believe they had at least a chance of winning, and they are no doubt working furiously at training and improving AlphaGo, so even though it seems unlikely that AlphaGo was ready for Lee Sedol back in October, it may be by March.  So I'm calling it a coin toss at this point."], [20, "2016-02-03T12:52:33Z", [0.05], ""], [10759, "2016-02-03T12:10:06Z", [0.25], ""], [4634, "2016-02-03T12:07:16Z", [0.25], ""], [18920, "2016-02-03T11:12:25Z", [0.05], "I'm a go player first, and interested in AI second. As a go player I am beyond amazed at the level of play demonstrated by Alpha Go against Fan Hui. It's an incredible step forward from the level I've seen previously from go playing programs such as Zen or Crazy Stone. Still, consider the gap between Fan Hui and Lee Sedol. Fan Hui is quite strong by amateur standards but not so at the top professional level. There actually is a video available online (accessible through the American Go association, https://www.youtube.com/c/USGOWeb) in which Myungwan Kim 9p analyzes some of the games between Fan Hui and Alpha Go. Myungwan Kim is stronger than Fan Hui, but less strong than Lee Sedol, and from a go perspective his opinion should count. Briefly stated, Alpha Go is not ready to compete at the top professional level. Reasons for that are too much dependence on standard patterns (not creative), soft play and no full understanding of Aji (hidden possibilities). If Alpha Go displays the level of skill shown in the Fan Hui games against Lee Sedol its chance of winning the match is near zero. The question is, then, how much can Alpha go improve in a couple of Months? That is an unknown to me, to most probably, but because all arguments are of a fundamental nature my guess is its chances are slim. Myungwan Kim 9p says he'll root for Alha Go because it's such an obvious underdog."], [10039, "2016-02-03T11:01:02Z", [0.95], ""], [446, "2016-02-03T10:43:21Z", [0.75], ""], [1738, "2016-02-03T08:58:13Z", [0.6], ""], [18914, "2016-02-03T08:10:44Z", [0.5], "It seems most of the people predicting has never played go, and are just randomly guessing. The (relative) difference between Fan Hui and Lee Sedol is HUGE. Goratings.org puts Lee sedol's ELO at 3519, and Fan Hui's at 2920. Lee is ranked no.5 in the world, Fan is ranked 631. And ratings for non-Korean/Chinese players on goratings are usually inflated, because they lack participation in international tournaments. A good example is Iyama Yuta, ranked 3rd on Goratings. According to most Chinese/Korean pros, Iyama Yuta should be ranked about 10-30.  If Lee Sedol played Fan Hui 10 matches, it's unlikely Fan Hui will win ANY of them, unless Lee makes a very silly blunder (which is possible, since he is human).  However, the absolute difference between Lee Sedol and Fan Hui is not that big. In Go, a difference in strength can be compensated with handicap stones, or adding points to your score at the end (komi). For example, if I played Lee Sedol, It's probably expected that he will beat me by at least 100 points. So if you add 100 points to my score at the end of the game, then I may have a chance of winning. The Absolute difference between Lee Sedol and Fan Hui is probably less than 10 points. To give you an idea, a game of go usually lasts 200-300 moves (unless someone dies big early on). Each move is worth around 10 points (if you're a very strong player), until endgame, when there are not many places/points left on the board. Now, when I say a move is worth around 10 points, I don't mean people end up with scores of say 1500 vs 1500 in a 300 move game. Because you might make 10 points with one move, but your opponent will reduce your points with his next move. But on average, the efficiency of a move should be around 10 points, which means that through out a whole game, some 2000-3000 points should've been exchanged. So a 10 point difference at the end is not that big, from an absolute point of view. Which is why I put the winning percentage at 50%. It purely depends on how much alphago has improved from October til March. For humans, those 10 points are like the difference between heaven and Earth. Many people can/have reached Fan's level, but very few have reached Lee Sedol's level. Most kids who study go as yuansheng/insei/yeongusaeng (basically kids who quit school and study go full time hoping to go professional) can get to Fan Hui's level in say 4 years of study from, an absolute beginner. But the vast majority of them never reach Lee Sedol's level. There's a huge rate of diminishing returns for humans, and the difficulty of improving by an average of 1 point per game increases exponentially at that level.  Will a DCNN bot also have the same kind of bottleneck? Will  a 1-point gain be 10x harder than the previous 1-point? Or will the bot not have such a bottleneck (or at least not yet)? That makes this very uncertain.  If Alphago performs anything similar to how it did when it played against Fan Hui, it doesn't have a chance against Lee Sedol. It's all about how much it can improve. So while I say it's a complete toss-up, I'm betting for a 5-0 result either way. Either Lee crushes AlphaGo, or AlphaGo completely obliterates human opponents once and for all. "], [18911, "2016-02-03T06:59:33Z", [0.01], "My rationale is pretty much straightforward. Lee is stronger than AlphaGo, and there's no way that AlphaGo can reach the level of Lee by upcoming March. AlphaGo needs something more to beat Lee. Something that no artificial intelligence ever accomplished -Creativity. I admit that by March, there is a good chance that AlphaGo is good at reading probably better than professionals, however it cannot go through the cognitive processes that pros do. Just like it's extremely difficult for a robot to do a simple task that a 6 year old can easily do, artificial intelligence cannot beat Lee yet unless it literally \"solves\" 19x19 go game."], [18909, "2016-02-03T06:43:36Z", [0.9], ""], [18888, "2016-02-03T04:29:27Z", [0.91], "Lee SeDol has won 68.28% of his matches as recorded on gobase.org: http://gobase.org/information/players/?pp=Lee+SeDol My first (admittedly strong) assumption is that Fan Hui, the professional player AlphaGo defeated, is as strong as Lee SeDol's average opponent, meaning that he would beat Lee SeDol 31.72% of the time. This leads to an Elo difference of 133 (http://www.3dkingdoms.com/chess/elo.htm). Let's further assume that there is a 50-50 chance that AlphaGo shuts out Fan Hui in a 5-game series any given day, not unreasonable considering that it has already accomplished just that. That would mean that AlphaGo has an 87% chance of beating Fan Hui in a single game (http://www.wolframalpha.com/input/?i=.5%3Dx%5E5), and their Elo difference would be 330. Thus, I am estimating a difference in Elo of 197 between Lee SeDol and AlphaGo in favor of AlphaGo, meaning there is approximately a 76% chance that AlphaGo beats Lee SeDol in a single game (http://www.caissa.com/support/chess-ratings.php). This translates to a 91% chance that AlphaGo will beat Lee SeDol in a best-of-5. TL;DR: Assume: 1) Fan Hui is equivalent to Lee SeDol's average opponent, and 2) AlphaGo had a 50-50 chance of shutting out Fan Hui. Convert these results to Elo ratings and compare."], [15506, "2016-02-03T03:51:11Z", [0.98], ""], [176, "2016-02-03T03:46:16Z", [0.1], "Swayed by Einstein"], [73, "2016-02-03T03:17:15Z", [0.39], ""], [60, "2016-02-03T03:08:11Z", [0.08], "Coming in for 8%. Amazing to see this one so high (!), but I guess that could change in the coming weeks. However, something tells me this IFP is probably likely to go up (rather than down, but ah well). So, lots of good stuff from some of the folks on here, so I'll simply aggregate and add a bit of commentary throughout. From @Dima-K --> https://www.gjopen.com/comments/comments/118118 \"No contest. Computer's estimated rating is too low in comparison. DeepBlue was beating grandmasters a decade before Kasparov. The time for this fake AI will come, just not this quickly.\" From @Jean-Pierre --> https://www.gjopen.com/comments/comments/118530 \"There's no comparison between the World Champion (9-dan pro) and the European Champion (2-dan pro). That would be like comparing World Chess Champion Magnus Carlsen to an ordinary International Master. The Europeans have a long way to go before they play Go as well as the Koreans, Chinese, and Japanese. Maybe AlphaGo could, in this analogy, be compared to a weak Grandmaster but I am not certain of this. Fan Hui didn't seem to be taking the match seriously. The wildcard is how much AlphaGo improves in the time between the two matches and whether Lee Sedol will have access to any of its games. Regarding the former, my guess is not nearly enough to win the match, let alone a single game.\" ~~~ For some data on what the two commenters above are discussing:  \"Can Google AlphaGo beat world Go champion Lee Sedol in March?\" --> https://www.quora.com/Can-Google-AlphaGo-beat-world-Go-champion-Lee-Sedol-in-March It's pretty hard to say given the limited amount of information we currently have. Overall, I think AlphaGo has a chance of beating Lee Sedol in a single game, but it would not be a decisive win. Over 5 games, AlphaGo is unlikely to prevail. So far all we know about AlphaGo is that it beat Fan Hui (2 dan) by a score of 5-0 with no handicap [1]. Although that by itself is a huge accomplishment and in no way should be downplayed, we should also be cognizant of the fact that Lee Sedol (9 dan) is a stronger player by a fair margin. To get an idea of how much stronger Lee Sedol is, we can ballpark Elo ratings for Lee Sedol (~2940) and Fan Hui (~2750) [2]. Based on conventional Elo rating distributions, the theoretical probability of Fan Hui winning a single game against Lee is around 25%. [1] https://googleblog.blogspot.ca/2016/01/alphago-machine-learning-game-go.html\n[2] https://en.wikipedia.org/wiki/Go_ranks_and_ratings ~~~~ Lastly, a comment from @GJDrew that leads me to be skeptical of AlphaGo's chances --> https://www.gjopen.com/comments/comments/118626 if you look at the history of Man v. Machine contests, e.g. chess or poker, in the beginning, sponsors of the Machine were willing to take a loss for publicity and to get a chance to learn. It's only a few years after the first match that they finally got the win. https://en.wikipedia.org/wiki/Human%E2%80%93computer_chess_matches (serious effort involving big corps starting late 80s) https://en.wikipedia.org/wiki/Computer_poker_players (serious effort starting I'd say at 2005). ~~~~ COMMENT: So, taking all that into consideration, I'm going to go with 8% (for now). I'd like to go lower, but I'm starting to learn my lesson on playing too close to the extremes on IFPs -- especially when I'm a little out of my element and let's be honest, while I've played Go (it's really fun!) I had no idea that Google had challenged the best player in the world to a match until I saw this IFP."], [3363, "2016-02-03T02:40:58Z", [0.34], ""], [11788, "2016-02-03T02:17:01Z", [1.0], ""], [12214, "2016-02-03T02:11:28Z", [0.9], "The game (go) is bounded, no reason why a computer cannot beat a human.  Agree with another poster that the media is playing this up.  The added complexity (within bounds) would favor computer over human."], [17023, "2016-02-03T01:31:57Z", [0.86], "The main argument in favor of AlphaGo is the way it learns. It started out being fed 30 million moves from the best players and then played itself thousands of times to develop pattern recognition and strategies.  The machine then uses statistics to identify the best moves instead of trying to go through each possible move.  This gives the program an absurd amount of 'experience' to build knowledge.  Additionally, this program beat the best player in Europe 5-0 so we know it knows what it's doing and can easily beat the best.   The second argument for the machine, is Sedol's play style. He often relies on taking larger risks and has the skill to make the risks pay off against lesser opponents. However, a computer isn't going to get flustered or play as risk averse as a regular opponent might.  Sedol has also been criticized for being too emotional and pessimistic; he conceded a game that was still believed to be well within his grasp.  This also reminds me of a strikingly familiar scenario when Kasparov resigned against Deep Blue when the game was not yet out of reach.  There may be a common psychological factor in play. What makes this more interesting, however, is that there isn't a single international system for rating players.  He may be much better than anticipated. Also, his unique style may possibly be enough to catch the program off guard. If he makes a move the computer found unlikely to be played because most players wouldn't play that way.  However, I don't see this being enough and will give AlphaGo a huge advantage. http://www.scientificamerican.com/article/computer-beats-go-champion-for-first-time/\nhttp://senseis.xmp.net/?LeeSedol\nhttp://www.europeangodatabase.eu/EGD/Player_Card.php?key=12633346\nhttp://www.economist.com/news/science-and-technology/21689501-beating-go-champion-machine-learning-computer-says-go"], [18893, "2016-02-03T00:39:58Z", [0.85], ""], [14375, "2016-02-03T00:34:17Z", [0.85], "The difficulty of computer Go is being built up and overstated, largely by tech media in search of a \"next big thing\" along the lines of Deep Blue or Watson. Expect a lot of hype around this in the coming months, but a predictable conclusion. Why would Google put it's reputation on the line for something like this if it wasn't really sure that it was going to win? The same question could have been asked about Watson or Deep Blue in their competitions. Google has no external deadline pressure to do this publicity stunt, meaning that they were able to wait until they were near certain of victory. Sedol probably doesn't mind that the odds are stacked against him because the match will raise his public profile either way. Go is hard, but I can't see why Google would do this unless they think they have a big advantage."], [102, "2016-02-03T00:08:45Z", [0.15], ""], [176, "2016-02-02T23:57:42Z", [0.15], "Playing the middle for now. I may yet regret this question. "], [18343, "2016-02-02T23:53:29Z", [0.98], ""], [241, "2016-02-02T23:52:17Z", [0.1], "There's no comparison between the World Champion (9-dan pro) and the European Champion (2-dan pro). That would be like comparing World Chess Champion Magnus Carlsen to an ordinary International Master. The Europeans have a long way to go before they play Go as well as the Koreans, Chinese, and Japanese. Maybe AlphaGo could, in this analogy, be compared to a weak Grandmaster but I am not certain of this. Fan Hui didn't seem to be taking the match seriously. The wildcard is how much AlphaGo improves in the time between the two matches and whether Lee Sedol will have access to any of its games. Regarding the former, my guess is not nearly enough to win the match, let alone a single game."], [14253, "2016-02-02T23:22:25Z", [0.95], "After beating the European champ it may now play millions/billions of more games before the big event. I think that, for a variety of reasons, a somewhat unjustified mystique has been built up around the game of Go. My guess is that if as much effort had been put into mastering Go over the years as was put into mastering chess this would have been solved already. Also, the remaing month of time left to improve its skills would be equivalent to many years of progress back in the 1990s. I say it destroys this carbon based ape!"], [11312, "2016-02-02T23:17:20Z", [0.8], ""], [17875, "2016-02-02T22:16:28Z", [0.76], "I'll raise a % because I don't think these are the relevant factors and I think this community may be underestimating AlphaGo. Maybe an edit or two would help get to them.  AlphaGo won 5-0 so it's significantly higher and the difference between European champ and World champ doesn't mean much because of that.  CPU isn't the limiting factor. AlphaGo doesn't follow human strategies ALL the time and in fact makes decisions that against \"normal\" play but are discovered to be great unexpected moves when analyzed ex-post (not creativity but simply unexpected as it uses pattern recognition).  Go is not like chess.  AlphaGo wins against other computer components 99% of the time and these vary but are generally considered so-so (even Facebook's version) as stated in the Nature article and other scholarly articles. I agree with everything @GJDrew  stated in the reply to my previous post with the Nature article link.  The factors in that post are more salient than these. I think the first thing I would like to know is a graph of (1) the rate at which AlphaGo's ELO score (or similarly comparable) has been increasing in absolute terms and relative to training time  (2) It's record relative to the opponents ELO score and world ranking (3) graph comparing both opponents \"time to prepare\" versus projected ELO levels, rate, and thresholds, as well as graphs for preparation to their outcomes (increase in ELO, margin of victory, thoughts about the play, etc. ) The things I'd like to know are known by Google and they scheduled the game.  So I'm confident they \"BELIEVE\" they can win while remaining doubtful Google/AI research can accomplish this so quickly.  If Google said, \"We know we can win, anywhere, anytime, and it's up to Lee Sedol to determine when he's ready,\" as a general public challenge then I'd know Google would win because it's stupid and reckless to make such a challenge as it probably wouldn't occur and there wouldn't be this question in GJP.  In poker that would signal a weak hand and push players out of the game.  Google basically \"called\", to use the poker term in @GJDrew comments,  and that signals they really do want this to happen with limited expectations (thus a huge victory if they win).  I think Google's wise in being low key and signals a strong poker hand. So they \"believe\" they can win, want expectations low so it's big deal when they win, and are playing it straight so they get a chance to put down the cards and make this a reality.  I'll go further and say I expect Google to speak VERY highly of Sedol and be extremely deferential at times to appear as though the cards are stacked against them. If they play this right and do win.  It would be really cool to get a rematch and spot Lee some number of stones as a handicap to really see how good it is. "], [2, "2016-02-02T21:34:53Z", [0.25], "Thinking through this, even the relevant factors are unknown. So going to take a whack at relevant factors: 1. How many ELO points is there between Fan Hui and Lee Sedol?  As I understand it, 400 ELO points often correspond to a large qualitative difference between player quality. 2. How well does the strength of AlphaGo scale in terms of computational strength? If they throw 2x CPUs at it, how much better does it get? 3. How weird does AlphaGo play when viewed by humans?  is it following normal strategies, but just playing them well, does it follow weird non-human strats? 4. Is Go even like Chess in terms of having conventional openings and strategies, or do different people play very differently?  5. Is Go a game where you make a lot of opponent specific adjustments (like poker) or one where you play your best game most everytime.  You've constructed a game, and it's up to your opponent to figure out how to break it.   Where it takes a lot of theory to construct and dismantle games? 6. Does Go like Chess have a easily assessible database of all the grandmaster games so that AlphaGo can have a very clear look on how Lee Sedol plays?  Will Lee have a database of AlphaGo games to look through?  Does he even care? 7. Were there any aspects of Go that the Pre-AlphaGo algorithms did well?  E.g. even in the early days of chess, finding a mate in 3 type problems were quickly solved.  Are there equivalent of \"tactics\" in Go that humans learned to avoid against computer chess?"], [691, "2016-02-02T21:28:33Z", [0.78], ""], [17875, "2016-02-02T20:12:01Z", [0.75], "I'm currently writing a paper on this for graduate school so my bias is pretty large in favor at the moment.  I would say it is \"very possible\" and that is a lot considering how difficult this challenge is. Funny @ISherman put very possible at 40% and I think it's 50/50 with an edge to Google given the analysis of the game they played last year beating the European champion 5-0.  I actually frame it in the obverse as Sedol only has had a year to prepare/learn.  I think Lee will have to take his game to a new level if he is going to win."], [2, "2016-02-02T20:00:02Z", [0.25], "Going down a bit on @Dima-K's comment.  It took computers a while in chess to climb the grandmaster hierarchy, and I'm sure there's an equivalent one in Go."], [13341, "2016-02-02T19:43:50Z", [0.4], "Very possibly- but it is still anyone's game.  I expect it to be a very tight margin (2 - 3).  I don't know that it will be learning much over five games, most of the time neural networks take thousands or millions of games for training.  I suspect that Sedol will be learning, too, and at a much faster rate- and he'll pull victory from the jaws of defeat."], [5001, "2016-02-02T19:39:39Z", [0.05], "No contest. Computer's estimated rating is too low in comparison. DeepBlue was beating grandmasters a decade before Kasparov. The time for this fake AI will come, just not this quickly. "], [63, "2016-02-02T18:39:13Z", [0.25], ""], [500, "2016-02-02T18:38:06Z", [1.0], ""], [12879, "2016-02-02T18:30:26Z", [0.78], ""], [823, "2016-02-02T18:28:52Z", [0.8], "Since this match is 5 games I think AlphaGo has a good chance to win since it probably stands to \"learn\" from playing the champ."], [10039, "2016-02-02T18:26:33Z", [0.9], ""], [2, "2016-02-02T18:26:20Z", [0.45], "@dada it made it as an official question!"], [14327, "2016-02-02T18:20:10Z", [0.75], "\"The really significant thing about AlphaGo is that it (and its creators) cannot explain its moves. And yet it plays a very difficult game expertly. So it\u2019s displaying a capability eerily similar to what we call intuition\"\nhttp://www.theguardian.com/commentisfree/2016/jan/31/google-alphago-deepmind-artificial-intelligence-intuititive Furthermore:\nhttp://www.theguardian.com/technology/2016/jan/27/google-hits-ai-milestone-as-computer-beats-go-grandmaster This program defeated the European grand master 5-0 (and has probably continued to adapt since the match)."], [4651, "2016-02-02T18:17:02Z", [0.78], "Google wouldn't schedule the match unless they were pretty sure they would win."], [102, "2016-02-02T18:14:43Z", [0.5], ""]]}